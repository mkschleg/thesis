#+title: Exploring the effectiveness of Recurrent Networks in Reinforcement Learning
#+FILETAGS: :THESIS:
#+author: Matthew Schlegel
#+STARTUP: overview
#+STARTUP: nolatexpreview
#+OPTIONS: toc:nil
#+OPTIONS: title:nil
#+OPTIONS: ':t
#+LATEX_CLASS: thesis
#+LATEX_HEADER: \input{variables.tex}
#+MACRO: c #+latex: %
#+MACRO: citeplease *[CITEPLEASE: $1, $2, $3, $4, $5, $6]*

* Preamble                                                           :ignore:
#+begin_comment
Preamble for UofA thesis. Needed to make thesis compliant. I use this in my candidacy as well, with specific
details commented out for brevity. This makes:
- title page
- abstract page
- table of contents
- list of tables
- list of figures

and sets formatting up for main text.
#+end_comment

#+BEGIN_EXPORT LaTeX

\renewcommand{\onlyinsubfile}[1]{}
\renewcommand{\notinsubfile}[1]{#1}

\preamblepagenumbering % lower case roman numerals for early pages
\titlepage % adds title page. Can be commented out before submission if convenient

\subfile{\main/tex/abstract.tex}

\doublespacing % possible options \truedoublespacing, \singlespacing, \onehalfspacing

%%%%%%%
% Additional files for thesis
%%%%%% 

% Below are the dedication page and the quote page. FGSR requirements are not
% clear on if you can have one of each or just one or the other. They do say to
% ask your supervisor if you should have them at all.
%
% The CS Department links to a comparison of pre- and post-Spring 2014 thesis
% guidelines (https://www.ualberta.ca/computing-science/graduate-studies/current-students/dissertation-guidelines)
% The comparison document lists an optional dedication page, but no quote page.

\subfile{\main/tex/preface.tex}
\subfile{\main/tex/dedication.tex}
\subfile{\main/tex/quote.tex}
\subfile{\main/tex/acknowledgements.tex}


\singlespacing % Flip to single spacing for table of contents settings
               % This has been accepted in the past and shouldn't be a problem
               % Now the table of contents etc.
               
\tableofcontents
\listoftables  % only if you have any
\listoffigures % only if you have any

% minimal support for list of plates and symbols (Optional)
%\begin{listofplates}
%...            % you are responsible for formatting this page.
%\end{listofplates}
%\begin{listofsymbols}
%...            % You are responsible for formatting this page
%\end{listofsymbols}
               
% A glossary of terms is also optional
\printnoidxglossaries
               
% The rest of the document has to be at least one-half-spaced.
% Double-spacing is most common, but uncomment whichever you want, or 
% single-spacing if you just want to do that for your personal purposes.
% Long-quoted passages and footnotes can be in single spacing
\doublespacing % possible options \truedoublespacing, \singlespacing, \onehalfspacing

\setforbodyoftext % settings for the body including roman numeral numbering starting at 1

#+END_EXPORT





* IN-PROGRESS Introduction
:PROPERTIES:
:CUSTOM_ID: chap:introduction
:END:

# - The egocentric perspective of MI agents
# - Learning to behave and predict using partial information about the world is critical for applying reinforcement learning (RL) algorithms to large, complex domains.
# - RL/DRL is one framework we can use to learn behavior in a large complex environment.
# - Deep Reinforcement Learning?
# - Architectural assumptions used from supervised learning for reinforcement learning
# - This dissertation contributes (lay out contributions)

Learning to behave and predict using partial information about the world is critical for applying reinforcement learning (RL) algorithms to large, complex domains. For example, a deployed automated spacecraft with a faulty sensor that is only able to read signals intermittently. For the spacecraft to stay in service it needs to deploy a learning algorithm to maintain helpful information (or state) about the history of intermittent sensor readings as it relates to the other sensors and how the spacecraft is behaving. Game playing systems such as StarCraft [[citep:&vinyals2019grandmaster]] provides another good example. An agent who plays StarCraft must build a working representation of the map, it's base and strategy, and any information about its rival's base and strategy as it focuses its observations on specific locations to perform actions.

Deep reinforcement learning has expanded reinforcement learning to wide spectrum of domains, specifically those with complex observations from the environment  [[cite:&mnih2015humanlevel;&vinyals2019grandmaster]]. Significant work has gone into engineering primarily non-recurrent networks [[cite:&hessel2018rainbow;&espeholt2018impala]]. Applications of recurrent networks to reinforcement learning problems typically apply the assumptions constructed in the supervised learning (SL) setting [[cite:&bakker2002reinforcement;&hausknecht2015deep;&onat1998recurrent;&kapturowski2019recurrent;&zhu2018improving]]. While the intuitions developed in SL set the stage for the reinforcement learning problem, many of these architectural and algorithmic intuitions might not translate to the new set of dynamics provided by reinforcement learning. A unifying justification for this view is the effect behaving and acting in the world may have on perception. [[cite:&noe2004action]] lays out this point clearly, suggesting behavior and decision making are linked to our (and other biological system's) strategies for perception. This is re-iterated in the deep reinforcement learning context in [[cite:&ostrovski2021difficulty]], where they show the difficulty in learning a representation in a deep Q-network (DQN) to be difficult if done passively in tandem with a controlling DQN. Finally, a related line of reasoning argues intelligence arises from the interactions between the mind, body and environment {{{citeplease tonyc}}}. While a clear definition of what constitutes a body over just a separate part of the environment hasn't been discussed in the reinforcement learning context, the importance of agency and acting during the foundation of perception is still highlighted as critical.

While we can--and arguably should--use the intuitions built in the SL setting, these intuitions must be tested and further justified as the problem setting changes to the continual learning or reinforcement learning setting. For example, often new cell architectures use a full sequence to estimate the gradients using backpropagation, which is not possible in a continual reinforcement learning problem. While several strategies exist to reduce the sequence length in approximating the gradient, these often lead to comprising on the length of temporal-dependencies the network can model (see Section ref:sec:bg:perception:tempdepend). Another example is in the incorporation of information into the recurrent cell. A reinforcement learning agent has three primary forms of information to incorporate into the state: sensations (or observations), prior beliefs, and action. The most straightforward (but possibly not best) solution is to combine these forms of information through a concatenation operation, passing the action and observation through separate feed-forward networks (see Chapter ref:chap:arnn for more details).

A critical structural assumption in agent-state construction is that search is driven by the gradients (or the importance of features) of the system with respect to the final error (i.e. the agent-state is free from constraints external to the main objective). One opposing idea is to search for agent-state through answering predictive questions of the agent's observations. Several approaches to state search using predictions have been proposed and studied. One approach is to use short histories to find a collection of observation action sequences to infer the probability of seeing another trajectory given the agent's current histry, also known as predictive state representations (PSRs) [[cite:&littman2002predictive;&singh2004predictive]]. Another architecture, known as Temporal-difference Networks [[cite:&sutton2005temporaldifference;&tanner2005temporal;&tanner2005td;&tanner2005thesis]], is highly related to the work presented in this dissertation. A TDN is a combination of a question network (built on the base observations) and an answer network (the parameter used for answering the questions). The TDN then used TD to learn the parameters of the answer network from experience.





# This is provided there is ample flexibility in the function class used to answer the state questions.
# Also such generalizations could be generally useful for the agent's downstream objectives.

# The /Predictive Representation Hypothesis/ is intuitively appealing with evidence provided through specific predictive approaches cite:&singh2004predictive;&sutton2004temporal;&sutton2005temporal;&schaul2013better;&sutton2011horde;&white2015thesis;&schlegel2021general. Unfortunately, finding sufficient evidence for this hypothesis is difficult, and likely future systems will need to leverage both predictive and memory based approaches (i.e. RNNs).  I believe the creation and study of approaches for state construction leveraging predictions will lead to a more nuanced understanding of what kinds of state are useful for agents. The following hypothesis emphasizes what explicit predictive representations may bring to state learning:

This dissertation contributes to the problem of searching for agent-state update functions through careful empirical evaluation of current architectures and the development of new algorithmic and architectural approaches to agent-state construction. In this thesis, I use gradients and predictions (in the form of general value functions) to inform the search process, and investigate some of the underlying principles of using recurrent networks to form the basis of a history summary mechanism in reinforcement learning. Finally, I develop deeper intuitions in using predictions as the primary driver of summarizing histories, leading to several algorithmic and architectural improvements in this line of work.

# ** Objective

The thesis seeks to explore solution methods and architectures for discovering agent-state through gradients and other methods. Particularly, we seek to explore the consequences of the thesis statement:
#+BEGIN_QUOTE
The architectural intuitions and assumptions developed in supervised learning for agent-state search are limiting in the reinforcement learning and continual reinforcement learning settings.
#+END_QUOTE
and begin to ask how recurrent networks might need to be adjusted for the reinforcement learning setting. The answer to this question has many facets, and we seek conclusive observations and evidence through a slow study of these assumptions in different conditions for a reinforcement learning agent. The approach taken in this thesis centers on clearly formulating various architectures and algorithms for state search and developing in-depth investigative experiments to uncover the dynamics of both predictive and non-predictive approaches. All work presented here uses gradient descent--using truncated BPTT (either through experience replay or full online systems) to estimate gradients--to clearly test different choices and architectures using a shared learning platform.

** Contributions


In this section, I outline the specific contributions made to the field of machine intelligence and reinforcement learning to satisfy the requirements of the doctoral degree at the University of Alberta.

1. Developing and empirically validating recurrent cells which incorporate action into their agent-state update functions (Chapter ref:chap:arnn).
2. Developing a predictive approach to learning agent-state update functions through learning answer to predictive questions posed by General Value Functions (Chapters ref:GVFNs, ref:chap:gvfn:algs, ref:chap:gvfn:empirical, ref:chap:gvfn:discovery).
3. Deriving a gradient algorithm, from which several learning rules are generated, for learning GVFNs.
4. Empirically validating the GVFN approach, comparing to other auxiliary task approaches in the prediction setting, and developing intuitions on suggested predictive questions to use.
5. Outlining a baseline discovery algorithm for generating questions through experience online in GVFNs.
6. Investigating and developing grounding for the targets of composite GVF questions ref:chap:composite.
7. Contributing to learning predictions off-policy more efficiently in the deep learning setting through the use of importance resampling ref:chap:resampling.




# 1. Discuss prediction learning off-policy in light of the experience replay buffer. Investigate the first application of resampling in off-policy learning, and think about its consequences for future development.
# 2. Explore in detail directly using gradients in recurrent networks to discover state. Specifically, empirically testing various architectures for incorporating actions into the state. (justify these as different??)
#    - Layout open questions and problems in learning in partially observable domains and specific solution problems in recurrent learning.
#    - Make a recommendation on a network change for reinforcement learning applications.
#    - Deep investigative experiments uncovering what the agent state dynamics look like, make recommendations for future work in understanding recurrent agents.
# 3. Explore one direction layed out in the open problems, specifically encoding the state of a recurrent network as GVFs.
#    - Develop the GVFN approach and connect it to predictive representation of state literature.
#    - Develop an extension to the gradient TDN objective for GVFNs.
#    - Propose a baseline discovery approach for finding GVFs.


# Another way of saying this succinctly:
# - Applied the importance re-sampling technique in learning predictions in the reinforcement learning.
# - Extensive empirical analysis of various recurrent architectures for incorporating action.
# - Formulating and empirically evaluating a novel predictive state representation, general value function networks (GVFNs), to learn long-temporal dependencies. The first comparison of a TDNet style architecture to a basic recurrent architecture.

** Graveyard                                                      :noexport:
*** DONE [#B] What is my thesis statement now?
CLOSED: [2022-09-06 Tue 13:59]
The proposal is centered on what GVFs can bring to the table in terms of learnability in recurrent networks. Now we want to incorporate RNNs more into the discussion. What should we do?
- Focus on understanding: The goal of my work generally is to understand. What are RNNs brining to the table, what are GVFNs brining to the table. Are they compatible?
- partial observability
- some History of RNNs in RL/online data.
- some History of pred reps.
- some History of perception.
*** What Am I writing the document about?

This document is primarily about partial observability in reinforcement learning.

Why focus on partial observability?

State Construction is...?
- Levels of state construction:
  - Reactive/low-level state vs abstractions for state?
  - What do we want to learn in a state? -> We don't know!
  - There isn't a clear set of criteria for determining what makes for a good state in reinforcement learning
    - Separability? Good Representations properties? Predictive of final task?

- At what abstraction should we be focused?
  - Low level: predictions in the sensor space.
  - High level: predictions/planning in the abstract/concept space.
  - Are these different??

Perception as a series of modules:
- "Is this a face?" much easier than "Is this x's face?"
- The brain is not just one big classification network, submodules are used to specialize. But "how to use submodules" is a hard question.
- Separate the conscious brain from the acting brain.
  - Audio circuit which short circuits the brain to act in the face of a loud noise -> no "control"
  - Other short circuits that bring visual stimuli towards the mid brain for control signals.
- RL is studying the algorithms of the mid brain/cerebellum. We should avoid extending the lessons we learn here to the entire functioning of the brain. In our studies of intelligence we need to be multi-modal. There isn't a single way to conceptualize the concepts, and finding the true underlying properties of the brains algorithms are beyond our capabilities to model mathematically.
- To understand intelligence, we must take the whole embodiment into consideration.

Two philosophies in state building:
- predictive approach
- summaries of histories

Both are valid, this is an exploration of what both bring to the table in terms of state construction and provide ideas for future work.

Ease of use of the history approaches, potential improvement in learnability (as shown in GVFNs, and discussed in the PSR literature).

Methods to deal with partial observability:
- Static histories based approaches
- PoMDPs/Belief States
- PSRs/TDNets
- Recurrent networks
  - RNNs
  - RNNs/models in them
  - TDNets?
  - Predictive state recurrent networks

**** What is my current thinking?
What is the problem:
- Partial observability in an embodied environment?
- Partial observability in an agent based system.
- Taking state construction seriously.
- Retrospective on state construction techniques.
- 

What is the set of solution methods:

*** More structured thinking/outline

- goal of the document is to think about "state construction".
  - Decompose the terms "state" and "construction" in context of the literature
  - Construction is not limited to composing fixed random functions or the schema mechanism.
- Searching and sorting. Q: What are we searching for? A: Something which helps us maximize return.
- What could we want when maximizing reward
  - Markov state?
  - sufficient statistic of the history of observations?
  - core tests -> ability to predict anything?

- Thesis statement: While many authors have proposed different algorithms for state construction, we take the attitude that little is known about how each of these work in prediction and control. This thesis will be focused on understanding and developing on current algorithms for state construction.

- This document is meant to:
  - Explore potential state constructing methods, discuss extensions, propose future research.
  - History based approaches, prediction based approaches
  - Understanding, understanding, understanding. Sensible recommendations for the current state of state construction.
  - What can we do to further the two approaches? What do both give? Problems with both?


What sections do I want to write?
- Introduction (1):
  - What specific research question are we addressing?
- Reinforcement Learning (2)
  - Agent perspective
  - Goal of an agent
  - Parts of an agent
- Predictions (Horde) (3/4)
  - Learning Predictions (resampling)
- Perception and Partial Observability (5)
- Recurrent neural networks in and out of RL (6)
- We have a long way to go in understanding and using rnns in RL (7/8/8.5?)
- Predictive state representations in and out of RL (9)
- Applying GVFs to learn state representations (10/11/12)
- Future Work (13)

* IN-PROGRESS [#A] [6/15] Background

In this thesis, we take the perspective that an agent is situated inside its environment and observes its world from an egocentric perspective continually. While this is not a particularly novel interpretation of the machine intelligence problem, it is worthwhile to clarify the terms we will use throughout intuitively before moving onto formal descriptions. 

The *agent* lives inside the *environment*, which is so large that the agent will never perceive the full set of experience the environment is capable of providing through its lifetime. This is also known as the big world hypothesis {{{citeplease sutton}}}. The agent observes the environment from an egocentric perspective, meaning the agent doesn't observe the full set of hidden properties of the environment and must maintain its current beliefs of the world internally through what is referred to as *agent-state* (we use state and agent-state interchangeably, making sure to emphasize when discussing the environment state where necessary). The environment's dynamics evolve according to internal properties unknown to the agent, and the agent can influence this through its decisions (or actions). On every interaction with the environment the agent observes through its senses and takes an action to maximize a notion of the cumulative sum of rewards.

The *practitioner* is you and me. Specifically, a practitioner is one who is creating the *agent* for an environment. In MI research, the goal is to construct agents which can behave in an environment, usually to accomplish a specific goal set by the practitioner. We seek solutions, algorithms, and systems which can do solve goals with as little imbued assumptions by the practitioner as possible {{{citeplease bitterlesson}}}.
One possible way to accomplish this goal is through setting goals for the agent in terms of reward functions. This form of machine intelligence has been discussed before with a hypothesis termed "Reward is Enough". This hypothesis has been used to form a centralized definition of intelligence and conjectures on how to create such an intelligence through designing reward functions {{{citeplase richintelligence}}} [[cite:&silver2021reward]].

While it is typical for the practitioner to construct a reward function for the agent, the agent can also use internal signals to drive its behavior. This is typically known as intrinsic motivations. In this thesis, we use hand constructed reward functions to gain insights into the algorithms and architectures we explore, but overall we are interested in an agent's ability to predict and control its stream of experience. In all, we make sure our approaches provide better prediction as well as lead to maximizing cumulative reward. Given the right experiences (i.e. behavior in the environment) an agent should be able to accurately make predictions in a computationally constrained way. For more on ways to improve behavior for learning predictions see {{{citeplease mcleod, others}}}.

One common example consistently used in this thesis is that of a small robot on wheels behaving in a room. This is a common example throughout MI and RL research and is also known as the vacuum robot {{{citeplease ModernAI}}}, or lovingly known as the critterbot [[cite:&sutton2011horde;&modayil2014multitimescale;&white2015developing]]. While we don't explore the full robotic setting, we often use simulations which approximate this setting to ask fundamental questions about the learning process.

In the remainder of this section, we provide the relevant general background for reading the rest of the document. This includes background on reinforcement learning (RL) (including off-policy prediction and control), and learning under the constraint of partial observability. Specific background details related to certain solution methods will be presented closer to their relevant sections. 

** DONE Reinforcement Learning
CLOSED: [2023-02-21 Tue 11:50]

The specific problem description considered in this thesis is reinforcement learning (RL). In short, a reinforcement learning agent seeks to maximize a reward signal by acting in the world. In this thesis, we are concerned with two learning problems in reinforcement learning. Specifically, we focus on the model-free prediction and control problem, but each share the same general framework. The agent-environment interaction consists of a stream of data (from the agent's senses), coming in at a consistent rate into the agent's central control systems. In most reinforcement learning, the agent-environment boundary is placed inside the agent's nervous system where parts of the agent's body which are defined through evolution are external to the learning process, and those that are learned and modified through an agent's lifetime are a part of the learning process. This enables RL researchers to focus on the core problem of learning a policy to maximize reward.

More mathematically grounded, the agent observes the sequence \(\obs_1, \action_1, \reward_2, \obs_2, \ldots, \obs_t, \action_t, \reward_{t+1}, \obs_{t+1}, \ldots\) in its lifetime. The observation \(\obs_t\) is the agent's window into the world through various sensing parts of its body. These can include a camera for vision, microphone for audio, lidar to measure distance from other objects, and many other analog-to-digital conversion technologies. The agent then selects an action \(\action_t\) which is passed to the agent's actuators or sub-level control system. By performing this action, the agent receives a reward \(\reward_{t+1}\) and another observation \(\obs_{t+1}\).

The agent-environment interaction can be formalized as a partially observable Markov decision processes (POMDP). The underlying dynamics are defined by a tuple \((\EnvStates, \Actions, \Pmat, f_\obs, \Rewards)\). Given a state \(\envstate \in \EnvStates\) and \(\action \in \Actions\) the environment transitions to a new state \(\envstate^\prime \in \EnvStates\) according to the state transition probability matrix \(\Pmat \defeq \EnvStates \times \Actions \times \EnvStates \rightarrow [0,\infty)\) with a reward given by \(\Rewards \defeq \EnvStates \times \Actions \rightarrow \Reals\). The observations can then be defined as a lossy function over the state \(\obs_t \defeq f_\obs(\envstate_t) \in \Reals^\obssize\), and the reward is \(\reward_t \defeq f_\reward(\envstate_0, \envstate_1, \ldots, \envstate_t) \in \Reals\). This thesis concerns itself primarily with the discrete action setting, where the set of actions is a finite discrete set of values \(\action \in \Actions \defeq [A_1, A_2, \ldots, A_n]\).

The agent has several canonical internal components. A *policy* is a mapping from states to actions \(\pi: \EnvStates \rightarrow \Actions\) and defines a way of interacting with the environment. Most often a policy defines a probability distribution over the space of Actions conditioned on the agent's state \(\pi(a|s)\defeq\text{The probability of selecting action $a$ in state $s$}\). A *value function* is a prediction of the future cumulated (discounted) reward the agent will obtain by following a policy. Specifically,
{{{c}}}
\[
V(\State) = \Expected_\pi[ G_t | s_t = \State, a \sim \pi(\cdot| S)]
\]
{{{c}}}
{{{c}}}
with a state-action value function defined similarly
\[
q(\State, \Action) = \Expected_\pi [ G_t | s_t = \State, a_t = \Action].
\]
This thesis uses both state value functions and state-action value functions to do prediction and control. In the following sections we will extend this framework to the partial observable case, and go into the specifics of the prediction problem and the control problem.

** IN-PROGRESS [1/4] Off-Policy Prediction

*************** DONE Outline and lay foundation for prediction section :noexport:
CLOSED: [2023-02-21 Tue 15:44]
*************** END


The prediction problem in RL is that of learning value functions effectively and efficiently. This process can be used to improve an agent's policy through value iteration {{{CITEPLEASE}}}, or to learn temporal abstractions of the sensorimotor stream through options or general value functions (see Section ref:sec:bg:temporal-abstractions for more details). A value function can be learned either on-policy or off-policy through temporal difference learning. In this section, we will be introducing the on and off-policy prediction problem as used throughout this text. To see a more complete treatment with respect to the deep reinforcement learning setting see Chapter ref:chap:resampling.

As introduced above, a *value function* is a prediction of the future cumulative (discounted) reward received by following a policy \(\tpolicy\),
\[
V_\tpolicy(\State) = \Expected_\pi[ G_t | s_t = \State, a \sim \tpolicy(\cdot| S)]
\]
where \(G_t = \sum_{i=1}^{\infty} \gamma^{i-1} r_{t+i} \) is the return. The operator \(\mathbb{E}_{\tpolicy}\) indicates an expectation with actions selected according to policy $\tpolicy$. GVFs encompass standard value functions, where the cumulant is a reward. Otherwise, GVFs enable predictions about discounted sums of others signals into the future, when following a target policy \(\tpolicy\). These values are typically estimated using parametric function approximation, with weights \(\theta \in \RR^d\) defining approximate values \(\Value_\theta(\state)\). 

The simplest algorithm to learn the value function is through Monte-Carlo sampling. The brief of the algorithm is to get samples of the return starting in state $\State$ following policy $\tpolicy$, which are then averaged to receive the return. This algorithm only requires the environment to be episodic (i.e. clear terminations) and converges to the true value function as the number of samples grows.

*************** TODO Clean up history of learning value functions on-policy :noexport:
*************** END

Another way to learn the value function is by taking advantage of the Bellman equation through dynamic programming.

Both of the above algorithms enforce restrictions on the types of problems addressable. Temporal-difference learning combines advantages of both the above algorithms, alleviating some of constraints imposed.

The off-policy prediction problem is equally concerned with learning value functions of policy $\tpolicy$, but must use data generated from a separate behavior policy $\bpolicy$.

*************** TODO Temporal-difference learning for on-policy prediction :noexport:
*************** END

In off-policy learning, transitions are sampled according to behavior policy, rather than the target policy. 
To get an unbiased sample of an update to the weights, the action probabilities need to be adjusted. Consider on-policy temporal difference (TD) learning, with update \(\alpha_t\delta_t\nabla_\theta \Value_{\theta}(s)\) for a given \(S_t = s\), for learning rate \(\alpha_t \in \RR^+$ and TD-error $\delta_t \defeq C_{t+1} + \gamma_{t+1}\Value_{\theta}(S_{t+1}) -  \Value_{\theta}(s)\). If actions are instead sampled according to a behavior policy \(\bpolicy: \States \times \Actions \rightarrow [0,1]\), then we can use importance sampling (IS) to modify the update, giving the off-policy TD update $\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\theta}(s)$ for IS ratio $\rho_t \defeq \frac{\tpolicy(\actionr_t | \stater_t)}{\bpolicy(\actionr_t | \stater_t)}$.  Given state $\stater_t = \state$, if $\bpolicy(a | s) > 0$ when $\tpolicy(a | s) > 0$, then the expected value of these two updates are equal. To see why, notice that
{{{c}}}
\begin{equation*}
  \mathbb{E}_\mu\left[\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\theta}(s) |S_t = s\right]
  =  \alpha_t\nabla_\theta \Value_{\theta}(s)\mathbb{E}_\mu\left[\rho_t\delta_t |S_t = s\right]
\end{equation*}
which equals $\mathbb{E}_\pi\left[\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\theta}(s) |S_t = s\right]$ because
{{{c}}}
\begin{align*}
\mathbb{E}_\mu\left[\rho_t\delta_t |\stater_t = \state\right] 
% &= \sum_{\action \in \Actions} \mu(\action | \state) \mathbb{E}\left[\rho_t\delta_t |\stater_t = \state, \actionr_t = \action \right]\\ 
&= \sum_{\action \in \Actions} \mu(\action | \state) \frac{\tpolicy(\action | \state)}{\bpolicy(\action | \state)} \mathbb{E}\left[\delta_t |\stater_t = \state, \actionr_t = \action \right]
% &= \sum_{\action \in \Actions} \tpolicy(\action | \state) \mathbb{E}\left[\delta_t |\stater_t = \state, \actionr_t = \action \right] \\
= \ \mathbb{E}_\pi\left[\delta_t |\stater_t = \state\right].
\end{align*}

Though unbiased, IS can be high-variance. A lower variance alternative is Weighted IS (WIS). For a batch consisting of transitions $\{(s_i, a_i, s_{i+1}, c_{i+1}, \rho_i)\}_{i=1}^n$, batch WIS uses a normalized estimate for the update.
For example, an offline batch WIS TD algorithm, denoted WIS-Optimal below, would use update \(\alpha_t \frac{\rho_t}{\sum_{i=1}^n \rho_i} \delta_t\nabla_\theta \Value_{\theta}(s)\). Obtaining an efficient WIS update is not straightforward, however, when learning online and has resulted in algorithms in the SGD setting (i.e. $n=1$) specialized to tabular \citep{precup2001offpolicy} and linear functions cite:&mahmood2014weighted;&mahmood2015off.

*************** TODO Add information on other choices for off-policy prediction learning :noexport:
*************** END


Per decision corrections

Lifetime corrections

Semi-gradient Temporal-difference learning
** DONE Control in Reinforcement Learning
CLOSED: [2023-02-21 Tue 11:50]
The bread and butter problem for reinforcement learning research is the control problem. The control problem is the process of searching (or learning) a policy which the agent can use to decide actions. There are many possible approaches for control in reinforcement learning, from value-based control (through q-learning) to direct policy optimization through policy gradient and actor critic methods. In this thesis, we are primarily concentrated on value-based control as a means to study the perception of reinforcement learning agents (see ref:sec:bg:perception for more details).

We again start with a value function, this time a state-action value function, as defined above
\[
q(\State, \Action) = \Expected_\optpolicy [ G_t | s_t = \State, a_t = \Action].
\]
where \(\optpolicy\) is the optimal policy. The goal of the agent is to search through the space of policies to maximize the total return the agent will receive from any state, or in other words to find the optimal policy \(\optpolicy\). In this thesis, our control experiments are restricted to Q-learning [[cite:&watkins1992qlearning;&mnih2015humanlevel]], an off-policy technique which learns the optimal policy. Q-learning, in its simplest form, is defined by the following set of updates
\begin{align*}
\delta_{t+1} &= Q_\theta (S_t, A_t) - (R_{t+1} + \gamma \max_a (Q_\theta(S_{t+1}, A_{t+1}))) \\
\Delta \theta &= \delta_{t+1} \nabla_\theta Q_\theta(S_t, A_t)
\end{align*}
See Sections ref:sec:bg:func-approx and ref:sec:bg:perception for details on how to apply this method when using deep learning function approximation and recurrent neural networks respectively.

** IN-PROGRESS [3/4] Perception and Partial Observability in Reinforcement Learning
:PROPERTIES:
:CUSTOM_ID: sec:bg:perception
:END:

We consider a partially observable setting, where the observations are a function of an unknown, unobserved underlying state.
The dynamics are specified by transition probabilities \(\Pfcn = \States \times \Actions \times \States \rightarrow [0,\infty)\) with state space \(\States\) and action-space \(\Actions\). On each time step the agent receives an observation vector \(\obs_t \in \Observations \subset \Reals^\obssize\), as a function \(\obs_t = \obs(\state_t)\) of the underlying state \(\state_t \in \States\). The agent only observes \(\obs_t\), not \(\state_t\), and then takes an action \(\action_t\), producing a sequence of observations and actions: \(\obs_{0}, a_{0}, \obs_{1}, a_1, \ldots\).

We define \(\Hist\) to be the minimal set of histories, that enables the Markov property for the distribution over next observation
{{{c}}}
{{{c}}}
\begin{equation}
\!\Hist = \left\{ \hvec_t \!=\! (\obs_0, a_0, \ldots, \obs_{t-1}, a_{t-1}, \obs_t) \ | \ \substack{\text{(Markov property)} \Pr(\obs_{t+1} | \hvec_t, a_t ) = \Pr(\obs_{t+1} | \obs_{-1} a_{-1} \hvec_t a_t), \\ \text{ (Minimal history) }   \Pr(\obs_{t+1} | \hvec_t ) \neq \Pr(\obs_{t+1} | \obs_1, a_1, \ldots, a_{t-1}, \obs_t )} \right\}
\end{equation}
{{{c}}}
The goal for the agent under partial observability is to identify a state representation \(\svec_t \in \RR^\numgvfs\) which is a sufficient statistic (summary) of history \(\Hist\), for targets \(y_t\). More precisely, such a /sufficient state/ ensures that \(y_t\) given this state is independent of history \(\hvec_t = \obs_0, a_{0}, \obs_1, a_1, \ldots, \obs_{t-1}, a_{t-1}, \obs_{t}\),
{{{c}}}
{{{c}}}
\begin{equation}
  p(y_{t} | \svec_t) = p(y_{t} | \svec_t, \hvec_t)
\end{equation}
{{{c}}}
{{{c}}}
or so that statistics about the target are independent of history, such as \(\mathbb{E}[Y_{t} | \svec_t] = \mathbb{E}[Y_{t} | \svec_t, \hvec_t]\).
Such a state summarizes the history, removing the need to store the entire (potentially infinite) history.

For a machine intelligent system with an egocentric perspective, sufficiently summarizing the history of interactions is critical to success in its lifetime. While a unique state can be defined as the set of all histories which induce the same predictions over all futures {{{citeplease}}}, an agent only has a single lifetime {{{citeplease markring}}} and must make due without living through multiple histories. Awash in the stream of sensor readings available to the agent, it is not always clear what regularities are important for the agent to capture for long-term success in the world or how to capture such regularities. Many other constrained definitions of a sufficient summary of state exists [[cite:&subramanian2022approximate]]. Many approaches focus on the capability of the agent to predict the reward function to develop a policy of behavior, but this approach might not be sufficient if its rewards are non-stationary or if the agent has multiple goals. Another idea is the agent's beliefs should be grounded in predictions about the sensorimotor stream directly. This includes predictions about the prescribed reward function, but also encompasses all real-valued signals for which the agent has access.

*** DONE [1/1] Learning Long-temporal Dependencies
:PROPERTIES:
:CUSTOM_ID: sec:bg:perception:tempdepend
:END:

*************** DONE edit LLTD section :noexport:
CLOSED: [2023-02-21 Tue 15:48]
*************** END


Learning long-temporal dependencies is the primary concern of both RL and SL applications of recurrent networks. While great work has been done to coalesce around a few potential architectures and algorithms for SL settings, these are often found lacking in the online-incremental RL context cite:&sodhani2020training;&rafiee2022eyeblinks;&schlegel2021general. 
# discussed in section \ref{sec:open_problems}.
Not only do agents need to learn from the currently stored data (i.e. in an experience replay buffer), they must also continually incorporate the newest information into their decisions (i.e. update online and incrementally). The importance of learning state from an online stream of data has been heavily emphasized in the past through predictive representations of state cite:&littman2002predictive, temporal-difference networks [[cite:&sutton2005temporaldifference]] and GVF networks [[cite:&schlegel2021general]], and in modeling trace patterning systems [[cite:&rafiee2022eyeblinks]]. From a supervised learning perspective, several problems like saturating capacity and catastrophic forgetting are cited as the most pressing for any parametric continual learning system [[cite:&sodhani2020training]]. Below we suggest a few alternative directions needing further exploration in the RL context.

The current standard in training recurrent architectures in RL is truncated BPTT. This algorithm trades off the ability to learn long-temporal dependencies with computation and memory complexity. Currently, the system designer must set the length of temporal sequences the agent needs to model (as would be needed for truncated BPTT to be effective [[cite:&mozer1995focused;&ke2018sparse;&tallec2018unbiased;&rafiee2022eyeblinks]]). Setting this length is a difficult task, as it interacts with the underlying environment and the agent's exploration strategy
# (see section \ref{sec:open_problems} for more details).
As the truncation parameter increases it is known that the gradient estimates become wildly variant [[cite:&pascanu2013difficulty;&sodhani2020training]], which can make learning slow.

An alternative to (truncated) BPTT is real time recurrent learning (RTRL) cite:&williams1989learning. Unfortunately RTRL is known to suffer high computational costs for large networks. Several approximations have been developed to alleviate these costs [[cite:&tallec2018unbiased;&mujika2018approximating]], but these algorithms often struggle from high variance updates making learning slow. The approximation to the RTRL influence matrix proposed by cite:&menick2020practical shows significant promise in sparse recurrent networks, even outperforming BPTT when trained fully online. citeauthor:&ke2018sparse (citeyear:&ke2018sparse) propose a sparse attentive backtracking credit assignment algorithm inspired by hippocampal replay, showing evidence the algorithm has beneficial properties of both BPTT and truncated BPTT. The focused architecture was often able to compete with the fully connected architecture on length of learned temporal sequence and prediction error on several benchmark tasks. Another line of search/credit assignment algorithms is generate and test [[cite:&kudenko1998feature;&mahmood2013representation;&dohare2022continual;&samani2021learning]]. These search algorithms aren't as tied to their initialization as other systems as they intermittently inject randomness into their search to jump out of local minima. Many of these approaches combine both gradient descent and generate and test to gain the benefits of both. While a full generate and test solution is possible, finding the right heuristics to generate useful state objects quickly could be problem dependent.

Learning long-temporal dependencies through regularizing objectives on the state has shown promise in alleviating the need for unrolling the network over long-temporal sequences. citeauthor:&schlegel2021general (citeyear:&schlegel2021general) use GVFs to make the hidden state of a simple RNN predictions about the observations showing potential in lightening the need for BPTT. This approach is sensitive the GVF parameters to use as targets on the state of the network. Predictive state recurrent neural networks [[cite:&downey2017predictive]] combine the benefits of RNNs and predictive representations of state [[cite:&littman2002predictive]] in a single architecture. They show improvement in several settings, but don't explore the model when starved for temporal information in the update. Another approach is through stimulating traces, as shown by [[cite:&rafiee2022eyeblinks]], where traces of observations are used to bridge the gap between different stimuli. Instead of traces, an objective which learns the expected trace [[cite:&hasselt2021expected]] of the trajectory could provide similar benefits as a predictive objective. One can even change the requirements on the architecture in terms of final objectives. [[cite:&mozer1991induction]] propose to predict only the contour or general trends of a temporal sequence, reducing the resolution considerably. Value functions are another object which takes an infinite sequence and reduces resolution to make the target easier to predict [[cite:&sutton1995td;&sutton2011horde;&modayil2014multitimescale;&vanhasselt2015learning]].

It is also possible to reduce or avoid the need for BPTT for modeling long-temporal sequences by adjusting the internal mechanisms of the recurrent architecture. Echo-state Networks [[cite:&jaeger2002adaptive]] are one possible direction. Related to the generate and test idea, echo-state networks rely on a random fixed "reservoir" network, where predictions are made by only adjusting the outgoing weights. Because the recurrent architecture is fixed, no gradients flow through the recurrent connections meaning no BPTT is needed to estimate the gradients. Unfortunately, these networks are dependent on their initializations making them hard to deploy in practice. [[citeauthor:&mozer1995focused]] ([[citeyear:&mozer1995focused]]) propose a focused architecture design, where recurrent connections are made more sparsely (even just singular connections). This significantly reduces the computational complexity of RTRL and allows for a focused version of BPTT.

Transformers [[cite:&vaswani2017attention]] are a widely used alternative to recurrent architectures in natural language processing. Transformers have also shown some success in reinforcement learning but either require the full sequence of observations at inference and learning time [[cite:&mishra2018simple;&parisotto2020stabilizing]] or turn the RL problem into a supervised problem using the full return as the training signal [[cite:&chen2021decision]]. Because of these compromises, it is still unclear if transformers are a viable solution to the state construction problem in continual reinforcement learning.

*** DONE Recurrent Neural Networks
CLOSED: [2023-02-22 Wed 13:17]
:PROPERTIES:
:CUSTOM_ID: sec:bg:rnns
:END:

Recurrent neural networks (RNNs) have been established as an important tool for learning predictions of data with temporal dependencies. They have been primarily used in language and video prediction [[cite:&mikolov2010recurrent;&wang2016largercontext;&saon2017english;&wang2018eidetic;&oh2015actionconditional]], but have also been used in traditional time-series forecasting [[cite:&bianchi2017recurrent]] and RL [[cite:&onat1998recurrent;&bakker2002reinforcement;&wierstra2007solving;&hausknecht2015deep;&heess2015memorybased;&zhu2018improving;&igl2018deep]]. Many specialized architectures have been developed to improve learning with recurrence. These architectures are designed to better learn long temporal dependence and avoid saturation including, Long-short Term Memory units (LSTMs) [[cite:&hochreiter1997long]], Gated Recurrent Units (GRUs)
[[cite:&cho2014properties;&chung2014empirical]], Non-saturating Recurrent Units (NRUs) [[cite:&chandar2019nonsaturating]], and others. Most modern RNN architectures integrate information through additive operations. However, some work has also examined multiplicative updating
[[cite:&sutskever2011generating;&wu2016multiplicative]] which follows from what were known as Second-order RNNs [[cite:&goudreau1994firstorder]].

In this Section we will outline the methods used to estimate gradients in recurrent neural networks in this thesis (Section ref:, discuss three major architectures applied in this thesis, and finally outline how we use recurrent neural networks in the reinforcement learning problem.

For effective prediction and control, the agent requires a state representation \(\agentstate_t\) that is a sufficient statistic of the past: \(\Expected\left[ G^c_t | \agentstate_t \right] = \Expected\left[G^c_t | \agentstate_t, \history_t\right]\). When the agent learns such a state, it can build policies and value functions without the need to store any history. For example, for prediction, it can learn \(V(\agentstate_t) \approx \Expected\left[ G^c_t | \agentstate_t \right]\).


An RNN provides one such solution to learning \(\agentstate_t\) and associated state update function. The simplest RNN is one which learns the parameters \(\weights \in \Reals^\numparams\) recursively
{{{c}}}
\[
  \agentstate_t = \sigma(\weights \xvec_t + \bvec)
\]
{{{c}}}
where \(\xvec_t = [\obs_t, \agentstate_{t-1}]\) and \(\sigma\) is any non-linear transfer function (typically tanh). While concatenating information (or doing additive operations) has become standard in RNNs, another idea explored earlier in the literature and in more modern cells is using multiplicative operations
{{{c}}}
\[
  (\agentstate_t)_i = \sigma\left(\sum_{j=1}^M \sum_{k=1}^N\weights_{ijk} (\obs_t)_j (\agentstate_{t-1})_k + \bvec_i\right) \quad\quad \triangleright \text{ where } \weights \in \Reals^{|\agentstate| \times |\obs| \times |\agentstate| }.
\]
{{{c}}}
Using this type of operation was initially called second-order RNNs [[cite:&goudreau1994firstorder]], and was also explored in one of the first landmark successes of RNNs [[cite:&sutskever2011generating]] in a character-level language modeling task.

There are several known problems with simple recurrent units (and to a lesser extent other recurrent cells). The first is known as the vanishing and exploding gradient problem [[cite:&pascanu2013difficulty]]. In this, as gradients are multiplied together (via the chain rule in BPTT) the gradient can either become very large or vanish into nothing. In either case, the learned networks often cannot perform well and a number of practical tricks are applied to stabilize learning [[cite:&bengio2013advances]]. The second problem is called saturation. This occurs when the weights \(\weights\) become large and the activations of the hidden units are at the extremes of the transfer function. While not problematic for learning stability, this can limit the capacity of the network and make tracking changes in the environment dynamics more difficult [[cite:&chandar2019nonsaturating]].

The experiments presented in this work use three cell types. The first was the simple RNN introduced earlier in this section. The other cells used are Long-short term memory cells (LSTM) [[cite:&hochreiter1997long]], and gated-recurrent units (GRU) [[cite:&chung2014empirical]] which are standard cells used throughout sequence prediction in supervised learning. Long-short term memory cells (LSTM) were developed to address the issues with modeling long-temporal dependencies and the vanishing gradients problem observed in simple RNN cells. Gated-recurrent units (GRU) are a modification from the LSTM cell which maintains performance in many settings, improves ease of use, and improves computational footprint [[cite:&greff2017lstm]]. Many of the observations we make in the following thesis likely generalize beyond the specific cell architecture used.

One issue with RNNs, however, is that training can be unstable and expensive [[cite:&pascanu2013difficulty]]. There are two well-known approaches to training RNNs. The first, Real Time Recurrent Learning (RTRL) \citep{williams1989alearning} relies on a recursive form to estimate gradients. This gradient computation is exact in the offline setting---when RNN parameters are fixed---but only an approximation when computing gradients online. RTRL is prohibitively expensive, requiring computation that is quartic in the hidden dimension size $\statesize$. Low-rank approximations have been developed \citep{tallec2018unbiased,mujika2018approximating,benzing2019optimal} to improve computational efficiency, but these approaches to training RNNs remain less popular than the simpler strategy of back-propagation through time.

RNNs are typically trained through the use of back-propagation through time (BPTT) [[cite:&mozer1995focused]]. This algorithm effectively unrolls the network through the sequence and calculates the gradient as if it was one large network with shared weights. This unrolling is often truncated at some number of steps \(\tau\). While this alleviates computational-cost concerns, the learning performance can be sensitive to the truncation parameter [[cite:&pascanu2013difficulty]], particularly if the dependencies back-in-time are longer than the chosen $p$---as we reaffirm in our experiments.

When calculating the gradients through time for a specific sample, we define our loss as
{{{c}}}
\[
  \mathcal{L}_{t}(\weights) = \sum_{i}^{N} (v_i(\agentstate_t(\weights)) - y_{t, i})^2
\]
{{{c}}}
where \(N\) is the size of the batch, and \(y\) is the target defined by the specific algorithm. This effectively means we are calculating the loss for a single step and calculating the gradients from that step only.

Details for these algorithms can be found in their respective papers, and in textbooks on deep learning. We refer to more specific modifications related to work in this thesis in later sections (see Section ref:sec:gvfn:gradbtt).

*** IN-PROGRESS State Construction through Predictions
   :PROPERTIES:
   :CUSTOM_ID: sec:bg:predreps
   :END:

The idea that an agent's knowledge might be represented as predictions has a long history in machine learning. The first references to such a predictive approach can be found in the work of citeA:&Cunninghambook, citeA:&becker1973model, and citeA:&drescher1991made, who hypothesized that agents would construct their understanding of the world from interaction, rather than human engineering. These ideas inspired work on predictive representations of state [[cite:&littman2002predictive]] and subsequently predictive state representations (PSRs) [[cite:&singh2004predictive]], as an approach to modeling dynamical systems. Simply put, a PSR can predict all possible interactions between an agent and it's environment by reweighting a minimal collection of core test (sequence of actions and observations) and their predictions, without the need for a finite history or dynamics model. Extensions to high-dimensional continuous tasks have demonstrated that the predictive approach to dynamical system modeling is competitive with state-of-the-art system identification methods [[cite:&hsu2012spectral]]. PSRs can be combined with options [[cite:&wolfe2006predictive]], and some work suggests discovery of the core tests is possible [[cite:&mccracken2006online]]. One important limitation of the PSR formalism is that the agent's internal representation of state must be composed exclusively of probabilities of action-observation sequences.

TD networks [[cite:&sutton2005temporaldifference]] were introduced after PSRs, and inspired by the PSR approach to state construction that is grounded in observations. GVFNs build on and are a strict generalization of TD networks. A TD network [[cite:&sutton2005temporaldifference]] is similarly composed of \(\numgvfs\) predictions, and updates using the current observation and previous step predictions like an RNN. TD networks with options [[cite:&rafols2005using]] condition the predictions on temporally extended actions similar to GVF Networks, but do not incorporate several of the recent modernizations around GVFs, including state-dependent discounting and convergent off-policy training methods. The key differences, then, between GVF Networks and TD networks is in how the question networks are expressed and subsequently how they can be answered. GVF Networks are less cumbersome to specify, because they use the language of GVFs. Further, once in this language, it is more straightforward to apply algorithms designed for learning GVFs.
   
Every approach to constructing state with predictions has three core components. The first is how a predictive question is asked or phrased. This can have dramatic changes to the hypothesis/function class of the predictive state, and induce large differences in the underlying algorithmic assumptions used for training. The second is in how the questions will be answered. An approach must consider the base function classes used to represent answers, the abstractions (either temporally or otherwise), and the learning algorithms applied to the architecture. The third, and probably less studied, is that of discovery. Discovery is the automatic specification of predictive questions to use. GVFNs use general value functions (GVFs) to define predictive questions, and a simple recurrent neural network to answer these questions. And algorithmic approach to discovery is still largely unexplored, tied to the discovery of GVFs more broadly, with some efforts applied to a generate-and-test approach cite:&schlegel2021general.
   
Choosing the semantics of how predictive questions are asked will have major effects in the question's discoverability and answerability. PSRs use histories of action observation pairs to construct predictive question, where the answer is a representation of the probability of the sequence of observations being seen given a history and the agent follows the action sequence cite:&littman2002predictive;&singh2004predictive. TDNs use an /answer network/ which is a graph of target dependencies with the core nodes representing specific parts of the observational space. This graph can be many layers, and is acyclic with a single exception. Both TDNs and PSRs were originally defined only using primitive actions to ask questions, but were later extended to included temporally abstract options through option-conditional TDNs cite:&sutton2005temporaldifference;&rafols2006temporal and hierarchical PSRs (HPSRs) cite:&wolfe2006predictive.
# GVFNs are most similar to option-conditional TDNs, using general value functions (GVFs) to define predictive questions. While GVFNs and OCTDNs both can ask the same set of questions, GVFs are a more convenient language to express predictive questions. This representation also makes the modification and application of new methods for learning value functions more straightforward cite:&schlegel2021general, and analysis of the learning dynamics simpler cite:&schlegel2017stable.

The second topic is that of learning and representing the answers of the predictive questions. While respectively different questions, they are deeply connected in the design of any system. The original work in PSRs restricted the sets of observations and actions to be finite. The reason this was needed was how the answers were represented, given a history and sequence of actions for the sequence of observations to not be trivially zero the observations must be sampled according to a mass function. This was addressed in later work using kernel density estimation and information-theoretic tools to realize PSRs in the case of continuous observations and actions cite:&wingate2007discovery. The answers were then represented as a matrix of predicted values for the core tests, which could be updated incrementally with new observations. TDNs use artificial neural networks to underly their representation of answers. While the organization of nodes is not restricted cite:&sutton2005temporaldifference, most of the empirical results shown can be described as using a recurrent neural network. cite:&schlegel2021general make this restriction more apparent, where they explicitly learn the predictive representation as the state of a recurrent network. This simplifies the comparison to non-predictive subjective state approaches (i.e. RNNs), while also enabling the application of backpropagation through time and real-time recurrent learning. In future work, we hope to expand on this simplified network architecture as discussed in chapter \ref{chap:proposal}.

The third and final topic is that of discovery. Discovery is the automatic specification of predictive questions to use in learning the predictive state. PSRs approached discovery by exploring the set of tests to construct a core set that enables all other tests to be answered cite:&james2004learning;&mccracken2005online;&wingate2007discovery.
This objective is trying to find a sufficient statistic of the history for all predictions, and has been discussed in various forms cite:&subramanian2022approximate. We conjecture that finding such a state is not feasible in large complex problems, and searching for such a state would be a poor use of a finite set of computational resources. Instead, the agent should focus on finding a set of questions which is useful for the agents overarching goals---for example, maximizing the return in the control problem.

Along this new objective several other approaches have been proposed. Generate and test is a general algorithm for searching through a large space with opaque dynamics cite:&mahmood2013representation;&javed2020learning. While a reasonable starting algorithm, the lack of heuristic information to guide the search can often be slow cite:&schlegel2021general and possibly infeasible in an agent's lifetime. Another approach is to define the predictive questions as a parametric optimization problem and use meta-gradient descent cite:&bacon2017theoption;&veeriah2019discovery. This approach splits the problem into two optimization problems: an inner problem and an outer problem. The inner optimization consists of the usual control or prediction procedure, where the agent seeks to maximize the discounted return or lower prediction error. The outer optimization calculates gradients through this procedure, with respect to the meta-parameters.

Given a predictive approach to state building requires consideration of these difficult algorithmic choices, a natural question arises ``Why shouldn't we use non-predictive subjective based approaches for learning state, such as the usual recurrent networks?''. While this thesis won't provide (or seek) a conclusive answer to this question, predictive approaches to state construction may have a positive effect on a system's ability to generalize and learn a state representation. This is stated in the /Predictive Representation Hypothesis/ cite:&schaul2013better:

#+begin_quote
  a(n) /(explicit) predictive representation of state/ will be able to continually construct useful generalizations of the regularities in an environment.
#+end_quote

An /(explicit) predictive representation of state/ is an algorithm, or architecture, which constrains the state to be predictions which minimize an objective separate (or jointly) from the agent's general goal in an environment. This class of algorithms includes PSRs, TDNs, GVFNs, and several others. Because the state will be made of small-specific predictive questions of the agent's sensory-motor stream, as the distributions of the underlying dynamics shift the answers to the questions should appropriately shift as well.

Researchers in reinforcement learning, decision making, and artificial intelligence aren't alone in asking if decision making systems use predictions to effectively navigate their world cite:&bubic2010prediction;&hawkins2004intelligence;&clark2013whatever.  Anticipation cite:&butz2003anticipatory;&pezzulo2008challenge --which has similar properties to the GVF approach to prediction--has been used to mean elevated processing prior to an event (also prediction) as well as the overall effect of prediction on an agent behaviour. An agent can anticipate an event in the future, and act accordingly. This requires the agent's policy to be defined in terms of predictions, or for the representation to have predictive/anticipatory properties. Hierarchical predictive coding cite:&rao1999predictive;&huang2011predictive was used to explain non-classical interference observed in the visual cortex. In this approach, feedback connections transport predictions (or priors) from higher layers to lower layers to give context to the current observations. Prospective codes cite:&schutz2007prospective take the theory of prospection and encode future events as representations used for planning and simulation.

While there is evidence to suggest organic decision making systems are directed forward in their representation of the world, memory and ``postdiction'' both play an important, separate role in building a systems underlying representations cite:&soga2009;&synofzik2013. While we focus on two distinct classes in this thesis (i.e. predictive and postdictive), future architectures should be built to take advantage of both approaches.

** IN-PROGRESS Temporal Abstractions in Reinforcement Learning

Reinforcement learning is built on predicting the effect of behavior on future observations and rewards. Many of our algorithms learn predictions of a cumulative sum of (discounted) future rewards, which is used as a bedrock for learning desirable policies. While reward has been the primary predictive target of focus, TD models [[cite:&sutton1995td]] lay out the use of temporal-difference learning to learn a world model through value function predictions. Temporal-difference networks [[cite:&tanner2005thesis;&sutton2005temporaldifference]] take advantage of this abstraction and build state and representations through predictions. [[citeauthor:&sutton2011horde]] ([[citeyear:&sutton2011horde]]) and [[citeauthor:&white2015developing]] (citeyear:&white2015developing) further the predictive perspective by developing a predictive approach to building world knowledge through general value functions (GVFs).

*************** IN-PROGRESS [#A] Fix citations below              :noexport:
*************** END

GVFs have been pursued broadly in reinforcement learning: citeauthor:&gunther2016intelligent (citeyear:&gunther2016intelligent) used GVFs to build an open loop laser welder controller, [[citeauthor:&linke2020adapting]] ([[citeyear:&linke2020adapting]]) and [[citeauthor:&mcleod2021continual]] ([[citeyear:&mcleod2021continual]]) used predictions and their learning progress to develop an intrinsic reward, citeauthor:&edwards2016application (citeyear:&edwards2016application) used GVFs to build controllers for myoelectric prosthetics, using gvfs for auxiliary training tasks to improve representation learning [[cite:&jaderberg2017reinforcement;&veeriah2019discovery]], to extend a value function's approximation to generalize over goals as well as states [[cite:&schaul2015universal]], and to create a scheduled controller from a set of sub-tasks for sparse reward problems [[cite:&riedmiller2018learning]]. Successor representations and features are predictions of the state, learned or given, which have been shown to improve learning performance cite:&dayan1993;&russek2017;&barreto2018;&sherstan2018.

# One component that is thought to be an important part of the space of solutions is the ability for agents to reason in temporally abstract ways.

Two objects in RL which enable agents to reason beyond the moment-to-moment stream of experience are known as *options* and *general value functions* (GVFs). Both of these construct can be wrapped in the framework of GVFs, through the idea of control demons and prediction demons respectively citep:&sutton2011horde. We use this framing here for simplicity, but the literature of options is rich and filled with insights applicable to both the prediction and control framing. 

We introduce GVFs citep:&sutton2011horde directly in the partially observable setting, to use them with RNNs in ref:GVFNs. This was first done in [[cite:&schlegel2021general]]. The first step is to replace state with histories using the definition of history in ref:sec:bg:perception.
{{{c}}}
{{{c}}}
A GVF question is a tuple \((\tpolicy, \cumulant, \gamma)\) composed of a policy \(\tpolicy: \Hist \times \Actions \rightarrow [0, \infty)\), cumulant
\(\cumulant: \Hist \times \Actions \times \Hist \rightarrow \RR\) and continuation function[fn:: The original GVF definition assumed the continuation was only a function of \(H_{t+1}\). This was later extended to transition-based continuation citep:&white2017unifying, to better encompass episodic problems. Namely, it allows for different continuations based on the transition, such as if there is a sudden change from \(\hvec_t\) to \(\hvec_{t+1}\). We use this more general definition for this reason, and because the cumulant itself is already defined on the three tuple \((\hvec_t, a_t, \hvec_{t+1})\).] \(\gamma: \Hist \times \Actions \times \Hist \rightarrow [0,1]\), also called the discount. On time step t, the agent is in \(H_t\), takes actions \(A_t\), transitions to \(H_{t+1}\) and observes[fn:: Throughout this document, unbolded uppercase variables are random variables; lowercase variables are instances of that random variable; and bolded variables are vectors. When indexing into a vector on time step \(t\), such as \(\hvec_t\), we double subscript as \(\hvec_{t,j}\) for the \(j\)th component of \(\hvec_t\).] cumulant \(C_{t+1}\) and continuation \(\gamma_{t+1}\). The answer to a GVF question is defined as the value function, \(V: \Hist \rightarrow \RR\), which gives the expected, cumulative discounted cumulant  from any history \(\hvec_t \in \Hist\). The value function which can be defined recursively with a Bellman equation as
{{{c}}}
{{{c}}}
\begin{align}
  V(\hvec_t) &\defeq \expect*{ C_{t+1} + \gamma_{t+1} V(H_{t+1}) | H_t = \hvec_t, A_{t} \sim \pi(\cdot | \hvec_t)} \label{eq_bewh}\\
  &= \sum_{\action_t \in \Actions} \pi(\action_t | \hvec_t) \sum_{\hvec_{t+1} \in \Hists} \Pr(\hvec_{t+1} | \hvec_t, \action_t) \left[\cumulant(\hvec_t, a_t, \hvec_{t+1}) + \gamma(\hvec_t,a_t,\hvec_{t+1}) V(\hvec_{t+1}) \right] \nonumber
 .
\end{align}
{{{c}}}
The sums can be replaced with integrals if \(\Actions\) or \(\Observations\) are continuous sets. We assume that \(\Hist\) is a finite set, for simplicity; the definitions and theory, however, can be extended to infinite and uncountable sets.

*************** TODO Difference between pred and control demons
*************** END

*Prediction demon vs Control demon*




* IN-PROGRESS [#A] [11/14] Incorporating action into a recurrent network
:PROPERTIES:
:CUSTOM_ID: chap:arnn
:END:

# #+CAPTION: Visualizations of the multiplicative and additive RNNs.
# #+NAME: fig:viz_rnn
# [[./plots/arnns/figures/RNN.pdf]]

*************** TODO [#B] Deal with the appendix from paper
*************** END


In this chapter, I will introduce different mechanisms for incorporating action into a recurrent cell. Some of these mechanisms have been introduced in other parts of the reinforcement learning literature, while some are novel to this thesis. These mechanisms can be applied broadly in any recurrent architecture. In this thesis, I focus on empirically evaluating the difference approaches in simple RNNs and in GRUs, leaving other cells to future work. The goal of this chapter is to bring together these difference mechanisms and perform a rigorous empirical evaluation.


** DONE Introduction
CLOSED: [2023-01-18 Wed 13:57]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:intro
:END:

# Learning to behave and predict using partial information about the world is critical for applying reinforcement learning (RL) algorithms to large complex domains. For example, a deployed automated spacecraft with a faulty sensor that is only able to read signals intermittently. For the spacecraft to stay in service it needs to deploy a learning algorithm to maintain helpful information (or state) about the history of intermittent sensor readings as it relates to the other sensors and how the spacecraft is behaving. A game playing systems such as StarCraft citep:&vinyals2019grandmaster provides another good example. An agent who plays StarCraft must build a working representation of the map, it's base and strategy, and any information about its rival's base and strategy as it focuses its observations on specific locations to perform actions.


# Deep reinforcement learning has expanded the types of problems reinforcement learning can be applied to, specifically those with complex observations from the environment citep:&mnih2015humanlevel;&vinyals2019grandmaster. Significant work has gone into engineering primarily non-recurrent networks citep:&hessel2018rainbow;&espeholt2018impala, while several challenges remain for recurrent architectures in reinforcement learning citep:&hausknecht2015deep;&zhu2018improving;&rafiee2022eyeblinks;&schlegel2021general. There are many design and algorithmic decisions required when applying a recurrent architecture to a reinforcement learning problem. We have a larger discussion on the open-problems for recurrent agents in Section ref:sec:arnn:open-problems.

Recurrent neural networks (RNNs) have been established as an important tool for modeling data with temporal dependencies. They have been primarily used in language and video prediction [[citep:&mikolov2010recurrent;&wang2016largercontext;&saon2017english;&wang2018eidetic;&oh2015actionconditional]], but have also been used in traditional time-series forecasting [[citep:&bianchi2017recurrent]] and RL citep:&onat1998recurrent;&bakker2002reinforcement;&wierstra2007solving;&hausknecht2015deep;&heess2015memorybased. Many specialized architectures have been developed to improve learning with recurrence. These architectures are designed to better model long temporal dependence and avoid saturation including, Long-short Term Memory units (LSTMs) citep:&hochreiter1997long, Gated Recurrent Units (GRUs) citep:&cho2014properties;&chung2014empirical, Non-saturating Recurrent Units (NRUs) citep:&chandar2019nonsaturating, and others. Most modern RNN architectures integrate information through additive operations. However, some work has also examined multiplicative updating citep:&sutskever2011generating;&wu2016multiplicative which follows from what were known as Second-order RNNs citep:&goudreau1994firstorder.

One important design decision is the strategy used to incorporate action in the state update function which can have a large impact on the agent's ability to predict and control (see Figure ref:fig:arnn:ring-world-example). This has been noted before, cite:&zhu2018improving provides a discussion on the importance of these choices developing an architecture which encodes the action through several layers before concatenating with the observation encoding. Other types of action encodings have been used for the state update in RNNs for RL citep:&schaefer2007recurrent;&zhu2018improving;&schlegel2021general, but without an in-depth discussion or focus on the ramifications of the particular choice of architecture.  In other cases, action has seemingly been omitted citep:&oh2015actionconditional;&hausknecht2015deep;&espeholt2018impala. Other state construction approaches also see action as a primary component, predictive representations of state encode predictions as the likelihood of seeing action-observation pairs given a history citep:&littman2002predictive.

#+CAPTION: Learning Curves for various RNN cells in Ring World using experience replay and three strategies to incorporate action into an RNN. The agent learns 20 GVF predictions for 300k steps and we report root mean squared value error averaged over 50 runs with \(95\%\) confidence intervals with window averaging over 1000 steps. See Section \ref{sec:learnability} for full details.
#+NAME: fig:arnn:ring-world-example
[[./plots/arnns/figures/ringworld_example_lc.pdf]]

Action plays an important role in perception in cognitive sciences. [[citeauthor:&noe2004action]] ([[citeyear:&noe2004action]]) proposed that perception is dependent on the actions we can take and have taken on the world around us. In effect, one can look at the objective of a reinforcement learning agent as the desire to control and predict the experience (or data) stream, which inevitably means we must model our agency on the data stream. Action has also played an important part in understanding representations (or codings) in the brain through common coding citeauthor:&prinz1990common (citeyear:&prinz1990common), and in the larger interplay between prediction and action in the brain citep:&clark2013whatever. While the RNN architecture is not exactly reminiscent of these cognitive models, the role of action in perception further motivates the need to study the role action plays in an RL agent's perceptual system more in-depth.

In this Chapter, we focus on several architectures for incorporating action into the state-update function of an RNN in partially observable RL settings. Many of these architectures have been proposed previously for recurrent architectures (i.e. cite:&zhu2018improving;&schlegel2021general), and others are either related to or obvious extensions of those architectures. We perform an in-depth empirical evaluation on several illustrative domains, and outline the relationship between the domain and architectures. Finally, we discuss future work in developing recurrent architectures designed for the RL problem and discuss challenges specific to the RL setting needing investigation in the future.

** DONE Problem Setting :noexport:
CLOSED: [2023-01-18 Wed 13:57]

We formalize the agent-environment interaction as a partially observable markov decision processes (POMDP). The underlying dynamics are defined by a tuple \((\States, \Actions, \Pmat, f_\obs, \Rewards)\). Given a state \(\envstate \in \States\) and \(\action \in \Actions\) the environment transitions to a new state \(\envstate\prime \in \States\) according to the state transition probability matrix \(\Pmat \defeq \States \times \Actions \times \States \rightarrow [0,\infty)\) with a reward given by \(\Rewards \defeq \States \times \Actions \rightarrow \Reals\). The agent observes the sequence \(\obs_t, \action_t, \reward_{t+1}, \obs_{t+1}, \action_{t+1}, \ldots\) where the observations are a lossy function over the state \(\obs_t \defeq f_\obs(\envstate_t) \in \Reals^\obssize\), the actions are selected by the agent's current policy \(\action_t \sim \pi(\cdot|\obs_0, \action_0, \ldots, \action_{t-1}, \obs_t) \rightarrow [0, \infty)\), and the reward is \(\reward_t \defeq f_\reward(\envstate_0, \envstate_1, \ldots, \envstate_t) \in \Reals\).

In this paper we perform experiments in two settings: prediction and control. For prediction, general value functions (GVFs) define the targets citep:&sutton2011horde;&white2015developing. A GVF is a tuple containing a cumulant \(c_{t+1} = f_c(o_t, a_t, o_{t+1}, r_{t+1}) \in \Reals\), a continuation function \(\gamma_{t+1} = f_\gamma(o_t, a_t, o_{t+1}) \in [0, 1]\), and a history \(\hvec_t = [\action_0, \obs_1, \action_1, \obs_2, \action_2, \ldots, \obs_t]\) conditioned policy \(\pi(\action_t|\hvec_t) \in [0,\infty)\). The goal of the agent is to learn a value function which estimates the expected cumulative return under \(\pi\), 
\begin{equation*}
\Expected_\pi\left[ G_t^c | H_t = \hvec_t \right] \quad\quad\text{ where } G_t^c \defeq c_{t+1} + \gamma_{t+1} G_{t+1}^c
.
\end{equation*}
{{{c}}}
To estimate the value function we use off-policy semi-gradient TD(0) citep:&sutton1988learning;&tesauro1994tdgammon. For the control setting we learn a policy which maximizes the discounted sum of rewards or return \(G_t \defeq \sum_{i=0}^\infty \gamma^{i} \reward_{i+t+1}\). In this paper, we use Q-learning citep:&watkins1992qlearning to construct an action-value function and take actions according to an epsilon-greedy strategy.

** DONE Constructing State with Recurrent Networks
CLOSED: [2023-01-18 Wed 13:57]

For effective prediction and control, the agent requires a state
representation \(\state_t \in \Reals^\statesize\) that is a sufficient statistic of the past: \( \Expected\left[ G^c_t | \state_t \right] = \Expected\left[G^c_t | \state_t, \hvec_t\right]\). When the agent learns such a state, it can build policies and value functions without the need to store any history. For example, for prediction, it can learn \(V(\state_t) \approx \Expected\left[ G^c_t | \state_t \right]\). In this section, we describe the strategies used in this work to learn state.


An RNN provides one such solution to learning \(\state_t\) and associated state update function. The simplest RNN is one which learns the parameters \(\weights \in \Reals^\numparams\) recursively
\[
  \state_t = \sigma(\weights \xvec_t + \bvec)
\]
where \(\xvec_t = [\obs_t, \state_{t-1}]\) and \(\sigma\) is any non-linear transfer function (typically tanh). While concatenating information (or doing additive operations) has become standard in RNNs, another idea explored earlier in the literature and in more modern cells is using multiplicative operations
\[
  (\state_t)_i = \sigma\left(\sum_{j=1}^M \sum_{k=1}^N\weights_{ijk} (\obs_t)_j (\state_{t-1})_k + \bvec_i\right) \quad\quad \triangleright \text{ where } \weights \in \Reals^{|\state| \times |\obs| \times |\state| }.
\]
Using this type of operation was initially called second-order RNNs cite:&goudreau1994firstorder, and was also explored in one of the first landmark successes of RNNs citep:&sutskever2011generating in a character-level language modeling task.


RNNs are typically trained through the use of back-propagation through time (BPTT) citep:&mozer1995focused. This algorithm effectively unrolls the network through the sequence and calculates the gradient as if it was one large network with shared weights.
This unrolling is often truncated at some number of steps \(\tau\). While this alleviates computational-cost concerns, the learning performance can be sensitive to the truncation parameter citep:&pascanu2013difficulty. When calculating the gradients through time for a specific sample, we follow citep:&schlegel2021general and define our loss as
\[
  \mathcal{L}_{t}(\weights) = \sum_{i}^{N} (v_i(\state_t(\weights)) - y_{t, i})^2
\]
where \(N\) is the size of the batch, and \(y\) is the target defined by the specific algorithm. This effectively means we are calculating the loss for a single step and calculating the gradients from that step only.


There are several known problems with simple recurrent units (and to a lesser extent other recurrent cells). The first is known as the vanishing and exploding gradient problem citep:&pascanu2013difficulty. In this, as gradients are multiplied together (via the chain rule in BPTT) the gradient can either become very large or vanish into nothing. In either case, the learned networks often cannot perform well and a number of practical tricks are applied to stabilize learning citep:&bengio2013advances. The second problem is called saturation. This occurs when the weights \(\weights\) become large and the activations of the hidden units are at the extremes of the transfer function. While not problematic for learning stability, this can limit the capacity of the network and make tracking changes in the environment dynamics more difficult citep:&chandar2019nonsaturating. Because of these issues, several variations on the simple recurrent cell have been developed including the LSTMs, GRUs, and NSRUs. We focus our experiments around the simple recurrent cells (RNNs) and GRUs.


Finally, to improve sample efficiency we incorporate experience replay (ER), a critical part of a deep (recurrent) system in RL citep:&mnih2015humanlevel;&hausknecht2015deep. There are two key choices here: how states are stored and updated in the buffer and how sequences are sampled citep:&kapturowski2019recurrent. We store the hidden state of the cell in the experience replay buffer as apart of the experience tuple. This is then used to initialize the state when we sample from the buffer for both the target and non-target networks. We pass back gradients to the stored state to update them along with our model parameters, see a full discussion in Section ref:sec:arnn:open-problems. We also stored a separate initial state for the beginning of episodes, which was updated with gradients. We slightly differ from the approach taken by cite:&kapturowski2019recurrent, but expect this architectural choice to have little impact on our discussion in this paper. If we sampled the beginning of an episode from the replay we used the most up to date version of this vector to initialize the hidden state. For sampling, we allowed the agent to sample states across the episode. For samples at the end of the episode, we simply use a shorter sequence length than \(\tau\).

** DONE Architectural Designs for Incorporating Action
CLOSED: [2023-01-18 Wed 13:57]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:design
:END:

#+CAPTION: Visualizations of the multiplicative and additive RNNs. The dimensions of the weight matrices use the size of the RNN's state $|s_{t-1}| = n$ and the size of the observation $|o_t| = m$.
#+NAME: fig:arnn:viz-rnn
#+attr_latex: :width 0.8\linewidth
[[./plots/arnns/figures/RNN.pdf]]


In this paper, we define two broad categories for incorporating action into the state update function of an RNN, and discuss various variations on these ideas (see Figure ref:fig:arnn:viz-rnn for a visualization of two main architectures).

*** Additive

The first category is to use an additive operation. The core concept of additive action recurrent networks is concatenating an action embedding as an input into the recurrent cell citep:&schaefer2007recurrent;&zhu2018improving. For example, the update becomes
{{{c}}}
\begin{align*}
  \state_t = \sigma\left( \Wmat^\xvec \xvec_t + \Wmat^\avec \avec_{t-1} + \bvec \right) \tag*{\bf (Additive)}
\end{align*}
{{{c}}} 
{{{c}}} 
where \(\Wmat^\xvec\) and \(\Wmat^\avec\) are appropriately sized weight matrices. This requires no changes to the recurrent cell if the action embedding \(\avec_{t-1} \in \Reals^\actionsize\) if concatenated to the observation vector. In the empirical experiments, the additive update cells use a hand-designed one-hot encoding function as all our domains have discrete actions.


A variant of the additive approach was explored in cite:&zhu2018improving, where they modified the architecture slightly to learn a function of the action input \(\avec_t = f_a(a_t)\). In this paper, we use the label *Deep Additive* for the architecture, where the action encoding function \(f_a\) is a feed-forward neural network. As in their architecture, we concatenate the action embedding with the observation encodings right before the recurrent network. This enables us to focus on the changes in the basic operation rather than enumerating all possible places the action can be concatenated before the recurrent operation.

*** Multiplicative

The second category is inspired by second-order RNNs citep:&goudreau1994firstorder and first appeared as a part of a state update function in cite:&rafols2006temporal, where the observation, hidden state, and action embedding are integrated using a multiplicative operation: 
{{{c}}}
\begin{align*}
  \state_t = \sigma\left(\Wmat \times_2 \xvec_{t} \times_3 \avec_{t-1}\right),  \tag*{\bf (Multiplicative)}
\end{align*}
{{{c}}} 
where \(\Wmat \in \Reals^{|\state_t| \times |\xvec_t| \times |\avec_{t-1}|}\) and \(\times_n\) is the \(n\)-mode product, which we detail in Appendix \ref{app:tensors}. This type of operation is known to expand the types of functions learnable by a single layer RNN citep:&goudreau1994firstorder;&sutskever2011generating, and decreases the networks sensitivity to truncation citep:&schlegel2021general. 

While this type of update has very clear advantages, there is also a tradeoff in terms of number of parameters and potential re-learning depending on the granularity of the action representation. For example, in the Ring World experiment above the RNN cell with additive used 285 parameters with hidden state size of \(15\). The multiplicative version would have used 510 parameters with the same hidden state size. While this doesn't seem like a lot, if we compare what it would be in a domain like Atari (with 18 actions, 1024 inputs, and \(|s_t| = 1024\)) the number of parameters would be ~2 million vs ~38 million respectively. As shown below in the empirical study, the size of the state can be significantly reduced when using a multiplicative update. In any case, it would be worthwhile to develop strategies to reduce the number of parameters, which we discuss next.

*** Reducing parameters of the Multiplicative

The first way we can reduce the number of parameters is by using a low-rank approximation of the tensor operations. Like matrices, tensors have a number of decompositions which can prove useful. For example, every tensor can be factorized using canonical polyadic decomposition, which decomposes an order-N tensor \(\Wmat \in \Reals^{I_1 \times I_2 \times \ldots \times I_N}\) into n matrices as follows
{{{c}}}
\begin{align*}
  \Wmat_{i_1, i_2, \ldots} &= \sum_{r=1}^\factors \lambda_r \Wmat^{(1)}_{i_1, r}  \Wmat^{(2)}_{i_2, r}  \ldots \Wmat^{(N)}_{i_N, r}
\end{align*}
{{{c}}}
where \(\Wmat^{(j)} \in \Reals^{I_j \times \factors}\), \(\lambda_r \in \Reals\) is the weighting for factor \(r\), and \(\factors\) is the rank of the tensor. This is a generalization of matrix rank decomposition and exists for all tensors with finite dimensions, see Appendix \ref{app:tensors} for more details. We can make several simplifications using the properties of n-mode products. Using the  definition of the multiplicative RNN update,
{{{c}}}
\begin{align*}
  \Wmat \times_2 \xvec_t \times_3 \avec_{t-1}
  &\approx \boldsymbol{\lambda} \Wmat^{out} \left(\xvec_t\Wmat^{in} \odot \avec_{t-1}\Wmat^{a}\right)^\trans
     \quad \triangleright \boldsymbol{\lambda}_{i,i} = \lambda_i.  \tag*{\bf(Factored)}
\end{align*}

Previous work explored using a low-rank approximation of a multiplicative operation. A multiplicative update was used to make action-conditional video predictions in Atari citep:&oh2015actionconditional.  This operation also appears in a Predictive State RNN hidden state update citep:&downey2017predictive, albeit it never performed as well as the full rank version. Our low rank approximation is also similar to the network used in cite:&sutskever2011generating, where they mention optimization issues (which were overcome through the use of quasi-second order methods).

*************** TODO [#B] Deal with deep action appendix section
*************** END

Another approach to reducing the number of parameters required---and to reduce redundant learning---by using an action embedding rather than a one-hot encoding. For example, in Pong it is known that only ~5 actions matter. By taking advantage of the structure of the action space we could potentially further reduce the number of parameters required to get these benefits. We explore this architecture briefly in Section \ref{app:sec:deep_action}. While this is an important piece of the puzzle, we do not focus on learning good action embeddings in this paper and leave it to future work.

** DONE Empirical Questions
CLOSED: [2023-01-18 Wed 13:57]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:experiments
:END:


#+caption: The illustrative environments used in Section \ref{sec:learnability} and Section \ref{sec:control} respectively. (*left*) The Ring World environment with 6 states is depicted, where the observation the agent receives is denoted in each of the circles, available actions denoted by the red arrows, and the agent's current location denoted by a double line. (*right*) The base TMaze environments are depicted with the available actions denoted below and labeled according to the Bakker's TMaze and Directional TMaze used in Section \ref{sec:control}.
#+name: fig:arnn:envs
[[./plots/arnns/figures/environments.pdf]]


In the following sections, we set out to empirically evaluate the three operations for incorporating action into the state update function: \textbf{N}o \textbf{A}ction input (``\textbf{NA}''), \textbf{A}dditive \textbf{A}ction (``\textbf{AA}''), \textbf{M}ultiplicative \textbf{A}ction (``\textbf{MA}''), \textbf{Fac}tored (``\textbf{Fac}''), \textbf{D}eep \textbf{A}dditive \textbf{A}ction (``\textbf{DAA}''). We explore all the variants using both standard RNNs and a GRU cell. Our experiments are primarily driven by the main hypothesis that the multiplicative will strictly outperform the other variants, as suggested by cite:&schlegel2021general. To explore this hypothesis we focus on two main empirical questions:
1. How do the different cells affect the properties of the learned value function and internal state of the agent?
2. Are there examples where the other variants outperform the multiplicative variant?


*Question 1:*

There are several properties we are interested in when analyzing the learning capabilities of our agent. First, and most obvious, is prediction error (calculated using root mean squared value error). While error is a reasonable method to compare different architectures, cite:&kearney2019making argue only inspecting error can be misleading in the quality of the prediction. To account for this in our analysis we visually inspect the raw predictions as well to confirm they are reasonably modeling the target returns. With respect to the internal state, we are primarily interested in understanding if there are qualitative differences which lead to differences in prediction quality.

*Question 2:*

The second question is more straightforward than the first, and requires a complete empirical investigation of all the variants on a set of problems with a diverse set of underlying dynamics and characteristics. You can see this question as an extension of the hypothesis implied by Figure \ref{fig:ring_world_example} and cite:&schlegel2021general:
{{{c}}}
\begin{quote}
  The multiplicative update outperforms the other variants in the reinforcement learning setting for both control and prediction.
\end{quote}
{{{c}}}
While we cannot confirm the above hypothesis empirically, if question 2 is affirmed the hypothesis is false. Counter examples for the hypothesis will also lead to more intuitive knowledge about when to apply one of the above variants.


*Other details:*

*************** TODO [#B] Deal with ARNN appendix empirical section
*************** END

In all control experiments, we use an \(\epsilon\)-greedy policy with \(\epsilon=0.1\). All networks are initialized using a uniform Xavier strategy citep:&glorot2010understanding, with the multiplicative operation independently normalizing across the action dimension (i.e. each matrix associated with an action in the tensor is independently sampled using the Xavier distribution). Unless otherwise stated, we performed a hyperparameter search for all models using a grid search over various parameters (listed appropriately in the Appendix \ref{app:emp}). To best to our ability we kept the number of hyperparameter settings to be equivalent across all models, except the factored variants which use several combinations of hidden state size and number of factors. The best settings were selected and reported using independent runs with seeds different from those used in the hyperparameter search, unless otherwise specified. We controlled all the network sizes such that they had an approximately equal number of free parameters. All final network sizes can be found in Appendix \ref{app:emp}.

All experiments were run using an off-site cluster.
In total, for all sweeps and final experiments we used \(\sim 20\) cpu years, which was approximated based off the logging information used by the off-site cluster. 
All of our code is written in Julia citep:&bezanson2017julia, and we use Flux and Zygote as our deep learning and auto-diff backend citep:&innes:2018;&Zygote.jl-2018.

** DONE Investigating Properties of the Predictions and State
CLOSED: [2023-01-18 Wed 13:57]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:learnability
:END:

#+caption: Ring World sensitivity curves of RMSVE over the final 50k steps for CELL (hidden size) *(left)* RNN (15), AARNN (15), MARNN (12), FacRNN (12 [solid] and 15 [dashed]), DARNN (12, \(|\avec|=2\)), and *(right)* GRU (12), AAGRU (12), MAGRU (9), FacGRU (9 [solid] and 12 [dashed]), DAGRU (9, \(|\avec|=10\)). Reported results are averaged over 50 runs with a \(95\%\) confidence interval. FacRNN used factors \(\factors=\{12, 8\}\) respectively, and FacGRU used \(\factors=\{14, 12\}\). All agents were trained over 300k steps.
#+name: fig:arnn:rw-sens
[[./plots/arnns/figures/ringworld_trunc.pdf]]

# \begin{figure}
#   \centering
#   \includegraphics[width=\linewidth]{./plots/arnns/figures/ringworld_trunc.pdf}
#   \caption{Ring World sensitivity curves of RMSVE over the final 50k steps for CELL (hidden size) {\bf (left)} RNN (15), AARNN (15), MARNN (12), FacRNN (12 [solid] and 15 [dashed]), DARNN (12, $|\avec|=2$), and {\bf (right)} GRU (12), AAGRU (12), MAGRU (9), FacGRU (9 [solid] and 12 [dashed]), DAGRU (9, $|\avec|=10$). Reported results are averaged over 50 runs with a $95\%$ confidence interval. FacRNN used factors $\factors=\{12, 8\}$ respectively, and FacGRU used $\factors=\{14, 12\}$. All agents were trained over 300k steps. \vspace{-0.5cm}} \label{fig:rw_sens}
# \end{figure}

We explore the first empirical qeustion by revisiting the Ring World environment, specifically to test model performance with various truncations, and to compare the architecture's learned state. The Ring World, depicted in Figure ref:fig:arnn:envs, consists of a cycle of states with a single state containing an active observation bit, and other states having an inactive observation bit. The agent can take actions moving either clockwise or counter clockwise in the cycle of states. The agent must keep track of how far it has moved from the active bit. For all experiments we use a Ring World with 10 underlying states.

The agent's objective is to learn a total of 20 GVFs with state-termination continuation functions of  \(\gamma \in \{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\}\). When the agent observes the active bit in Ring World (i.e. enters the first state) the predictions are terminated (i.e. \(\gamma = 0.0\)). The GVFs use the observed bit as a cumulant. Half follow a persistent policy of going clockwise and the other follow the opposite direction persistently. The agent follows an equiprobable random behavior policy. The agent updates its weights on every step following a off-policy semi-gradient TD update with a truncation values denoted. We train the agent for \(300000\) steps and averaged over 50 independent runs. We use root mean squared value error (RMSVE) as the core error metric, which is \(\text{RMSVE}_t = \frac{1}{|V(h_t)|} ||V(h_t) - V_{\text{oracle}}(\envstate_t)||_2\), where \(V_{\text{oracle}}\) is a known oracle for the true value function.

*Results:*

We start with a survey over truncation values for all the architectures in Figure ref:fig:arnn:rw-sens. For both the RNN and GRU cells the MA variant performs the best, while the additive performs the worst of the cells which include action information. Interestingly, the factored variants for the GRU perform almost identically, while the FacRNN with a smaller hidden state perform marginally better. All factored variants straddled the performance of the additive and multiplicative updates. The DAAGRU performs similarly to the AAGRU, while the DAARNN fails to learn in this setting. Finally, the MARNN performs the best overall, only needing a truncation value of \(\tau=6\) to learn, which is shorter than the Ring World. We conclude that with the same number of parameters, the operation used to update the state can have a significant effect on the required sequence length and final performance.

#+caption: Ring World predictions of $\text{seed}=62$ for the multiplicative and additive RNNs. Discounts listed with the target policy persistently going counter-clockwise.
#+ATTR_LATEX: :float wrap :width 0.38\textwidth :placement [15]{r}{0.4\textwidth}
#+name: fig:arnn:rw-pred
[[./plots/arnns/figures/ringworld_pred_truth_vert.pdf]]

To ground the prediction error reported, we present two representative examples of the learned predictions for the additive and multiplicative RNNs in Figure ref:fig:arnn:rw-pred. These plots show a single seed (selected as the best for the additive) over a small snippet of time, but are representative of our observations of the general performance for both cells. The multiplicative follows the actual prediction within a small delta being as close to zero error as we should expect, while the additive has many artifacts and other miss-predictions for both the myopic (\(\gamma = 0.0\)) and long-horizon (\(\gamma=0.9\)) predictions. In Figure ref:fig:arnn:rw-ind-lcs, we report all the individual learning curves for the additive and multiplicative.

#+caption: Individual learning curves for the additive (hidden size of 15) and multiplicative (hidden size 12) RNNs in Ring World with truncation $\tau=6$. The plots are smoothed with a moving average with 1000 step window sizes. The gray box denotes the seed used in Figures ref:fig:arnn:rw-pred and ref:fig:arnn:rw-tsne. Overall, we see the multiplicative is quite resilient to initialization, but the distance from zero error in Figure ref:fig:arnn:ring-world-example can be explained by a few bad initializations.
#+name: fig:arnn:rw-ind-lcs
[[./plots/arnns/figures/ringworld_ind_lcs.pdf]]


#+ATTR_LATEX: :width 0.88\linewidth
#+caption: TSNE plots for the additive and multiplicative RNNs for truncation \(\in \{1, 6\}\). Given the learning objective (described in Section ref:sec:arnn:learnability), we would want the state to have 10 distinct clusters for each state of the underlying environment. We should expect the truncation $\tau=1$ to not be able to produce this kind of state for either cell variant. The learning curves correspond to a single seed (seed=62 which is best for the Additive update). The top scatter plots are colored on the underlying state the agent is currently in, the bottom scatter plots are colored based on the previous action the agent took. We initialized TSNE with the same random seed, with max iterations set to 1000, and perplexity set to 30. We present {\bf (top)} additive and {\bf (bottom)} multiplicative update functions.
#+name: fig:arnn:rw-tsne
[[./plots/arnns/figures/tsne_combined_seed_62.pdf]]

#+ATTR_LATEX: :width 0.88\linewidth
#+caption: TSNE plots for the additive and multiplicative RNNs for truncation $\in \{1, 6\}$. Given the learning objective (described in Section ref:sec:arnn:learnability), we would want the state to have 10 distinct clusters for each state of the underlying environment. We should expect the truncation $\tau=1$ to not be able to produce this kind of state for either cell variant. The learning curves correspond to a single seed. The top scatter plots are colored on the underlying state the agent is currently in, the bottom scatter plots are colored based on the previous action the agent took. We initialized TSNE with the same random seed, with max iterations set to 1000, and perplexity set to 30. We present the median seeds for both cells {\bf (top)} additive uses seed=55 and {\bf (bottom)} multiplicative uses seed=67.
#+name: fig:arnn:rw-tsne-median
[[./plots/arnns/figures/tsne_combined_median.pdf]]

*Looking beyond performance:*

A natural question is why might the multiplicative cell perform significantly better than the other cells in this simple setting? One hypothesis is that the multiplicative cell does a better job at separating the histories on action sequence as compared to the additive operation. While this question is difficult to test, we can peer into the learned state of each cell and see if there are qualitative features that appear to help explain the better performance. To do this we take learned agents over different truncation values started using the same seed. After learning (using the same parameters as in Figure ref:fig:arnn:rw-sens) we collect another 1000 steps of hidden states. With these hidden states we use TSNE citep:&maaten2008visualizing to reduce the space of hidden states to two dimensions. The resulting scatter plots for the additive and multiplicative simple RNNs can be seen in Figures ref:fig:arnn:rw-tsne and ref:fig:arnn:rw-tsne-median.

Overall, we observe the additive and multiplicative separate on the previous action equally well, matching our initial hypothesis. While action is important, the additive seems to be hyper-focused on action even as the cell is able to partition on environment state. The multiplicative, on the other hand, is able to cluster the hidden states for various environment states together with only minor separation on action as seen in states 1 and 7. It is possible this is a natural part of th learning process for both the cells, but the multiplicative is able to cluster the states in less samples. If we look at the median performer (seed=55 and seed=67 for the additive and multiplicative respectively) the additive fails to separate on environment state, while the multiplicative looks similarly to the previous seed.

Above, we hypothesized the separation of action faced by the additive agent could have been an artifact of the learning dynamics. To test this hypothesis we created TSNEs for various number of steps in the environment for both the additive and multiplicative. The results can be seen in Figure ref:fig:arnn:tsnes-over-time. For the multiplicative we choose [50000, 75000, 100000, 300000] which shows the major learning milestones of the network. For the additive we choose [50000, 150000, 200000, 500000] which goes beyond the original experiment's sample limits and shows the major milestones when the network separates the histories according to state. For 100000 steps of training for the multiplicative we can see similar properties where the actions taken to get to specific states are quite separated. As the number of samples grow, to 300000, we see the states converging to be mostly clustered together regardless of the action taken. The additive version never sees the states converging, where even after 500000 timesteps the actions are still regarded highly by the network.

\begin{figure}
  \centering
  \begin{subfigure}{\textwidth}
    \includegraphics[width=0.95\linewidth]{plots/arnns/figures/ringworld_tsne_marnn_1_6_time_67.pdf}
    \caption{Multiplicative for $\tau=6$ and seed=67.}
  \end{subfigure}
  
  \begin{subfigure}{\textwidth}
    \includegraphics[width=0.95\linewidth]{plots/arnns/figures/ringworld_tsne_aarnn_1_6_time_62.pdf}
    \caption{Additive for $\tau=6$ and seed=62.}
  \end{subfigure}
  \caption{TSNE plots for multiplicative and additive RNNs for various number of training samples.} \label{fig:arnn:tsnes-over-time}
\end{figure}

** DONE Understanding when Action Encoding Does and Does Not Matter
CLOSED: [2023-01-20 Fri 15:59]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:control
:END:

In this section, we investigate learning behavior in two environments with slightly differing properties. The first domains is called TMaze citep:&bakker2002reinforcement, depicted in Figure ref:fig:arnn:envs, with a size of 10, which was initially proposed to test the capabilities of LSTMs in RL using Q-Learning. The environment is a long hallway with a T-junction at the end. The agent receives an observation indicating whether the goal state is in the north position or south position at the T-junction (which is randomly chosen at the start of the episode). The agent can take actions in the compass directions. On each step the agent receives a reward of -0.1 and in the final transition receives a reward of 4 or -1 depending if the agent was able to remember which direction the goal was in. The agent deterministically starts at the beginning of the hallway. The observation in the first state is \([1, 1, 0]\) if the goal state is located above the agent and \([0, 1, 1]\) if the goal state is below the agent. In the final state of the hallway the agent receives \([0, 1, 0]\) as an observation, and everywhere else the observation is \([1, 0, 1]\).

#+caption: *(left)* Bakker's TMaze box plots and violin plots over the performance averaged over the final $10\%$ with 50 independent runs. Trained over 300k steps with $\tau=10$. All GRUs use a state size 6, while RNNs use a state size 20. The deep additive used an action encoding of $|\avec|=4$. *(right)* Directional TMaze comparison over the performance averaged over the final $10\%$ of episodes with 100 independent runs trained over 300k steps with $\tau=12$ for CELL (hidden size): RNN (30), AARNN (30), MARNN (18), DARNN (25, $|\avec|=15$), GRU (17), AAGRU (17), MAGRU (10), DAGRU (15, $|\avec|=8$).
#+name: fig:arnn:tmazes
[[./plots/arnns/figures/dirtmaze_and_tmaze.pdf]]

# \begin{figure}
#   \centering
#   \includegraphics[width=\linewidth]{./plots/arnns/figures/dirtmaze_and_tmaze.pdf}
#   \caption{{\bf (left)} Bakker's TMaze box plots and violin plots over the performance averaged over the final $10\%$ with 50 independent runs. Trained over 300k steps with $\tau=10$. All GRUs use a state size 6, while RNNs use a state size 20. The deep additive used an action encoding of $|\avec|=4$. {\bf (right)} Directional TMaze comparison over the performance averaged over the final $10\%$ of episodes with 100 independent runs trained over 300k steps with $\tau=12$ for CELL (hidden size): RNN (30), AARNN (30), MARNN (18), DARNN (25, $|\avec|=15$), GRU (17), AAGRU (17), MAGRU (10), DAGRU (15, $|\avec|=8$).} 
# \end{figure}

Our control agents are constructed similarly to those used in the Ring World environment. The agent's network is a single recurrent layer followed by a linear layer. We perform a sweep over the size of the hidden state and learning rates, and selected all variants of a cell type to have the same value. We train our network over 300000 steps with further details reported in appendix ref:app:emp_tm. We report the learned policy's performance over the final \(10\%\) of episodes by averaging the agent success in reaching the correct goal. We report our results using a box and whisker plot with the distribution. The upper and lower edges of the box represent the upper and lower quartiles respectively, with the median denoted by a line. The whiskers denote the maximum and minimum values, excluding outliers which are marked.

Shown in Figure ref:fig:arnn:tmazes (left), all the cells have similar median performance with the GRU (with no action input) performing the best with the least amount of spread. This conclusion is the same across the size of the hidden state, where the multiplicative and factored variants performed poorly (see Appendix ref:app:emp for factored results). While this initially suggests the action embedding is not important beyond our simple Ring World experiment, notice the difference in how the environment's dynamics interact with the agent's action. In the TMaze, the underlying position of the agent is affected by only two of the actions (the East and West action), while the North and South actions only transition to a different state at the very end of the maze. Also, the agent's actions do not affect needs to be remembered, no matter what trajectory the agent sees the meaning of the first observation is always the same. Thus, these results are much less surprising. For example, the multiplicative variants will have to learn the update dynamics multiple times for the North and South actions.

#+caption: Sensitivity curves over number of factors \(\factors\) with standard error for the *(top)* FacRNN (30) and *(bottom)* FacGRU (17). All agents were trained over 300k steps. See Appendix ref:app:emp_dtm for sweeps over different state sizes. We use the data generated by a sweep over the learning rate with 40 runs and compare to the data in figure ref:fig:arnn:tmazes. The red labels on the x-axis indicate when the network has the same number of parameters as the multiplicative.
#+name: fig:arnn:dirtmaze-fac
#+attr_latex: :width 0.6\textwidth
[[./plots/arnns/figures/dirtmaze_fac.pdf]]

# \begin{wrapfigure}[25]{r}{0.4\textwidth}
#   \centering
#   \includegraphics[width=\textwidth]{./plots/arnns/figures/dirtmaze_fac.pdf}
#   \caption{Sensitivity curves over number of factors $\factors$ with standard error for the {\bf (top)} FacRNN (30) and {\bf (bottom)} FacGRU (17). All agents were trained over 300k steps. See Appendix \ref{app:emp_dtm} for sweeps over different state sizes. We use the data generated by a sweep over the learning rate with 40 runs and compare to the data in figure \ref{fig:tmazes}. The red labels on the x-axis indicate when the network has the same number of parameters as the multiplicative.} \label{fig:arnn:dirtmaze-fac}
# \end{wrapfigure}

To better replicate these dynamics in TMaze we add a direction component to the underlying state. For example, many robotics systems must be able to orient and turn to progress in a maze, which we hypothesize actions will be critical for modeling the state.  The agent can take an action moving forward, turning clockwise, or turning counter-clockwise. Instead of the observations only being a function of the position, the agents direction plays a critical role. In the first state, the agent receives the goal observation \([1, 1, 0]\) when facing the wall corresponding to the goal's direction. All other walls have the observation \([0, 1, 0]\), and when not facing a wall the agent receives the observation \([0, 0, 1]\). In DirectionalTMaze the agent is forced to contextualize its observation by the action it takes before or after seeing the observation. We evaluate the state updates using the same settings as in the TMaze with results reported in Figure ref:fig:arnn:tmazes (right).


Now that the agent must be mindful of its orientation, the action again becomes a critical component in learning. We see the multiplicative variants outperforming all other variants in this domain. Without action, the GRU and RNN are unable to learn, and even the additive and deep additive versions are unable to learn in 300000 steps. We also sweep over the number of factors and report the performance compared to the multiplicative and additive variants as shown in Figure ref:fig:arnn:dirtmaze-fac}. We found that as the factors increase, generally the performance increases as well. This matches our expectations, as with increased factors the factored variants should better approximate the multiplicative variances. But there is a tradeoff when adding too many factors, causing performance to decrease substantially. While the factored variant has some interesting properties, we decide to focus the remaining experiments using the base architectures (NA, MA, AA, DA) and report full results with the factored variant in Appendix ref:app:emp.

** DONE Combining Cell Architectures
CLOSED: [2023-01-20 Fri 15:45]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:combining
:END:

# \begin{SCfigure}
#   \includegraphics[width=0.6\linewidth]{./plots/arnns/figures/combo_cell.pdf}
#   \caption{Two variants of combining cells. State size chosen based on procedures of previous environments. ({\bf top}) Performance of success rates ({\bf left}) TMaze with same basic parameters as above for CELL (hidden size): Softmax GRU (6), Cat GRU (6), Softmax RNN (20), Cat RNN (20). ({\bf right}) Directional TMaze with same parameters as above for CELL (hidden size): Softmax GRU (8), Cat GRU (12), Softmax RNN (15), Cat RNN (22). ({\bf Bottom}) Average softmax weights of cells over training with standard error over runs.} \label{fig:arnn:combination}
# \end{SCfigure}

#+caption: Two variants of combining cells. State size chosen based on procedures of previous environments. *(top)* Performance of success rates *(left)* TMaze with same basic parameters as above for CELL (hidden size): Softmax GRU (6), Cat GRU (6), Softmax RNN (20), Cat RNN (20). *(right)* Directional TMaze with same parameters as above for CELL (hidden size): Softmax GRU (8), Cat GRU (12), Softmax RNN (15), Cat RNN (22). *(bottom)* Average softmax weights of cells over training with standard error over runs.
#+name: fig:arnn:combination
#+attr_latex: :width 0.6\textwidth
[[./plots/arnns/figures/combo_cell.pdf]]

In this section, we consider the effects of combining the additive and multiplicative cells through two types of combination techniques. We see these architectures as a minor step toward building an architecture which learns the structural bias currently hand designed.

We combine the hidden state between an additive and multiplicative operation through two techniques. The first is through an element-wise softmax. Both the additive and multiplicative have the same size hidden state (\(\state^a\) and \(\state^m\) respectively), and each element of the hidden states are weighted by
{{{c}}}
\[
  \state_i = \frac{e^{\theta^a_i} \state^a_i + e^{\theta^m_i} \state^m_i}{e^{\theta^a_i} + e^{\theta^m_i}}
\]
{{{c}}}
where \(\boldsymbol{\theta}^a, \boldsymbol{\theta}^m \in \Reals^\statesize\). This should learn which cell to use depending on the structure of the problem. The second combination is through concatenating the two hidden state together \(\state = cat(\state^a, \state^m)\). This gives more room for experts to add more state to the different architectures, but in this work we fix the two architectures to have the same state size.

We compare these combinations to the original architectures in TMaze and Directional TMaze following the same procedure as above. We expect these cells to perform as well as either the additive or the multiplicative (which ever is doing the best in the specific domain). The results can be seen in Figure ref:fig:arnn:combination. Overall, the softmax combination performs similarly or slightly better than the multiplicative version except in the Directional TMaze for the GRUs. In TMaze, concatenating the two states together performed better than the additive and multiplicative cells, but this operation worked slightly worse than the multiplicative in the Directional TMaze. To test the hypothesis that the softmax weighting should emphasize the better cell in a given domain we show the softmax weighting over the training period. For the TMaze the weightings end being approximately equivalent while the Directional TMaze shows a very distinct separation where the multiplicative is weighted significantly more and the additive is continually down-weighted.

** DONE Learning State Representations from Pixels
CLOSED: [2023-01-20 Fri 15:49]

Finally, we perform an empirical study in two environments with non-binary observations. We are particularly interested in whether the recurrent architectures perform comparably when the observation needs to be transformed by fully connected layers, or when the observation is an image. We only use the GRU cells in these experiments. Full details can be found in Appendix ref:app:emp.

The first domain we consider is a version of DirectionalTMaze which uses images instead of bit observations. The agent receives a gray scale image observation on every step of size \(28\times28\). The agent sees a fully black screen when looking down the hallway, and a half white half black screen when looking at a wall. The agent observes an even (or odd) number sampled from the MNIST citep:&lecun2010mnist dataset when facing the direction of (or opposite of) the goal. The  rewards are -1 on every step and 4 or -4 for entering the correct and incorrect goal position respectively. We report the same statistic as in the prior TMaze environments, with the environment size set to 6. Notice the hallway size is smaller and the negative reward is larger, this was to speed up learning for all architectures.

Results for the Image DirectionalTMaze can be seen in Figure ref:fig:arnn:scaling-up}. In this domains, the multiplicative performs quite well, although not as well as in the simple version. The AAGRU is unable to learn in this setting, and the deep additive variant performs slightly better than the additive.

#+caption: *(left)* Image Directional TMaze percent success over the final \(10\%\) of episodes for 20 runs for CELL (hidden size): AAGRU (70), MAGRU (32), DAGRU (45, \(|\avec| = 128\)). Using ADAM trained over 400k steps, \((\tau = 20)\). GRU omitted due to prior performance. *(center) Lunar Lander average reward over all episodes for CELL (hidden size): GRU (154), AAGRU (152), MAGRU (64), DAGRU (152, \(|\avec|=64\)) and \((\tau = 16)$. {\bf (right)} Lunar Lander learning curves over total reward. Ribbons show standard error and a window averaging over 100k steps was used. Lunar Lander agents were trained for 20 independent runs for 4M steps.
#+name: fig:arnn:scaling-up
#+attr_latex: :width \linewidth
[[./plots/arnns/figures/scale.pdf]]

# \begin{figure}
#   \centering
#   \includegraphics[width=\linewidth]{./plots/arnns/figures/scale.pdf}
#   \caption{{\bf (left)} Image Directional TMaze percent success over the final $10\%$ of episodes for 20 runs for CELL (hidden size): AAGRU (70), MAGRU (32), DAGRU (45, $|\avec| = 128$). Using ADAM trained over 400k steps, $(\tau) = 20$. GRU omitted due to prior performance. {\bf (center)} Lunar Lander average reward over all episodes for CELL (hidden size): GRU (154), AAGRU (152), MAGRU (64), DAGRU (152, $|\avec|=64$) and $(\tau) = 16$. {\bf (right)} Lunar Lander learning curves over total reward. Ribbons show standard error and a window averaging over 100k steps was used. Lunar Lander agents were trained for 20 independent runs for 4M steps.}
# \label{fig:scaling_up}
# \end{figure}

** DONE Learning State Representations from Agent-Centric Sensors
CLOSED: [2023-01-23 Mon 12:56]

The second domain is a partially observable version of the LunarLander-v2 environment from OpenAI Gym cite:&brockman2016openai. The goal is to land a lander on the moon within a landing area. Further details and results can be found in Appendix ref:app:emp_ll. To make the observation partially we remove the anglular speed, and we filter the angle \(\theta\) such that it is 1 if \(-7.5 \le \theta \le 7.5\) and 0 otherwise. We report the average reward obtained over all episodes, and learning curves.

As seen in Figure ref:fig:arnn:scaling-up, our findings generalize to this domain as well. The multiplicative variant improves over the factored (see Appendix ref:app:emp, additive, and deep additive variants significantly. In the LunarLander environment the multiplicative learns faster, reaching a policy which receives on average 100 total reward per episode. Both the additive and factored eventually learn similar policies, while the standard GRU seems to perform less well (although not statistically significant from the additive variant). The average return is ~100 less than some of the best agents on this domains. When we look at the individual median curves we see the agent does this well \(50\%\) of the time (see Appendix ref:app:emp). This difference can be explained by the failure start states being more frequent than in the fully observed case.

** DONE Summary and Conclusions
CLOSED: [2023-01-23 Mon 12:57]

This chapter empirically evaluated several strategies for incorporating the previous action into the state update of a recurrent neural network.
The impact of this choice was shown to have a large impact on an RL agent's performance in several environments from several observation types. These empirical results suggest that the multiplicative operation performs the best even when using a smaller state vector, and the factored and the deep additive versions perform marginally better than the additive versions in most domains.

While the multiplicative seems to be the clear winner on the tested domains, it is important to note not all domains require this architecture. One interesting strategy could be to use the softmax combined cells to decide which cell to use in your final architecture (by looking at the softmax weighting). One could also imagine an architecture which is able to learn which cell to use conditioned on the history of the agent (see Section ref:app:sec:learning_bias). Until better architectures for RL are defined this choice is left to system designers. While the additive and deep additive versions under-performed compared to the other encodings, it still out-performed naively using RNNs without action input.

What is apparent in these experiments and other empirical evidence recently gathered on the performance of recurrent architectures in the online setting citep:&rafiee2022eyeblinks;&schlegel2021general is that the methods and architectures developed and utilized by supervised learning might not be suitable for the reinforcement learning problem. This paper uncovered a simple choice can have a large impact, and provides some evidence that the assumptions made in supervised learning might be holding back recurrent architectures in the reinforcement learning setting (see Appendix ref:app:arch_choice for details). Small, focused studies using recurrent agents in controlled experiments will continue to produce insights on the limitations of the base algorithms and continue to inspire future algorithm developments.

* General Value Function Networks


In this chapter, we introduce a predictive state representation approach known as General Value Function Networks. In short, these networks constrain the state of a recurrent network to answer predictive questions in the form of GVFs. In later chapters we will derive a set of learning algorithms for these architectures (Chapter ref:chap:gvfn:algs), and then empirical evaluate these architectures along several dimensions (Chapter ref:chap:gvfn:empirical).

** Representations of State

Most domains of interest are partially observable, where an agent only observes a limited part of the state. In such a setting, if the agent uses only the immediate observations, then it has insufficient information to make accurate predictions or decisions. A natural approach to overcome partial observability is for the agent to maintain a history of its interaction with the world. For example, consider an agent in a large and empty room with low-powered sensors that reach only a few meters. In the middle of the room, with just the immediate sensor readings, the agent cannot know how far it is from a wall. Once the agent reaches a wall, though, it can determine its distance from the wall in the future by remembering this interaction. This simple strategy, however, can be problematic if a long history length is needed [[citep:&mccallum1996learning]].

State construction enables the agent to overcome partial observability, with a more compact representation than an explicit history. Because most environments and datasets are partially observable---in time series prediction, in modeling dynamical systems and in reinforcement learning---there is a large literature on state construction. These strategies can be separated into Objective-state and Subjective-state approaches.

Objective-state approaches specify a true latent space, and use observations to identify this latent state. An objective representation is one that is defined in human-terms, external to the agent's data-stream of interaction. They typically require an expert to provide feature generators or models of the agent's motion and sensor apparatus. Many approaches are designed for a discrete set of latent states, including HMMs citep:&baum1966statistical and POMDPs [[citep:&kaelbling1998planning]].
A classical example is Simultaneous Localization and Mapping, where the agent attempts to extract its position and orientation as a part of the state [[cite:&durrant-whyte2006simultaneous]].
These methods are particularly useful in applications where the dynamics are well-understood or provided, and so accurate transitions can be used in the explicit models. When models need to be estimated or the latent space is unknown, however, these methods either cannot be applied or are prone to misspecification.

The goal of subjective-state approaches, on the other hand, is to construct an internal state only from a stream of experience. This contrasts objective-state approaches in two key ways. First, the agent is not provided with a true latent space to identify. Second, the agent need not identify a true latent state, even if there is one. Rather, it only needs to identify an internal state that is sufficient for making predictions about target variables of interest. Such a state will likely not correspond to objective quantities like meters and angles, but could be much simpler than the true latent state and can be readily learned from the data stream. Examples of subjective-state approaches to state construction include Recurrent Neural Networks (RNNs) [[cite:&hopfield1982neural;&lin1993reinforcement]], Predictive State Representations (PSRs) [[cite:&littman2002predictive]] and TD Networks [[cite:&sutton2005temporaldifference]].

RNNs have emerged as one of the most popular approaches for online state construction, due to their generality and the ability to leverage advances in optimizing neural networks. An RNN provides a recurrent state-update function, where the state is updated as a function of the (learned) state on the previous step and the current observations. These recurrent connections can be unrolled back in time, making it possible for the current RNN state to be dependent on observations far back in time. There have been several specialized activation units crafted to improve learning long-term dependencies, including long short-term memory units (LSTMs) [[cite:&hochreiter1997long]] and gated recurrent units (GRUs) cite:&cho2014properties. PSRs and TD Networks are not as widely used, because they make use of complex training algorithms that do not work well in practice (see [[cite:&mccracken2006online;&boots2011closing]] and [[cite:&vigorito2009temporaldifference;&silver2013gradient]] respectively). In fact, recent work has investigated facilitating use of these models by combining them with RNNs

[[cite:&downey2017predictive;&choromanski2018initialization;&venkatraman2017predictivestate]]. Other subjective state approaches based on filtering can be complicated to extend to nonlinear dynamics, such as system identification approaches [[cite:&ljung2010perspectives]] or Predictive Linear Gaussian models [[cite:&rudary2005predictive;&wingate2006mixtures]].

One issue with RNNs, however, is that training can be unstable and expensive. There are two well-known approaches to training RNNs. The first, Real Time Recurrent Learning (RTRL) [[cite:&williams1989learning]] relies on a recursive form to estimate gradients. This gradient computation is exact in the offline setting---when RNN parameters are fixed---but only an approximation when computing gradients online. RTRL is prohibitively expensive, requiring computation that is quartic in the hidden dimension size \(\statesize\). Low-rank approximations have been developed [[cite:&tallec2018unbiased;&mujika2018approximating;&benzing2019optimal]] to improve computational efficiency, but these approaches to training RNNs remain less popular than the simpler strategy of Back propagation through time (BPTT).

BPTT explicitly computes gradients of the parameters, by using the chain rule back in time, essentially unrolling the recursive RNN computation. This approach requires maintaining the entire trajectory, which is infeasible for many online learning systems we consider here. A truncated form of BPTT (p-BPTT) is often used to reduce the complexity of training, where complexity grows linearly with p: \(O(p \statesize^2)\).
Unfortunately, training can be highly sensitive to the truncation parameters [[cite:&pascanu2013difficulty]], particularly if the dependencies back-in-time are longer than the chosen \(p\)---as we reaffirm in our experiments.

One potential cause of this instability is precisely the generality of RNNs. These systems require expertise in selecting architectures and tuning hyperparameters [[cite:&pascanu2013difficulty;&sutskever2013training]]. This design space can already be difficult to navigate with standard feed-forward neural networks, and is exacerbated by the recurrence that makes the learning dynamics more unstable. Further, it can be hard to leverage domain expertise to constrain the space of RNNs, and so improve trainability. Specialized, complex architectures have been designed for speech recognition [[cite:&saon2017english]] and NLP [[cite:&peters2018deep]]; redesigning such systems for new problems is an onerous task. Many general purpose architectural restrictions have been proposed, such as GRUs and skip connections (see [[cite:&greff2017lstm]] and [[cite:&trinh2018learning]] for thorough overviews). These methods all provide tools to design, and tune, better architectures, but still do not provide a simple mechanism for a non-expert in deep learning to inject prior knowledge.

An alternative direction, that requires more domain expertise than RNN expertise, is to use predictions as auxiliary losses. Auxiliary unsupervised losses have been used in NLP to improve trainability [[cite:&trinh2018learning]]. Less directly, auxiliary losses were used in reinforcement learning [[cite:&jaderberg2017reinforcement]] and for modeling dynamical systems citep:&venkatraman2017predictivestate, to improve the quality of the representation; this is a slightly different but nonetheless related goal to trainability. The use of predictions for auxiliary losses is an elegant way to constrain the RNN, because the system designers are likely to have some understanding of the relevant system components to predict. For the larger goals of AI, augmenting the RNN with additional predictions is promising because one could imagine the agent discovering these predictions autonomously---predictions by design are grounded in the data stream and learnable without human supervision. Nonetheless, the use of predictions as auxiliary tasks provides a more indirect (second-order) mechanism to influence the state variables. In this work, we ask: is there utility in directly constraining states to be predictions?

To answer this question, we need a practical approach for learning RNNs, where the internal state corresponds to predictions. We propose a new RNN architecture, where we constrain the hidden state to be multi-step predictions, using an explicit loss function on the hidden state.
In particular, we use general policy-contingent, multi-step predictions---called General Value Functions (GVFs) [[cite:&sutton2011horde]]---generalizing the types of predictions considered in related predictive representation architectures [[cite:&rafols2005using;&silver2013gradient;&sun2016learning;&downey2017predictive]]. These GVFs have been shown to represent a wide array of multi-step predictions
[[cite:&modayil2014multitimescale]]. In this paper, we develop the objective and algorithm(s) to train these GVF networks (GVFNs).

We then demonstrate through a series of experiments that GVFNs can effectively represent the state and are much more robust to train, allowing even simple gradient updates with no gradients needed
back-in-time. We first investigate accuracy on two time series datasets, and find that our approach is
competitive with a baseline RNN and more robust to BPTT truncation length. We then investigate GVFNs more deeply in several synthetic problems, to determine 1) if robustness
to truncation remains for a domain with long-term dependencies and 2) the impact of the prediction specification---or misspecification---on GVFN performance. We find that GVFNs have consistent robustness properties across problems, but that, unsurprisingly, the choice of predictions do matter, both for improving learning as well as final accuracy. Our experiments provide evidence that constraining states to be predictions can be effective, and raise the importance of better understanding what these predictions should be.

Our work provides additional support for the /predictive representation hypothesis/, that state-components restricted to be predictions about the future result in good generalization [[cite:&rafols2005using]]. Constraining the state to be predictions could both regularize learning---by reducing the hypothesis space for state construction---and prevent the constructed state from overfitting to the observed data and target predictions. To date, there has only been limited investigation into and evidence for this hypothesis.
[[citeauthor:&rafols2005using]] ([[citeyear:&rafols2005using]]) showed that, for a discrete state setting, learning was more sample efficient with a predictive representation than a tabular state representation and a tabular history representation.
[[citeauthor:&schaul2013better]] ([[citeyear:&schaul2013better]]) showed how a collection of optimal GVFs---learned offline---provide a better state representation for a reward maximizing task, than a collection of optimal PSR predictions.
citeA:&sun2016learning showed that, for dynamical systems, constraining state to be predictions about the future significantly improved convergence rates over auto-regressive models and n4sid.
Our experiments show that RNNs with state composed of GVF predictions can have notable advantage over RNNs in building state with p-BPTT, even when the RNN is augmented with auxiliary tasks based on those same GVFs.

** Constraining State to be Predictions
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:constraining
:END:

Let us start in a simpler setting and explain how the hidden units could be trained to be n-horizon predictions about the future. Imagine you have a multi-dimensional time series of a power-plant, consisting of \(d\) sensory observations with the first sensor corresponding to water temperature. Your goal is to make a hidden node in your RNN predict the water temperature in 10 steps, because you think this feature is useful to make other predictions about the future.

This can be done simply by adding the following loss: \((\svec_{t,1} - \xvec_{t+10, 1})^2\). The combined loss \(L_t(\weights)\) on time step \(t\) is
{{{c}}}
{{{c}}}
\begin{equation}
L_t(\weights) \defeq
\ell(\yhat_t, y_t) +  (\svec_{t,1} - \xvec_{t+10, 1})^2
\end{equation}
{{{c}}}
where both \(\yhat_t\) and \(\svec_t\) are implicitly functions of \(\weights\). This loss still encourages the RNN to find a hidden state \(\svec_t\) that predicts \(y_t\) well. There is likely a whole space of solutions that have similar accuracy for this prediction. The second loss constrains this search to pick a solution where the first state node is a prediction about an observation 10 steps into the future. This second term can be seen as a regularizer on the network, specifying a preference on the learned solution. In general, more than one state node---even all of \(\svec_t\)---could be learned to be predictions about the future.

The difficulty in training such a state depends on the chosen targets. For example, long horizon targets---such as 100 steps rather than 10 steps into the future---can be high variance. Even if such a predictive feature could be useful, it may be difficult to learn accurately and could make the state-update less stable. Using n-horizon predictions also requires a delay in the update: the agent must wait 100 steps to see the target to update the state at time \(t\).

We therefore propose to restrict ourselves to a class of prediction that have been shown to be more robust to these issues [[citep:&vanhasselt2015learning;&sutton2011horde;&modayil2014multitimescale]]. This class of predictions correspond to predictions of discounted cumulative sums of signals into the future, called General Value Functions (GVFs). We have algorithms to estimate these predictions online, without having to wait to see outcomes in the future. This property of GVFs is called /independence of span/ citep:&vanhasselt2015learning, meaning learning can be achieved with computation and memory independent of the horizon. Such a property is doubly critical for predictions within an RNN, as it is more likely that we can actually learn these predictions sufficiently quickly to be usable as state. Further, there is some evidence that this class of predictions is sufficient for a broad range of predictions about the future [[citep:&sutton2011horde;&modayil2014multitimescale;&momennejad2018predicting;&banino2018vectorbased;&white2015developing;&pezzulo2008coordinating]], and so the restriction to GVFs does not significantly limit representability. We therefore focus on developing an approach for this class of predictions within RNNs.

** GVF Networks
:PROPERTIES:
:CUSTOM_ID: GVFNs
:END:

In this section, we introduce GVF Networks, an RNN architecture where hidden states are constrained to predict policy-contingent, multi-step outcomes about the future. We first describe GVFs and the GVF Network (GVFN) architecture. In the following section, we develop the objective function and algorithms to learn GVFNs. There are several related predictive approaches, in particular TD Networks, that we discuss in Section [[ref:sec:gvfn:connections]], after introducing GVFNs.

We first need to extend the definition of GVFs citep:&sutton2011horde to the partially observable setting, to use them within RNNs. The first step is to replace state with histories.
We define \(\Hist\) to be the minimal set of histories, that enables the Markov property for the distribution over next observation
{{{c}}}
{{{c}}}
\begin{equation}
\!\Hist = \left\{ \hvec_t \!=\! (\obs_0, a_0, \ldots, \obs_{t-1}, a_{t-1}, \obs_t) \ | \ \substack{\text{(Markov property)} \Pr(\obs_{t+1} | \hvec_t, a_t ) = \Pr(\obs_{t+1} | \obs_{-1} a_{-1} \hvec_t a_t), \\ \text{ (Minimal history) }   \Pr(\obs_{t+1} | \hvec_t ) \neq \Pr(\obs_{t+1} | \obs_1, a_1, \ldots, a_{t-1}, \obs_t )} \right\}
\end{equation}
{{{c}}}
{{{c}}}
A GVF question is a tuple \((\tpolicy, \cumulant, \gamma)\) composed of a policy \(\tpolicy: \Hist \times \Actions \rightarrow [0, \infty)\), cumulant
\(\cumulant: \Hist \times \Actions \times \Hist \rightarrow \RR\) and continuation function[fn:: The original GVF definition assumed the continuation was only a function of \(H_{t+1}\). This was later extended to transition-based continuation citep:&white2017unifying, to better encompass episodic problems. Namely, it allows for different continuations based on the transition, such as if there is a sudden change from \(\hvec_t\) to \(\hvec_{t+1}\). We use this more general definition for this reason, and because the cumulant itself is already defined on the three tuple \((\hvec_t, a_t, \hvec_{t+1})\).] \(\gamma: \Hist \times \Actions \times \Hist \rightarrow [0,1]\), also called the discount. On time step t, the agent is in \(H_t\), takes actions \(A_t\), transitions to \(H_{t+1}\) and observes[fn:: Throughout this document, unbolded uppercase variables are random variables; lowercase variables are instances of that random variable; and bolded variables are vectors. When indexing into a vector on time step \(t\), such as \(\hvec_t\), we double subscript as \(\hvec_{t,j}\) for the \(j\)th component of \(\hvec_t\).] cumulant \(C_{t+1}\) and continuation \(\gamma_{t+1}\). The answer to a GVF question is defined as the value function, \(V: \Hist \rightarrow \RR\), which gives the expected, cumulative discounted cumulant  from any history \(\hvec_t \in \Hist\). The value function which can be defined recursively with a Bellman equation as
{{{c}}}
{{{c}}}
\begin{align}
  V(\hvec_t) &\defeq \expect*{ C_{t+1} + \gamma_{t+1} V(H_{t+1}) | H_t = \hvec_t, A_{t} \sim \pi(\cdot | \hvec_t)} \label{eq_bewh}\\
  &= \sum_{\action_t \in \Actions} \pi(\action_t | \hvec_t) \sum_{\hvec_{t+1} \in \Hists} \Pr(\hvec_{t+1} | \hvec_t, \action_t) \left[\cumulant(\hvec_t, a_t, \hvec_{t+1}) + \gamma(\hvec_t,a_t,\hvec_{t+1}) V(\hvec_{t+1}) \right] \nonumber
 .
\end{align}
{{{c}}}
The sums can be replaced with integrals if \(\Actions\) or \(\Observations\) are continuous sets. We assume that \(\Hist\) is a finite set, for simplicity; the definitions and theory, however, can be extended to infinite and uncountable sets.


A GVFN is an RNN, and so is a state-update function \(f\), but with the additional criteria that each element in \(\svec_t\) corresponds to a prediction---to a GVF.
A GVFN is composed of \(\numgvfs\) GVFs, with each hidden state component \(\svec_{t,j}\) trained such that at time step \(t\), \(\svec_{t,j} \approx \vifunc{j}(\hvec_t)\) for the \(j\)th GVF and history \(\hvec_t\). Each hidden state component, therefore, is a prediction about a multi-step policy-contingent question. The hidden state is updated recurrently as \(\svec_t \defeq f_\weights(\svec_{t-1}, \xvec_t)\) for a parametrized function \(f_\weights\), where \(\xvec_t = [a_{t-1}, \obs_t]\) and \(f_\weights\) is trained so that \(\svec_j \approx \vifunc{j}(\hvec_t)\). This is summarized in Figure [[ref:fig_gvfnsrnns]].

#+name: fig_gvfnsrnns
#+caption: GVF Networks (GVFNs), where each state component $\svec_{t,i}$ is updated towards the return $G_{t,i} \defeq C_{t+1}^{(i)} + \gamma_{t+1}^{(i)} \svec_{t+1,i}$ for the $i$th GVF. The solid forward arrows indicate how state is updated; in fact, the update is the same as a standard RNN. The difference is with the dotted lines, that indicate training. The dotted black arrows indicate the targets for the state components. The dotted red arrows indicate that the target $G_{t,i}$ are created using the observation and state on the next step.
#+attr_latex: :width 0.5\textwidth
[[./plots/gvfn/figures/GVFN_v2.pdf]]

General value functions provide a rich language for encoding predictive knowledge. In their simplest form, GVFs with constant \(\gamma\) correspond to multi-timescale predictions referred to as Nexting predictions citep:&modayil2014multitimescale. Allowing \(\gamma\) to change as a function of state or history, GVF predictions can combine finite-horizon prediction with predictions that terminate when specific outcomes are observed citep:&modayil2014multitimescale.

\begin{figure}
  \centering
  \begin{subfigure}{0.43\textwidth}
    \includegraphics[width=0.8\textwidth]{plots/gvfn/figures/compworld_with_agent.pdf}
  \end{subfigure}
    \caption{ The Compass World: A partially observable grid world with observations of the color directly in front of the agent. \textbf{Actions:} The agent can take the actions Move Forward (one cell), Turn Left, and Turn Right. \textbf{Observations:} The agent observes the color of the grid cell it is facing. This means the agent can only observe a color if it is at the wall and facing outwards. The agent depicted as an arrow would see Blue. In the middle of the world, the agent sees White.  \textbf{Goal:} The agent's goal is to make accurate predictions about which direction it is facing. } \label{fig:compass_world_env}
\end{figure}

To build some intuition, we provide some examples in Compass World. This environment is used in our experiments and depicted in Figure [[ref:fig:compass_world_env]]. Compass World is a grid world where the agent is only provided information about the color directly in front it. This world is partially observable, with all the tiles in the middle having a white observation, with the only distinguishing color information available to the agent at the walls. The actions taken by the agent are to move forward, turn left, or turn right.

In this environment, the agent might want to know if it is facing the red wall. This can be specified as a GVF question: ``If I go forward until I hit a wall, what is the probability I will see red?". The policy is to always go forward. If the current observation is `Red', then the cumulant is 1; otherwise it is zero. The continuation \(\gamma\) is 1 everywhere, except when the agent hits a wall and see a color; then it becomes zero. The sampled return from a state is 1.0 if the agent is facing the Red wall, because going forward will result in summing many zero plus a 1 right before termination. If the agent is not facing the Red wall, the return is 0, because the agent terminates when hitting the wall but only sees cumulants that are zero for the entire trajectory. Because the outcome is deterministic, the probabilities are 1 or 0.

*************** TODO Try and make following paragraph better (GVFNs)
*************** END

The agent could also ask about how frequently it will see Red, within a horizon of about 10 steps. We can obtain an approximation to this question by using a constant continuation of \(\gamma = 0.9\). The intuition for this comes from thinking of \(1-\gamma\) as a success probability for a geometric distribution: the probability of successfully terminating. The mean of this geometric distribution is \(\tfrac{1}{1-\gamma}\)---which in this case is \(\tfrac{1}{1-0.9}= 10\)---provides the expected number of steps until the first success. Recall that termination indicates that a return is cut-off, and so a cumulant is not included in the sum after termination. This probabilistic termination means that even if Red is seen after 10 steps, it will still be included in the return. However, it does indicate its contribution has been significantly decayed. This exponential prediction loses precision, and so the GVF only provides an approximation to this question.

The agent could also also ask if it will see Red, within a horizon of about 10 steps. In this case, the continuation would be \(0.9\) until the agent observed Red, at which point it would become zero (indicating termination). The GVF answer corresponds to a discounted probability of observing Red, with a smaller number if Red is observed further in the future. If the agent always see Red in 1 step from \(\hvec_t\), then it observes \(C_{t+1} = \) 1 and \(\gamma_{t+1} = 0\) and the value is precisely 1. If the agent sees Red in 2 steps from \(\hvec_t\), then \(C_{t+1} = 0, \gamma_{t+1} = 0.9, C_{t+2} = 1\) and \(\gamma_{t+2} = 0\) resulting in a value of \(0.9\). If the agent sees Red in 10 steps from \(\hvec_t\), then the value is \(0.9^9 \approx 0.4\). If just a few more steps into the future, say 15 steps, then the value would be \(0.2\). The magnitudes start to get quite low, indicating that it is less likely to observe Red in this window.

Notice that though we define the cumulants and continuation functions on the underlying (unknown) state \(\hvec_t\), this is a generalization of defining it on the observations. The observations are a function of state; the cumulants and continuations \(\gamma\) that are defined on observations are therefore defined on \(\hvec_t\). In the examples above, these functions were defined using just the observations. More generally, we consider them as part of a problem definition. This means they could be defined using short histories, or other separate summaries of the history. As we discuss in Section [[ref:chap:gvfn:algs]], we can also consider cumulants that are a function of our own predictions or constructed state.

A natural question is how these GVFs are chosen. This problem corresponds to the discovery problem for predictive representations. In this work, we first focus on the utility of this architecture, with simple heuristics or expert chosen GVFs. We briefly discuss simple ideas for discovery in Section ref:chap:gvfn:discovery, but leave a more systematic investigation of the discovery problem to future work.

** A Case Study using GVFNs for Time Series Prediction
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:case-study
:END:


#+caption: *(left)* Example learning curve for MSO with GVFNs and simple RNNs *(right)* The returns for different \(\gamma\), corresponding to GVFs in the GVFN, for a small section of the MSO time series dataset. The dotted red line for \(\gamma = 0\) looks overlayed with the time series plotted in black, but is actually the observation one step in the future.
#+name: fig:gvfn:mg-example
#+attr_latex: :width 0.95\textwidth
[[./plots/gvfn/timeseries/mso_example_comb.pdf]]



# Before discussing the objective and training algorithms for GVFNs, we provide a simple demonstration of their use in a synthetic single-variable time series dataset to build intuition.
GVFNs can be used for time series prediction by simply assuming that a fixed (unknown) policy generates the data. The GVFs within the network are assumed to have this same fixed unknown policy in common, but differ in the pair of continuation and cumulant functions. For a multi-variate time series, one GVF could have a cumulant corresponding to the first entry of the observation on the next time step, and another GVF could use the second entry. Even for a single-variate time series, we can define meaningfully different cumulants. For example, one GVF could correspond to the probability that the observation becomes larger than 1. The cumulant would be zero until this event occurs, at which point it would be 1. In Figure ref:fig:gvfn:mg-example (left) we provide a preliminary result using a GVFN to forecast 12-steps into the future on the single-variate Multiple Super-imposed Oscillator (MSO) time series dataset. We discuss the full empirical set-up in Section ref:sec:gvfn:exp-forecasting, and here simply provide some insights relevant to building intuition for how to use GVFNs.

The GVFN consists of a recurrent, constrained layer of 128 GVFS with \(\gamma\)s spaced linearly in \([0.1,0.97]\) to learn the state. To make predictions, we can additionally add feedforward layers from this recurrent layer; here we add a ReLu layer for additional nonlinearity in the prediction. For comparison, we also include a simple RNN, which similarily uses an additional ReLu layer after its recurrent layer. The prediction target is the observation 12 steps into the future. Both the RNN and GVFN have to wait 12 steps to see the accuracy of their prediction, delaying updates based on the target by 12 steps. The GVFN, however, can use the loss on the state at each step, and so more directly influence the value of states with the most recent observations. Both methods use p-BPTT, with truncation \(p\). With a sufficiently high \(p\), both perform well (see Section ref:sec:gvfn:exp-forecasting for results with many \(p\)). We report the result here for \(p = 1\), where the GVFN already obtains near-optimal performance.

It might be surprising that this simple GVFN, with GVFs only differing in continuation \(\gamma\), can perform well. For time series data, however, such constant \(\gamma\) predictions provide anticipatory information about observations in the future. To see why, we plot the time series as well as returns for \(\gamma\in\{0, 0.75,0.9, 0.96\}\) as dotted lines in Figure ref:fig:gvfn:mg-example (right). These returns reflect the type of information that would be provided by a GVF prediction. At each time point \(t\) on the x-axis, we can see that the smaller \(\gamma\), like \(\gamma = 0.75\) as dotted green, anticipate the observations in a nearby window. If the time series is starting to rise in the near future, then the dotted green starts to rise right now. Returns can thus provide useful predictive information about increases and decreases that are expected to soon appear in the time series. Notice that the magnitude of the returns are approximately equal. For practical use, we want the magnitude of each GVF prediction to be similar, to avoid large differences in magnitude between state variables. With large \(\gamma\), however, the return becomes large and so too does the value function. The standard fix to this is straightforward: each GVF uses a scaled cumulant of \((1-\gamma) o_{t+1}\).

Notice, though, that there is a trade-off between anticipating a cumulant farther into the future and the precision of predictions about the future. Returns with lower continuations predict trends closer to when they occur in the dataset and have higher resolution. Returns with higher continuations anticipate changes further in the future, at the cost of smoothing over the detailed changes in the dataset. By using both lower and higher continuations, we hope to obtain the benefits of both. We further discuss this simple heuristic---GVFs with the same cumulant and varying \(\gamma\)---as a general purpose heuristic in Section ref:chap:gvfn:discovery.
** Connections to Other Predictive State Approaches
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:connections
:END:

The idea that an agent's knowledge might be represented as predictions has a long history in machine learning. The first references to such a predictive approach can be found in the work of citeA:&Cunninghambook, citeA:&becker1973model, and citeA:&drescher1991made, who hypothesized that agents would construct their understanding of the world from interaction, rather than human engineering. These ideas inspired work on predictive state representations (PSRs) [[citep:&littman2002predictive]], as an approach to modeling dynamical systems. Simply put, a PSR can predict all possible interactions between an agent and it's environment by reweighting a minimal collection of core test (sequence of actions and observations) and their predictions, without the need for a finite history or dynamics model. Extensions to high-dimensional continuous tasks have demonstrated that the predictive approach to dynamical system modeling is competitive with state-of-the-art system identification methods [[citep:&hsu2012spectral]]. PSRs can be combined with options [[citep:&wolfe2006predictive]], and some work suggests discovery of the core tests is possible [[citep:&mccracken2006online]]. One important limitation of the PSR formalism is that the agent's internal representation of state must be composed exclusively of probabilities of action-observation sequences.

TD networks [[citep:&sutton2005temporaldifference]] were introduced after PSRs, and inspired by the PSR approach to state construction that is grounded in observations. GVFNs build on and are a strict generalization of TD networks. A TD network [[citep:&sutton2005temporaldifference]] is similarly composed of \(\numgvfs\) predictions, and updates using the current observation and previous step predictions like an RNN. TD networks with options [[citep:&rafols2005using]] condition the predictions on temporally extended actions similar to GVF Networks, but do not incorporate several of the recent modernizations around GVFs, including state-dependent discounting and convergent off-policy training methods. The key differences, then, between GVF Networks and TD networks is in how the question networks are expressed and subsequently how they can be answered. GVF Networks are less cumbersome to specify, because they use the language of GVFs. Further, once in this language, it is more straightforward to apply algorithms designed for learning GVFs.

More recently, there has been an effort to combine the benefits of PSRs and RNNs. This began with work on Predictive State Inference Machines (PSIMs) citep:&sun2016learning, for inference in linear dynamical systems. The state is learned in a supervised way, by using statistics of the future \(k\) observations as targets for the predictive state. This earlier work focused on inference in linear dynamical systems, and did not state a clear connection to RNNs. Later work more explicitly combines PSRs and RNNs citep:&downey2017predictive,choromanski2018initialization, but restricts the RNN architecture to a bilinear update to encode the PSR update for predictive state. In parallel, another strategy to incorporate ideas from PSRs into RNNs, without restricting the RNN architecture, called Predictive State Decoders (PSDs) citep:&venkatraman2017predictivestate. Instead of constraining internal state to be predictions about future observations, statistics about future observations are used as auxiliary tasks in the RNN.

Of all these approaches, the most directly related to GVFNs is PSIMs. This connection is most clear from the PSIM objective \citep[Equation 8]{sun2016learning}, where the goal is to make predictive state match a vector of statistics about future outcomes. There are some key differences, mainly due to a focus on offline estimation in PSIMs. The predictive questions in PSIMs are typically about observations 1-step, 2-step up to \(k\)-steps into the future. To use such targets, batches of data need to be gathered and statistics computed offline to create the targets. Further, the state-update (filtering) function is trained using an alternating minimization strategy, with an algorithm called DAgger, rather than with algorithms for RNNs. Nonetheless, the motivation is similar: using an explicit objective to encourage internal state to be a predictive state.

A natural question, then, is whether the types of questions used by GVFNs provides advantages over PSIMs. Unlike \(k\)-step predictions in the future, GVFs allow questions about outcomes infinitely far into the far, through the use of cumulative discounted sums. Such predictions, though, do not provide high precision about such future events. As motivated in Section ref:sec:gvfn:constraining, GVFs should be easier to learn online. In our experiments, we include a baseline, called a Forecast Network, that uses \(k\)-step predictions as predictive features, to provide some evidence that GVFs are more suitable as predictive features for online agents.

While GVFs provide, in our opinion, a better language to ask complex predictive questions, we should understand what we are giving up when moving away from PSRs and PSIMs in predictive power. In the next two sections we show that the questions used in PSIMs and PSRs can be posed by GVFs not only through standard cumulants, but also through composite GVFs. Below we use composite GVFs to show the potential predictive power of complex networks of GVFs. See Chapter ref:chap:composite for a more detailed exploration into some of the properties of composite predictions.


*** \(k\)-step forecasts using GVFs

To warm up, we first show how to construct k-step forecasts using GVFs and then move to the more complex case of the core-tests in PSRs. The simplest way to make \(k\)-step forecasts with a GVF is to construct a cumulant function that requires the agent to receive \(k\) new observations and then update the forecast through any usual means. While this is reasonable in the offline setting, when training online we would like to update our prediction as soon as possible without waiting for \(k\) timesteps. We can do this trough composite predictions. With myopic discounts (i.e. \(\gamma=0\)) we can create a chain of GVFs such that each GVF in the chain has a cumulant of the prior GVF's prediction on the next time step. The \(k-1th\) GVF in this sequence will be predicting the observation of the first GVF \(k\) steps into the future.

*** Core-tests in Predictive State Representations can be Defined by Composite GVFs

A predictive state representation is made up of a finite set of core-tests \(\{q_1, q_2, \ldots, q_n\}\) and a set of likelihoods indicating the probability of seeing the core-test given the current history \(\Prob(q_1 | H_t)\). The history is constructed as a sequence of observation-actions from the beginning of the agent's lifetime \(\Hist_t = \{o_0 a_0 o_1 a_1 \ldots o_t\}\). Each core-test is a sequence \(q = \{a_0, o_1, a_1, o_2, \ldots, a_{l-1}, o_{l}\}\). Note how the sequence starts with the pair \((a_0, o_1)\), which is different from the usual notation used in the PSR literature. We may simply construct a GVF similarly to the \(k\)-step forecasts which has a cumulant function that incorporates future observation (the entirety of the core-test sequence). If we use an indicator function over this sequence for the 

To construct a single core-test using composite GVFs we can chain a sequence of myopic predictions much like the \(k\)-step forecasts. The first GVF in the chain of GVFs will have a cumulant \(c(o_t, a_t, o_{t+1}) = \indicator(a_t = a_{l-1}, o_{t+1} = o_l)\), where \(\mathbb{1}\) is the indicator function. Given a history \(\Hist_t\), the expected target of this GVF will be \(v^l = \Expected[\indicator(a_t = a_{l-1}, o_{t+1} = o_{l}) \lvert \Hist_t] = \Prob(a_{l-1}, o_l | \Hist_t)\). Now if we chain GVFs together, where subsequent gvfs are chained according to the cumulant function \(c(o_t, a_t, o_{t+1}, \hat{v}_{l-i}) = \indicator(a_t = a_{l-i-1}, o_{t+1} = o_{l-i}) \text{ and }  \hat{v}^{l-i}_{t+1}\) (where the "and" represents a product). As the chain continues the final GVF should represent the likelihood \(\Prob(q | \Hist_t)\) which is the likelihood for a PSR test. We prove the two chain in the following lemma, but the more general case very easily extends from this case.

#+begin_corollary
Given a length 2 sequence of actions and observations \(\mathcal{S} = a_0, o_1, a_1, o_2\) and a history \(\Hist_t\), the probability of seeing the sequence \(\mathcal{S}\) given the history can be represented as two composed value function targets.
#+end_corollary
#+begin_proof

The above corollary can be easily proven through the rules of conditional expectation and probabilities. The probability of seeing a sequence \(\mathcal{S}\) given a history \(\Hist_t\) can be written as
\begin{align*}
\Prob(A_t = a_0, O_{t+1} = o_1, A_{t+1} = a_1, O_{t+2} = o_2 | \Hist_t) = \\ \hspace{2cm} \Expected[\indicator\{A_t = a_0, O_{t+1} = o_1, A_{t+1} = a_1, O_{t+2} = o_2 |\} \Hist_t]
\end{align*}

\noindent where \(\indicator\) is the indicator function. Now we follow the procedure laid out above to build composite GVFs. The first GVF in the chain will have a cumulant \(c_2(o_t, a_t, o_{t+1}) = \indicator\{A_t = a_{1}, O_{t+1} = o_{2}\}\) with a myopic discount (i.e. \(\gamma = 0\)). The resulting value function prediction is
\[
v_2(\Hist_t) = \Expected[\indicator(a_t = a_{1}, o_{t+1} = o_{2}) \lvert \Hist_t] = \Prob(A_t = a_{1}, O_{t+1} = o_{2} \lvert \Hist_t).
\]

The next value function in the chain uses the previous predictions value \(c_1(o_t, a_t, o_{t+1}, H_t) = \indicator\{A_t = a_{0}, O_{t+1} = o_{1} \hat{v}_2(\{\Hist_t A_t, O_{t+1}\}) \} \). The resulting value function will predict

\begin{align*}
v_1(\Hist_t) &= \Expected[\indicator(A_t = a_{0}, O_{t+1} = o_{1}) \Expected[\indicator(A_{t+1} = a_{1}, O_{t+2} = o_{2}) \lvert \Hist_t, A_t, O_{t+1}]  \lvert \Hist_t] \\
&= \sum \indicator(A_t = a_{0}, O_{t+1} = o_{1}) \Prob(A_t, O_{t+1} \lvert \Hist_t) \sum \indicator(A_{t+1} = a_{1}, O_{t+2} = o_{2}) \Prob(A_{t+1}, O_{t+2} \lvert \Hist_t, A_{t}, O_{t+1}) \\
&= \sum \sum \indicator(A_t = a_{0}, O_{t+1} = o_{1}) \indicator(A_{t+1} = a_{1}, O_{t+2} = o_{2}) \Prob(A_t, O_{t+1} \lvert \Hist_t) \Prob(A_{t+1}, O_{t+2} \lvert \Hist_t, A_{t}, O_{t+1}) \\
&= \sum \sum \indicator(A_t = a_{0}, O_{t+1} = o_{1}, A_{t+1} = a_{1}, O_{t+2} = o_{2}) \Prob(A_t, O_{t+1} \lvert \Hist_t) \frac{\Prob(A_{t}, O_{t+1}, A_{t+1}, O_{t+2} \lvert \Hist_t)}{\Prob(A_t, O_{t+1} \lvert \Hist_t)} \\
&= \sum \sum \indicator(A_t = a_{0}, O_{t+1} = o_{1}, A_{t+1} = a_{1}, O_{t+2} = o_{2}) \Prob(A_{t}, O_{t+1}, A_{t+1}, O_{t+2} \lvert \Hist_t) \\
&= \Prob(A_{t} = a_0, O_{t+1}=o_1, A_{t+1}=a_1, O_{t+2}=o_2 \lvert \Hist_t)
\end{align*}




#+end_proof


** Summary

This chapter presented the GVFN predictive representations of state and compares this approach to various types of representations of state including recurrent neural networks, predictive state representations, and TD Networks. This chapter concluded by providing some intuitive constructions of GVFNs corresponding to state in compass world, \(k\)-forecasts, and finally the core-tests of a PSR.

The aim of this chapter was to lay the ground work needed for the next sections, and to discuss the intuitive motivation behind the GVFN approach to state construction. As discussed, GVFNs lay squarely as a subjective state approach, only considering possible state variables which are constructed from general value functions. The motivation behind the GVFN work is extremely similar to that of TD networks and PSRs, except we explicitly consider sufficiency to be tied only to an agent's goals rather than all possible predictions provided by the agent. While this makes intuitive sense from the perspective of searching and learning the agent state, proving viability becomes intractable from these constraints [[cite:&subramanian2022approximate]]. The subsequent chapters will focus on the question of usefulness of such an approach through empirical means.

* Learning Algorithms for GVFNs
:PROPERTIES:
:CUSTOM_ID: chap:gvfn:algs
:END:


In this section, we introduce the objective function for GVFNs, that constrains the learned state to be GVF predictions.
Each state component of a GVFN is a value function prediction, and so is approximating the fixed point to a Bellman equation with history in Equation \eqref{eq_bewh}. The extension is not as simple as using a standard Bellman operator, however, because the GVFs are in a network. In fact, the Bellman equations are coupled in two ways: through composition---where one GVF can be the cumulant for another GVF as seen in section [[ref:sec:gvfn:emp:poorlyspecified]]---and through the parametric recurrent state representation. We first discuss the Bellman network operator in Section [[ref:sec:gvfn:operator]], which extends the typical Bellman operator to allow for composition. We then explain how the coupling that arises from the recurrent state representation can be handled using a projected operator, and provide the objective for GVFNs, called the Mean-Squared Projected Bellman Network Error (MSPBNE), in Section [[ref:sec:gvfn:objective]]. Then we discuss several algorithms to optimize this objective in Section [[ref:sec:gvfn:algs]].


The GVFN objective we introduce can be added to the standard RNN objective, to provide an RNN where the learned states are both useful for prediction of the target and encouraged---or regularized---to be GVF predictions. In this work, we only train GVFNs with the GVFN objective, without including the loss to a target, to focus the investigation on the utility of the proposed objective and on predictive features.

** The Bellman Network Operator
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:operator
:END:

To understand the Bellman network operator, it is useful to first revisit the Bellman operator for learning a single GVF.
We assume the set of histories \(\Hist\) is finite.
# \footnote{It is common to assume finite state spaces when analyzing value functions and defining Bellman operators. Extensions to infinite spaces is possible, but complicates presentation.}
Assume a tabular encoding for the values, \(\vi{j} \in \RR^{|\Hists|}\), for a GVF question \((\pij{j}, \cj{j}, \gammaj{j})\). The Bellman equation in [[ref:eq_bewh]] can be written as a fixed point equation, with Bellman operator
{{{c}}}
\begin{equation}
\Bn^{(j)} \vi{j} \defeq \Cpij{j} + \Ppigammaj{j} \vi{j}
\end{equation}
{{{c}}}
where \(\Cpij{j} \in \RR^{|\Hists|}\) is the vector of expected cumulant values under \(\pij{j}\), with entries
{{{c}}}
\begin{equation}
\Cpij{j}(\hvec_t) \defeq \sum_{a_t \in \Actions} \pij{j}(a_t | \hvec_t) \sum_{\hvec_{t+1} \in \Hists} \Pr(\hvec_{t+1} | \hvec_t, a_t) \cj{j}(\hvec_t, a_t, \hvec_{t+1})
.
\end{equation}
{{{c}}}
and
 \(\Ppigammaj{j} \in \RR^{|\Hists| \times |\Hists| }\) is the matrix of values satisfying
{{{c}}}
\begin{equation}
\Ppigammaj{j}(\hvec_t, \hvec_{t+1}) = \sum_{a_t \in \Actions} \pij{j}(a_t | \hvec_t) \Pr(\hvec_{t+1} | \hvec_t, a_t) \gammaj{j}(\hvec_t, a_t, \hvec_{t+1})
.
\end{equation}
{{{c}}}
If the operator \(\Bn^{(j)}\) is a contraction, then iteratively applying this operator converges to a fixed point. More precisely, if for any \(\vi{j}_1, \vi{j}_{2} \in \RR$, $\| \Bn^{(j)} \vi{j}_1 -  \Bn^{(j)}\vi{j}_2 \| <  \| \vi{j}_1 - \vi{j}_2 \|\), then iteratively applying \(\Bn^{(j)}\), as \(\vi{j}_2 = \Bn^{(j)} \vi{j}_1, \ldots, \vi{j}_{t+1} = \Bn^{(j)} \vi{j}_t, \ldots\), converges to a fixed point.  Because temporal difference learning algorithms are based on this fixed-point update, the Bellman operator is central to the analysis of many algorithms for learning value functions, and is used in the definition of objectives for value estimation.

We can similarly define a Bellman operator that accounts for the relationships between GVFs in the network. Assume there are \(\numgvfs\) GVFs, with \(\vinone \in \RR^{\numgvfs | \Hists |}\) the stacked values for all the GVFs,
{{{c}}}
\begin{equation}
\vinone \defeq \left[\begin{array}{c}
\vi{1}\\
\vdots \\
\vi{\numgvfs}
\end{array}
\right]
.
\end{equation}
{{{c}}}
The cumulants may now be functions of the values of other GVFs; we therefore explicitly write \(\Cpij{j}_{\vinone}\).
The Bellman network operator \(\Bn\) is
{{{c}}}
\begin{equation}
\Bn \vinone
\defeq
\left[\begin{array}{c}
\Cpij{1}_{\vinone} + \Ppigammaj{1} \vi{1}\\
\vdots \\
\Cpij{\numgvfs}_{\vinone} + \Ppigammaj{\numgvfs} \vi{\numgvfs}
\end{array}
\right]
.
\end{equation}
{{{c}}}
The Bellman network operator needs to be treated as a joint operator on all the GVFs because of compositional predictions, where the prediction on the next step of GVF \(j\) is the cumulant for GVF \(i\). When iterating the Bellman operator \(\vi{j}\) is not only involved in its own Bellman equation, but also in the Bellman equation for \(\vi{i}\). Notice that if there were no compositions, the Bellman network operator would separate into individual Bellman operators, that operate on each \(\vi{j}\) independently.

To use such a Bellman network operator, we need to ensure that iterating under this operator converges to a fixed point. For no composition, this result is straightforward, as it simply follows from previous results showing when the Bellman operator is a contraction. We state this explicitly below in Corollary ref:cor:gvfn:main. Under composition, we need to consider the effect of the current value function on the cumulant. Consequently, the operator may no longer be a simple linear projection of the values, followed by a sum of expected cumulants.

We first identify a necessary condition: the connections between GVFs must be acyclic. For example, GVF \(i\) cannot be a cumulant for GVF \(j\), if \(j\) is already a cumulant for \(i\). More generally, the connections between GVFs cannot create a cycle, such as \(1 \rightarrow 2 \rightarrow 3 \rightarrow 1\). We provide a counterexample, where the Bellman network operator is not a contraction when there is a cycle, to illustrate that this condition is necessary.

We further place restrictions on the cumulant, if it is a function of other GVFs. In particular, we require that the cumulant is a Lipschitz function of the other value functions.
Note that this restriction encompasses the setting for a non-compositional GVF, because the cumulant can be a constant w.r.t. these values. It also encompasses the setting we use in our experiments: that each cumulant is a linear function of the GVF values on the next step.

#+ATTR_LATEX: :options [Acyclic Connections]
#+ATTR_HTML: :title Acyclic Connections
#+begin_assumption
The directed graph \(G\) is acyclic. \(G\) consists of \(\numgvfs\) vertices, each corresponding to a GVF, and each directed edge \((i,j)\) indicates that \(j\) is used in the cumulant for \(i\).
#+end_assumption

#+ATTR_LATEX: :options [Lipschitz Compositional Cumulants]
#+ATTR_HTML: :title Lipschitz Compositional Cumulants
#+begin_assumption
If GVF \(i\) has directed edges to \(\{j_1, \ldots, j_k\}\), then the cumulant \(c^{(i)}_{\vinone}(\hvec_{t+1})\) is Lipschitz in \(\vi{j_1}, \ldots, \vi{j_k}\) with Lipschitz constant \(K_i\). That is, for \(\vinone_1,\vinone_2 \in \RR^{\numgvfs | \Hists |}\), \(\| \Cpij{i}_{\vinone_1} - \Cpij{i}_{\vinone_2} \| \le K_i \sum_{l=1}^k \|  \vi{j_l}_1 - \vi{j_l}_2 \|\).
#+end_assumption

Note that this assumption is satisfied if we assume that for some bounded weights \(w_1, \ldots, w_k \in \RR\), the cumulant must satisfy \(c^{(i)}_{\vinone}(\hvec_{t+1}) = \sum_{l=1}^k w_l \vi{j_l}(\hvec_{t+1})\) or equivalently, \(\Cpij{i}_{\vinone} = \sum_{l=1}^k w_l \Ppigammaj{j_l} \vi{j_l}\).  This is because \(\Ppigammaj{j_l}\) is a non-expansion, and so
\begin{align*}
\| \Cpij{i}_{\vinone_1} - \Cpij{i}_{\vinone_2} \|
= \left\| \sum_{l=1}^k w_l \Ppigammaj{j_l} (\vi{j_l}_1 - \vi{j_l}_2) \right\|
 &\le \sum_{l=1}^k | w_l | \| \Ppigammaj{j_l} (\vi{j_l}_1 - \vi{j_l}_2) \|\\
& \le (\max_{l} | w_l | ) \sum_{l=1}^k \| \vi{j_l}_1 - \vi{j_l}_2 \|
.
\end{align*}

The third assumption is standard for showing Bellman operators are contractions, and is easily satisfied if the policy is proper: is guaranteed to visit at least one state where the continuation is less than 1.

#+ATTR_LATEX: :options [Discounted Transitions are Contractions]
#+ATTR_HTML: :title Discounted Transitions are Contractions
#+begin_assumption
For all \(j \in \{1, \ldots, \numgvfs\}\), \(\beta_j \defeq \| \Ppigammaj{j} \| < 1\), where \(\| \cdot \|\) is the spectral norm.
#+end_assumption
{{{c}}}
With these three assumptions, we can prove the main result.
{{{c}}}

#+name thm_main
#+begin_theorem
Under Assumptions 1-3, iterating \(\vt{t+1} = \Bn \vt{t}\) converges to a unique fixed point.
#+end_theorem

#+begin_proof
We first prove that the sequence of value estimates converges (Part 1) and then that it converges to a unique fixed point (Part 2 and 3).

\noindent
\textbf{Part 1:} \textit{The sequence $\vinone_{1}, \vinone_{2}, \ldots$ defined by $\vinone_{t+1} = \Bn \vinone_t$ converges to a limit $\vinone^* \in \RR^{\numgvfs|\Hists|}$.}

Because \(G\) is acyclic, we have a linear topological ordering of the vertices, \(i_1, \ldots, i_\numgvfs\): for each directed edge \((i,j)\), \(i\) comes before \(j\) in the ordering. Therefore, starting from the last GVF \(j = i_\numgvfs\), we know that the Bellman operator \(\Bn^{(j)}\) is a contraction with rate \(\beta_{j} < 1\),
{{{c}}}
\begin{equation*}
\| \Bn^{(j)} \vit{j}{1} - \Bn^{(j)} \vit{j}{0} \| = \| \Ppigammaj{j} \vit{j}{1} - \Ppigammaj{j}  \vit{j}{0} \| \le \beta_j\| \vit{j}{1} - \vit{j}{0} \|
.
\end{equation*}
{{{c}}}
Therefore, iterating \(\Bn\) for \(t\) steps results in the error
{{{c}}}
\begin{equation*}
\| \vit{j}{t+1} - \vit{j}{t} \| \le \beta_j^t \| \vit{j}{1} - \vit{j}{0} \|
\end{equation*}
{{{c}}}
and as \(t \rightarrow \infty\), \(\vit{j}{t}\) converges to its fixed point.

We will use induction for the argument, with the above as the base case.
Assume for all \(j \in \{i_k, \ldots, i_{\numgvfs}\}\) there exists a ball of radius \(\epsilon(t)\) where \(\| \vit{j}{t+1} - \vit{j}{t} \| \le \epsilon(t)\) and \(\epsilon(t) \rightarrow 0\) as \(t \rightarrow \infty\).
Consider the next GVF in the ordering, \(i = i_{k-1}\).

\textbf{Case 1: } There are no outgoing edges from \(i\). If \(i\) does not use another GVF \(j\) in its cumulant, then iterating with \(\Bn\) independently iterates \(\vit{i}{t}\) with \(\Bn^{(i)}\). Therefore, as above, \(\vit{i}{t}\) converges because the Bellman operator is a contraction. In this setting, clearly such an \(\epsilon_i(t)\) exists because \(\| \vit{j}{t+1} - \vit{j}{t} \| \rightarrow 0\) as \(t \rightarrow \infty\).

\textbf{Case 2: } The cumulant for GVF \(i\) is composed of the values for the set of GVFs \(\mathcal{J} \subseteq  \{i_k, \ldots, i_{\numgvfs}\}\). The basic idea, formalized below, is that GVF \(i\) will be guaranteed to converge once the GVFs used to construct the become sufficiently accurate. The update is \(\vit{i}{t+1} =  \Cpij{i}_{\vinone_t}  + \Ppigammaj{i} \vit{i}{t}\). The change in \(\vit{i}{t}\) is
{{{c}}}
\begin{align*}
\| \vit{i}{t+1} - \vit{i}{t} \|
&=  \| (\Cpij{i}_{\vinone_t} - \Cpij{i}_{\vinone_{t-1}})+ \Ppigammaj{i} ( \vit{i}{t} - \vit{i}{t-1}) \|\\
&\le K_i \sum_{j \in \mathcal{J}} \| \vit{j}{t} - \vit{j}{t-1}\| + \beta_i \| \vit{i}{t} - \vit{i}{t-1} \|\\
&\le  \numgvfs K_i  \epsilon(t-1) + \beta_i \| \vit{i}{t} - \vit{i}{t-1} \|
.
\end{align*}
{{{c}}}
In the first inequality, the first term is due to Lipschitz continuity of the cumulant and the second term is due to the fact that \(\| \Ppigammaj{i}  \| = \beta_i\). In the second inequality, we know \(\| \vit{j}{t} - \vit{j}{t-1}\| \le \epsilon_j(t)\), under the inductive hypothesis. The second inequality is loose, as the sum only involves \(|\mathcal{J}| < \numgvfs\) terms, but we use \(\numgvfs\) for simplicity since the results goes through with this constant as well.
For sufficiently large \(t\), \(\epsilon(t-1)\) can be made arbitrarily small.
If \(\numgvfs K_i \epsilon(t-1) < (1-\beta_i) \| \vit{i}{t} - \vit{i}{t-1} \|\), i.e., \(\epsilon(t-1) < \tfrac{(1-\beta_i)}{\numgvfs K_i} \| \vit{i}{t} - \vit{i}{t-1} \|\) then
{{{c}}}
\begin{align*}
\| \vit{i}{t+1} - \vit{i}{t} \|
&\le \tilde{\beta}_i \| \vit{i}{t} - \vit{i}{t-1} \| \hspace{1.0cm}\text{for some $\tilde{\beta}_i < 1$}
\end{align*}
{{{c}}}
and so the iteration is a contraction on step \(t\).
Else, if \(\epsilon(t-1) \ge \tfrac{(1-\beta_i)}{\numgvfs K_i} \| \vit{i}{t} - \vit{i}{t-1} \|\), then this implies the difference \(\| \vit{i}{t+1} - \vit{i}{t} \|\) is already within a small ball, with radius \(\numgvfs K_i \epsilon(t-1)/(1-\beta_i)\).  As \(t \rightarrow \infty\), the difference can oscillate between being within this ball, which shrinks to zero because \(\epsilon(t)\) shrinks to zero, or being iterated with a contraction that also shrinks the difference. In either case, there exists an \(\epsilon_i(t)\) such that  \(\| \vit{i}{t+1} - \vit{i}{t} \| \le \epsilon_i(t)\), where \(\epsilon_i(t) \rightarrow 0\) as \(t \rightarrow \infty\).

By induction, we have such an \(\epsilon_i\) for all GVFs in the network. Therefore, we know the sequence \(\vit{i}{t}\) converges.

\noindent
\textbf{Part 2:} \textit{$\vinone^*$ is a fixed point of $\Bn$.}

Because the Bellman network operator is continuous, the limit can be taken inside the operator
{{{c}}}
\begin{equation*}
\vinone^* = \lim_{t \rightarrow \infty} \vt{t}
= \lim_{t \rightarrow \infty} \Bn\vt{t-1}
= \Bn \left(\lim_{t \rightarrow \infty} \vt{t-1}\right) = \Bn \vinone^*
\end{equation*}

\noindent
\textbf{Part 3: } \textit{$\vinone^*$ is the only fixed point of $\Bn$.}

Consider an alternative solution \(\vinone\). Because of the uniqueness of fixed points under Bellman operators, all those GVFs that have non-compositional cumulants have unique fixed points and so those components in \(\vinone\) must be the same as \(\vinone^*\). All the GVFs next in the ordering that use those GVFs as cumulants must then also converge to a unique value, because their Bellman operators with fixed GVFs as cumulants have a unique fixed point. This argument continues for the remaining GVFs in the ordering.
#+end_proof

#+name: cor:gvfn:main
#+begin_corollary
Under Assumption 3 with non-compositional cumulants (no edges in \(G\)), iterating \(\vt{t+1} = \Bn \vt{t}\) converges to a unique fixed point.
#+end_corollary

#+ATTR_LATEX: :options [Necessity of Acyclic Composition]
#+ATTR_HTML: :title Necessity of Acyclic Composition
#+begin_proposition
There exists transition function \(\Pfcn: \States \times \Actions \times \States \rightarrow [0,1]\) and policy \(\pi: \States \times \Actions \rightarrow [0,1]\) such that, for two GVFs in a cycle, iteration with the Bellman network operator diverges.
#+end_proposition

#+begin_proof
Assume there are two states, with the policy defined such that we get the following dynamics for the Markov chain
\begin{equation}
\Ppi =
\left[\begin{array}{cc}
0.9 & 0.1\\
0.1 & 0.9
\end{array}
\right]
.
\end{equation}
{{{c}}}
Assume further that \(\gamma = 0.95\). The resulting Bellman iteration is
{{{c}}}
\begin{align*}
\twovec{\vi{1}}{\vi{2}}
&= \Ppi \twovec{\vi{2}}{\vi{1}} + \gamma  \Ppi \twovec{\vi{1}}{\vi{2}} \\
&= \Ppi \left[\begin{array}{cc}
0 & 1\\
1 & 0
\end{array}
\right] \twovec{\vi{1}}{\vi{2}} + \Ppi \left[\begin{array}{cc}
\gamma & 0\\
0 & \gamma
\end{array}
\right] \twovec{\vi{1}}{\vi{2}} \\
&= \Ppi \left[\begin{array}{cc}
\gamma & 1\\
1 & \gamma
\end{array}
\right] \twovec{\vi{1}}{\vi{2}}
\end{align*}
{{{c}}}
Since the matrix \(\Ppi \left[\begin{array}{cc}
\gamma & 1\\
1 & \gamma
\end{array}
\right] \)
is an expansion, for many initial \(\twovec{\vi{1}}{\vi{2}}\) this iteration goes to infinity, such as initial \(\vi{1} = \vi{2} = \twovec{1}{1}\).
#+end_proof

** The Objective Function for GVFNs
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:objective
:END:

With a valid Bellman network operator, we can proceed to defining the objective function for GVFNs. The above fixed point equation assumes a tabular setting, where the values can be estimated directly for each history. GVFNs, however, have a restricted functional form, where the value estimates must be a parametrized function of the current observation and value predictions from the last time step. Under such a functional form, it is unlikely that we can exactly solve for the fixed point. Rather, we will solve for a projected fixed point, which projects into the space of representable value functions.

Define the space of functions as
{{{c}}}
\begin{align}
\mathcal{F} = \Big\{ &\vinone_\weights = [\vi{1}_\weights, ..., \vi{\numgvfs}_\weights] \in \RR^{\numgvfs|\Hists|}  \ \ | \ \ \text{ where } \weights \in \weightspace \ \ \text{ and } \\
&V_\weights(\hvec_{t+1}) = f_\weights([\viweights{1}(\hvec_t), \ldots, \viweights{\numgvfs}(\hvec_t)], \xvec_{t+1})
\ \ \text{ when } \text{Pr}(\hvec_{t+1} | \hvec_t, \xvec_{t+1}) > 0 \Big\} \nonumber
\end{align}
{{{c}}}
Recall that \(\xvec_{t+1} = [a_t, \obs_{t+1}]\). We know \(\text{Pr}(\hvec_{t+1} | \hvec_t, \xvec_{t+1}) > 0\) only when \(\hvec_{t+1} \equiv \hvec_t a_t \obs_{t+1}\), and so expect this to only be true for one outcome \(\hvec_{t+1}\). We write that \(\hvec_{t+1}\) is equivalent, rather than equal, to the current history appended with action \(a_t\) and observation \(\obs_{t+1}\), because \(\hvec_{t+1}\) might be shorter (more minimal): earlier actions and observations might not be needed.
Define the projection operator
{{{c}}}
\begin{align}
\Pi_{\mathcal{F}}(\vinone) &\defeq \min_{\hat{\vinone} \in \mathcal{F}} \| \vinone - \hat{\vinone} \|_{\dw}^2
\hspace{0.5cm}\text{ where } \| \vinone - \hat{\vinone} \|_{\dw}^2 \defeq \sum_{\hvec \in \Hist} \dw(\hvec) (V(\hvec) - \hat{V}(\hvec))^2
\end{align}
{{{c}}}
{{{c}}}
where \(\dw: \Hists \rightarrow [0,1]\) is the sampling distribution over histories. Typically, we assume data is generated by following a behavior policy \(\mu: \Hists \rightarrow [0,1]\), and that \(\dw\) is the stationary distribution for this policy. The value functions for policies \(\pi_i\) are typically learned off-policy, since in general \(\pi_i\) will not equal \(\mu\). The behavior policy \(\mu\) used to gather the data is different, or off of, the policy---or policies---that we are evaluating.

To obtain the projected fixed point solution, a natural goal is to minimize the following projected objective,
{{{c}}}
\begin{equation}
\min_{\weights \in \weightspace} \| \Pi_{\mathcal{F}} \Bn \vinone_\weights - \vinone_\weights \|_{\dw}^2
\end{equation}
{{{c}}}
Unfortunately, this objective can be hard to compute, because the projection operator \(\Pi_{\mathcal{F}}\) onto the nonlinear manifold can be intractable. Instead, we take the same approach as citeA:&maei2010toward, when defining the nonlinear MSPBE for learning value functions with neural networks and other nonlinear function approximators. The idea is to approximate the projection onto the nonlinear manifold by assuming it is locally linear. Then, we can use a linear projection operator, defined locally at the current set of parameters \(\weights \in \weightspace\), spanned by the basis \(\phivec_{j,\weights}(\hvec) \defeq \nabla_\theta \vi{j}_\weights(\hvec)\) for all \(\hvec \in \Hists\) and GVFs \(j\). Let \(\phimat_{j,\weights}\) correspond to the matrix of stacked \(\phivec_{j,\weights}(\hvec)^\trans\) for all \(\hvec \in \Hists\), having \(|\Hists|\) rows.  We further define
{{{c}}}
\begin{align*}
  \phimat_{\weights}
  \defeq
  \left[\begin{array}{c}
          \phimat_{1, \weights}\\
          \vdots \\
          \phimat_{\numgvfs, \weights}
        \end{array}\right]
  \quad
  \quad
  \quad
  \dwdiag \defeq \diag\left[
  \begin{array}{c}
    \dw \\
    \vdots \\
    \dw
  \end{array}
  \right]
  \quad
  \quad
  \quad
  \Pi_{\weights}
  \defeq
    \phimat_{\weights}
    (\phimat_{\weights}^\trans \dwdiag \phimat_{\weights})^\inv
    \phimat_{\weights}^\trans \dwdiag
    .
\end{align*}
{{{c}}}
{{{c}}}
Using this locally linear approximation to the objective potentially expands the set of stationary points. The fixed points under the original projection are still fixed points under this locally linear approximation. But, there could be points that are fixed points under this locally linear approximation, that would not be under the original.

We call the final objective using this projection the MSPBNE[fn:: A variant of the MSPBNE has been introduced for TD networks citep:&silver2013gradient; the above generalizes that MSPBNE to GVF Networks. Because it is a strict generalization, we use the same name.], defined as
{{{c}}}
\begin{align}
    \text{MSPBNE}(\weights) &\defeq \| \Pi_{\weights} \Bn \vinone_\weights - \vinone_\weights \|_{\dw}^2 \label{eq_projform}
   \end{align}
   {{{c}}}


We show in the following lemma, with proof, that in can be rewritten in a way that makes it more amenable to compute and sample gradients.[fn:: Since developing the MSPBNE, an alternative approach to defining a nonlinear MSPBE has been developed using a conjugate form for the Bellman error (see citeauthor:&dai2017learning (citeyear:&dai2017learning) and in-preparation work that makes the connection the MSPBE more explicit citep:&patterson2022generalized). The extension here should be relatively straightforward, as we formulate the objective using histories.] We will use this reformulation to develop algorithms to minimize this objective in the next section.

#+name: lemma:gvfn:mspbne-exp
#+begin_lemma
{{{c}}}
The MSPBNE defined in Equation \eqref{eq_projform} can be rewritten as
{{{c}}}
\begin{align}
\text{MSPBNE}(\weights) &= \boldsymbol{\delta}(\weights)^\top W(\weights)^\inv  \boldsymbol{\delta}(\weights) \label{eq_mspbne}
\end{align}
{{{c}}}
where
{{{c}}}
\begin{align}
 W(\weights) &\defeq
       \Expected_d\bigg[\sum_{j=1}^\numgvfs \phivec_{j,\weights}(H) \phivec_{j,\weights}(H)^\trans \bigg]
       = \sum_{\hvec \in \Hists} d(\hvec) \sum_{j=1}^\numgvfs \phivec_{j,\weights}(\hvec) \phivec_{j,\weights}(\hvec)^\trans \label{eqn:gvfn:w}\\
     \boldsymbol{\delta}(\weights) &\defeq
     \sum_{j=1}^\numgvfs \Expected_{d,\pi_j}\bigg[\tderror_j(H, A, H') \phivec_{j,\weights}(H) \bigg] \nonumber\\
     \tderror_j(H,A,H') &\defeq c^{(j)}(H, A, H') + \gamma^{(j)}(H, A, H')\viweights{j}(H') - \viweights{j}(H) \nonumber
     .
\end{align}
#+end_lemma


#+begin_proof

Starting with equation \eqref{eq_projform} and for $\Delta_{\weights} \defeq \Bn \vinone_\weights - \vinone_\weights$, we get
\begin{align*}
     \text{MSPBNE}(\weights)
     & = \| \Pi_{\weights} \Bn \vinone_\weights - \vinone_\weights \|_{\dw}^2\\
     & = \| \Pi_{\weights} \left[\Bn\vinone_\weights - \vinone_\weights \right] \|_{\dw}^2\\
     & = \| \Pi_{\weights}\Delta_{\weights} \|_{\dw}^2
\end{align*}
We can wrap the projection operator around the full TD error $\Delta_{\weights}$, because it has no affect on $\vinone_\weights$ which is already in the space. We then plug in the definition of $\Pi_\weights$
\begin{align}
     \Pi_\weights^\top \dwdiag \Pi_\weights
         &= \dwdiag^\top \phimat_\weights (\phimat_\weights^\top \dwdiag \phimat_\weights)^\inv \phimat^\top \dwdiag \nonumber \\
     \| \Pi_{\weights}\Delta_{\weights} \|_{\dw}^2
         &= \Delta_\weights^\top \Pi_\weights^\top \dwdiag \Pi_\weights \Delta_\weights \nonumber \\
    &= \Delta_\weights^\top \dwdiag^\top \phimat_\weights (\phimat_\weights^\top \dwdiag \phimat_\weights)^\inv \phimat^\top \dwdiag \Delta_\weights \label{mspbne_mat}
\end{align}
As in prior gradient TD work we then convert the matrix operations to expectation forms.
\begin{align*}
     \phimat_\weights^\top \dwdiag \phimat_\weights &= \sum_{j=1}^n \sum_{\hvec\in\Hists} \dw(\hvec) \phivec_{j,\weights}(\hvec) \phi_{j, \weights}(\hvec)^\top = \Expected_d\left[\sum_{j=1}^n \phivec_{j,\weights}(H)\phivec_{j,\weights}(H)^\top\right]\\
     &= W(\weights)\\
     \phimat_\weights^\top \dwdiag \Delta_\weights &= \sum_{j=1}^n \sum_{\hvec\in\Hists} \dw(\hvec) \phivec_{j,\theta}(\hvec) \sum_{a\in\Actions} \pi_j(a|\hvec) \Expected[\delta_j(\hvec,a,H')] = \sum_{j=1}^n\Expected_{d,\pi_j}\left[\delta_j(H,A,H')\phivec_{j,\weights}(H)\right]\\
     &=  \boldsymbol{\delta}(\weights)
\end{align*}
Then substituting into equation \eqref{mspbne_mat}, we get the result $\text{MSPBNE}(\weights) = \boldsymbol{\delta}(\weights)^\top W(\weights)^\inv \boldsymbol{\delta}(\weights)$.
#+end_proof

Now we do not actually get samples according to $\pi_j$; instead, we get them according to the behaviour $\mu$. Throughout this work, we have assumed a coverage property for $\mu$. This means that the behaviour policy $\mu$ satisfies $\mu(a | \hvec) > 0$ if any $\pi_j(a | \hvec) > 0$ for policies $\pi_1, \ldots, \pi_\numgvfs$. 

#+name: col:gvfn:mspbne-is
#+begin_corollary
For importance sampling ratios $\rho_j(a | \hvec) \defeq \frac{\pi_j(a | \hvec)}{\mu(a | \hvec)}$ and
  \begin{align*}
    \boldsymbol{\delta}_\mu(\weights) &\defeq \Expected_{d, \mu}\bigg[\sum_{j=1}^\numgvfs \rho_j(H,A) \tderror_j(H,A,H') \phivec_{j,\weights}(H) \bigg]\\
         &= \sum_{\hvec \in \Hists} d(\hvec) \sum_{a \in \Actions} \mu(a|\hvec) \sum_{j=1}^\numgvfs  \rho_j (a | \hvec) \Expected\bigg[\tderror_j(H,A,H') \phivec_{j,\weights}(\hvec) | H = \hvec, A = a \bigg] 
  \end{align*} 
  then we can show that $\boldsymbol{\delta}_\mu(\weights) = \boldsymbol{\delta}(\weights)$ and so we can write
   \begin{align*}
    \text{MSPBNE}(\weights) &= \boldsymbol{\delta}_\mu(\weights)^\top W(\weights)^\inv  \boldsymbol{\delta}_\mu(\weights)
   \end{align*}
#+end_corollary

#+begin_proof
The key is simply to show that $\boldsymbol{\delta}_\mu(\weights) = \boldsymbol{\delta}(\weights)$, because $W(\weights)$ depends only on $d$, not on the policies $\pi$ or $\mu$. This is straightforward with the typical cancellation in importance sampling ratios
  \begin{align*}
   \boldsymbol{\delta}_\mu(\weights)
   &= \sum_{\hvec \in \Hists} d(\hvec) \sum_{a \in \Actions} \mu(a|\hvec) \sum_{j=1}^\numgvfs  \rho_j (a | \hvec) \Expected\bigg[\tderror_j(\hvec, a, H') \phivec_{j,\weights}(h) | H = h, A = a \bigg] \\
     &= \sum_{\hvec \in \Hists} d(\hvec)  \sum_{j=1}^\numgvfs  \sum_{a \in \Actions} \mu(a|\hvec) \rho_j (a | \hvec) \Expected\bigg[\tderror_j(\hvec, a, H') \phivec_{j,\weights}(h) | H = h, A = a \bigg] \\ 
     &= \sum_{\hvec \in \Hists} d(\hvec)  \sum_{j=1}^\numgvfs  \sum_{a \in \Actions} \pi_j (a | \hvec) \Expected\bigg[\tderror_j(\hvec, a, H') \phivec_{j,\weights}(h) | H = h, A = a \bigg] \\ 
&= \boldsymbol{\delta}(\weights)         .
  \end{align*}
#+end_proof
From here on, therefore, we assume that $\boldsymbol{\delta}(\weights)$ is defined more generally as the above $\boldsymbol{\delta}_\mu(\weights)$, since they result in the same objective but this more general expression more obviously highlights off-policy sampling. 

From this reformulation, one can see that the MSPBNE objective is a weighted quadratic objective, with weighting matrix \(W(\weights)\) on vector \(\boldsymbol{\delta}(\weights)\). The objective is zero---and so minimal---when \(\boldsymbol{\delta}(\weights) = \zerovec\). This is similar to the temporal difference (TD) learning fixed point criteria. In fact, TD implicitly optimizes the linear MSPBE, which corresponds to the above objective with \(\numgvfs = 1\) and fixed features that do not depend on the parameters. Once we have a projected Bellman error objective, we can take advantage of the many advances in formulating TD algorithms to optimized MSPBE objectives. Therefore, though this objective looks quite complex, there is substantial literature to facilitate minimizing the MSPBNE.

** Algorithms for the MSPBNE
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:algs
:END:

The algorithms to optimize the MSPBNE are a relatively straightforward combination of standard algorithms for RNNs and the TD algorithms designed to optimize the MSPBE. To provide some intuition on these algorithms, and how to obtain this combination of TD and RNN algorithms, we begin with a simpler setting: extending TD to a recurrent setting,  with one GVF. From there, we introduce two algorithms for the MSPBNE: Recurrent TD and Recurrent GTD.

Consider first the on-policy TD update, without recurrence, assuming the true state \(\svec_t\) at time t is given:
{{{c}}}
\begin{align*}
\weights_{t+1} \gets \weights_t + \alpha_t \delta_t \nabla_\weights V_\weights(\svec_t) \hspace{0.5cm} \text{ where } \delta_t \defeq C_{t+1} + \gamma_{t+1} V_\weights(\svec_{t+1}) - V_\weights(\svec_{t})
.
\end{align*}
{{{c}}}
With recurrence, where the state is estimated and so is a function of \(\weights\), the only difference to this update is in the computation of \(\nabla_\weights V_\weights(\svec_t)\), where \(\svec_t\) should instead be thought of \(\svec_t(\weights)\). This gradient now requires the chain rule, to account for the impact of \(\weights\) on the last state, and the state before then, and so on:
{{{c}}}
\begin{equation*}
\frac{\partial V_\weights(\svec_t)}{\partial \weights_i} = \frac{\partial V_\weights(\svec_t)}{\partial \svec_t}^\top \frac{\partial \svec_t}{\partial \weights_i}
\end{equation*}
{{{c}}}
where \(\svec_t = f_\weights(\svec_{t-1}, \xvec_t)\).
{{{c}}}
Computing this gradient back-in-time, \(\nabla_\weights \svec_t\)---which is also called the \emph{sensitivity}---is precisely the aim of most RNN algorithms, including truncated BPTT and RTRL. Any algorithm that computes sensitivities can be used to obtain a TD update with recurrent connections to estimate the state.

For GVFNs, there are two differences: we need to account for off-policy sampling and the fact that state is itself composed of these value estimates, rather than being learned to estimate values. Value estimation within GVFNs requires off-policy updates, because the target policies \(\pi_j\) are not typically equal to the behavior policy \(\mu\). Therefore, we also need to include importance sampling ratios in the update
\begin{align*}
\rho_{t,j} \defeq \frac{\pi_j(A_t | \hvec_t)}{\mu(A_t | \hvec_t)} \ \ \ \ \text{ for all $j \in \{1, 2, \ldots, \numgvfs\}$}
.
\end{align*}
{{{c}}}
This ratio multiplies the TD update, to adjust the expectation of the update to be as if action \(A_t\) had been taken under \(\pi_j\) rather than the behavior \(\mu\). For the second difference, the Recurrent TD update is actually even simpler because the value function itself is the state. For the \(j\)-th value function---which is the \(j\)-th state variable---we get that \(\nabla_\weights \vifunc{j}_\weights\) at time \(t\) is \(\nabla_\weights \svec_{t,j}\). Notice that this gradient actually corresponds to using the above chain rule update, by using \(\vifunc{j}(\svec_t) = \svec_{t,j}\) as a selector function into the state variable.

The *Recurrent TD* update for GVFNs is
{{{c}}}
\begin{align}
\svec_t &\gets f_{\weights_t}(\svec_{t-1}, \xvec_t) &&\triangleright \text{ where } \xvec_t \defeq [a_{t-1}, \obs_t] \nonumber\\
\svec_{t+1} &\gets f_{\weights_t}(\svec_{t}, \xvec_{t+1}) &&\triangleright \text{ where } \xvec_{t+1} \defeq [a_{t}, \obs_{t+1}]\nonumber\\
\phivec_{t,j} &\gets \nabla_\weights \svec_{t,j} &&\triangleright \text{ Compute sensitivities using truncated BPTT} \nonumber\\
\delta_{t,j} &\gets C_{t+1}^{(j)} + \gamma_{t+1}^{(j)} \svec_{t+1,j} - \svec_{t,j}   \nonumber\\
\rho_{t,j} &\gets \frac{\pi^{(j)}(a_t | \obs_t)}{\mu(a_t | \obs_t)} &&\triangleright \text{ Policies can be functions of histories, not just of $\obs_t$}  \nonumber\\
\weights_{t+1} &\gets \weights_{t} + \alpha_t \bigg[ \sum_{j=1}^{\numgvfs} \rho_{t,j}\tderror_{t,j} \phivec_{t,j}  \bigg] \label{eq_rtd}
\end{align}
{{{c}}}

The TD update, however, is only an approximate semi-gradient update, even in the fully observable setting. To obtain exact gradient formulas, we turn to Gradient TD (GTD) algorithms. In particular we extend the nonlinear GTD strategy developed by citeA:&maei2010toward, to the MSPBNE. As above, we will immediately be able to use any algorithm to compute the sensitivities in the Recurrent GTD algorithm. But, the algorithm becomes more complex, simply because nonlinear GTD is more complex than TD even without recurrence.

We can use the following theorem to facilitate estimating the gradient. The main idea is to introduce an auxiliary weight vector, \(\wvec\), to provide a quasi-stationary estimate of part of the objective. This proof and explicit derivation for the resulting Recurrent TD algorithm is given in the Section ref:sec:gvfn:fullrgtd. As a warm up, we derive the result for non-compositional GVFs: no GVFs predict the outcomes of other GVFs. This makes the algorithm easier to follow. We prove the more general result and derivation in Section ref:sec:gvfn:fullrgtd.
{{{c}}}
#+name: thm:gradients
#+begin_theorem
Assume that \(V_{\weights}(\hvec)\) is twice continuously differentiable as a function of \(\weights\) for all histories \(\hvec\in\Hist\) where \(\dw(\hvec)>0\) and that \(W(\cdot)\), defined in Equation eqref:eqn:gvfn:w, is non-singular in a small neighbourhood of \(\weights\). Assume further that there are no compositional GVFs in the GVFN: no GVFs has a cumulant that corresponds to another GVFs prediction. Then for \(W(\weights)\) and \(\boldsymbol{\delta}(\weights) \) defined in Lemma ref:lemma:gvfn:mspbne-exp,
\begin{align}
    \wvec(\weights) &\defeq
    W(\weights)^\inv \boldsymbol{\delta}(\weights) \label{eqn:gvfn:secondw}
\end{align}
\begin{align}
  \hat{\delta}_{j,\theta}(H) &\defeq \phivec_{j,\weights}(H)^\trans \wvec(\weights) \nonumber\\
     \psivec(\weights) &\defeq \Expected_{d, \mu}\left[\sum\limits_{j=1}^{\numgvfs} \rho_j(H,A)\Big(\delta_j(H,A,H') - \hat{\delta}_{j,\theta}(H)\Big)  \nabla^2 \viweights{j}(H)  \wvec(\weights) \right] \label{eqn:gvfn:hv}
\end{align}
we get the gradient
\begin{align}
   -\tfrac{1}{2} \nabla  \text{MSPBNE}(\weights) &=
       \boldsymbol{\delta}(\weights) -
       \Expected_{d,\mu}\bigg[\rho_j(H,A)\gamma^{(j)}(H,A,H') \hat{\delta}_{j,\theta}(H) \phivec_{j,\weights}(H') \bigg] - \psivec(\weights) \label{eqn:gvfn:tdc}
\end{align}
#+end_theorem
{{{c}}}
We now have two additional terms to estimate beyond the standard sensitivities in a typical RNN gradient. First, we need to estimate this additional weight vector \(\wvec\), given in Equation eqref:eqn:gvfn:secondw. This can be done using standard techniques in reinforcement learning. Second, we also need to estimate a Hessian-vector product, given in Equation eqref:eqn:gvfn:hv. Fortunately, this can be computed using R-operators, without explicitly computing the Hessian-vector product, using only computation linear in the length of the vector.


The *Recurrent GTD* update, for this simpler setting without composition, is[fn:: As mentioned above, we could have considered an alternative MSPBNE, using an in-development nonlinear MSPBE objective citep:&patterson2022generalized. The resulting Recurrent GTD algorithm would look very similar, except the Hessian-vector product could be omitted: \(\psivec_t\) is simply dropped in the update to \(\theta\).]
{{{c}}}
\begin{align}
\svec_t &\gets f_{\weights_t}(\svec_{t-1}, \xvec_t) \nonumber\\
\svec_{t+1} &\gets f_{\weights_t}(\svec_{t}, \xvec_{t+1}) \nonumber\\
\phivec_{t,j} &\gets \nabla_\weights \svec_{t,j} \hspace{2.0cm} \triangleright \text{ Compute sensitivities using truncated BPTT}  \nonumber\\
\phivec'_{t,j} &\gets \nabla_\weights \svec_{t+1,j}  \nonumber\\
\rho_{t,j} &\gets \frac{\pi^{(j)}(a_t | \obs_t)}{\mu(a_t | \obs_t)}  \nonumber\\
\vvec_t &\gets \nabla^2 \svec_t \wvec_t \hspace{2.0cm} \triangleright \text{ Computed using R-operators, see Appendix \ref{sec:gvfn:gradbtt}} \nonumber\\
\hat{\delta}_{t,j} &\gets \phivec_{t,j}^\trans  \wvec_t \nonumber\\
  \psivec_t &\gets \sum_{j=1}^{\numgvfs} ( \rho_{t,j}\delta_{t,j} - \hat{\delta}_{t,j})  \vvec_t \nonumber\\
  \weights_{t+1} &\gets \weights_{t} + \alpha_t \bigg[ \sum_{j=1}^{\numgvfs}  \rho_{t,j} \tderror_{t,j} \phivec_{t,j} - \rho_{t,j}  \gamma_{j,t+1} \hat{\delta}_{t,j} \phivec'_{t,j} \bigg] - \alpha_t\psivec_t  \label{eq_rgtd}\\
   \wvec_{t+1} &\gets \wvec_t + \beta_t \bigg[ \sum_{j=1}^{\numgvfs}  \rho_{t,j} (\tderror_{t,j} - \hat{\delta}_{t,j} )\phivec_{t,j} \bigg] \nonumber
\end{align}
{{{c}}}
The derivation for this algorithm is similar to the derivation for Gradient TD Networks citep:&silver2013gradient, though for this more general setting with GVF Networks.

As alluded to, there are a variety of possible strategies to optimize the MSPBNE for GVFNs. This variety arises from different strategies to optimize RNNs, back-in-time, as well as from the variety of strategies to optimize the MSPBE for value estimation. For example, we can compute sensitivities using truncated BPTT or RTRL and its many approximations. Similarly, for the MSPBE, there are a variety of different strategies to approximate gradients, because the gradient is not straightforward to sample. These including a variety of gradient TD methods---such as GTD and GTD2---saddlepoint methods and semi-gradient TD (see citeauthor:&ghiassian2018online (citeyear:&ghiassian2018online) for a more exhaustive list).

** Deriving the Full Recurrent GTD Update
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:fullrgtd
:END:

Now that the objective is written in its expectation form, the gradients can be take with respect to the weight parameter. The main body stated the result for a simplified setting (Theorem ref:thm:gradients), to make it simpler to understand the result. We provide the more general result here, for compositional GVFs.

# \begin{restatable}{theorem}{gradtheoremgen}\label{thm:gradientsgen}
#+name: thm:gvfn:gradientsgen
#+begin_theorem
Assume that $V_{\weights}(\hvec)$ is twice continuously differentiable as a function of $\weights$ for all histories $\hvec\in\Hist$ where $\dw(\hvec)>0$ and that $W(\cdot)$, defined in Equation eqref:eqn:gvfn:w, is non-singular in a small neighbourhood of $\weights$. Then for
\begin{align*}
    \boldsymbol{\delta}(\weights)
      &\defeq 
        \Expected_{d,\mu}\bigg[ \sum_{j=1}^\numgvfs \rho_j(H,A) \tderror_j(H,A,H') \phivec_{j,\weights}(H) \bigg] \\
    \wvec(\weights)
      &= W(\weights)^\inv\boldsymbol{\delta}(\weights) \\
     \psivec(\weights) &= \Expected_{d, \mu}\left[\sum\limits_{j=1}^{\numgvfs} \Big(\rho_j(H,A)\delta_j(H,A,H') - \phivec_{j,\weights}(H)^\trans  \wvec(\weights)\Big)  \nabla^2 V^{(j)}_{\weights}(H)  \wvec(\weights) \right]  
\end{align*}
we get the gradient
\begin{align}
  & -\frac{1}{2} \nabla  \textrm{MSPBNE}(\weights)
    =
     -\Expected_{d, \mu}\bigg[ 
     \sum\limits_{j=1}^\numgvfs  \rho_j(H,A) \nabla_\weights \delta_j(H,A,H') \phivec_{j,\theta}(H)^\top  \bigg] \wvec(\weights)  - \psivec(\weights) \label{eq_gtd2}\\
    &=
    \boldsymbol{\delta}(\weights) - \psivec(\weights) \\
      & \ \ \ \ - \Expected_{d,\mu}\bigg[ \sum\limits_{j+1}^\numgvfs  \rho_j(H,A) \bigg[\sum_{i=1}^\numgvfs \cfunc(j,i) \phivec_{i,\weights}(H) + \gamma_j(H,A,H') \phivec_{j,\weights}(H')\bigg] \phivec_{j,\weights}(H)^\trans \wvec(\weights) \bigg]    \nonumber
\end{align}
#+end_theorem

#+begin_proof
For simplicity in notation below, we drop the explicit dependence on the random
variable $H$ in the expectations. 
\begin{align*}
  \phivec_{j,\weights}(H) \rightarrow \phivec_{j,\weights}&,\hspace{1cm}
  \phivec_{j,\weights}(H') \rightarrow \phivec_{j,\weights}'\\
  \tderror_j(H,A,H') \rightarrow \tderror_j&,\hspace{1cm}\rho_j(H,A) \rightarrow \rho_j
\end{align*}
{{{c}}}
Further, we will use $\partial_i$ to indicate the partial derivative with respect to $\weights_i$. 
We also assume all expectations are with respect to $d, \text{ and } \mu$. We use $J$ to denote the MSPBNE, which from Lemma ref:lemma:gvfn:mspbne-exp and Corollary ref:col:gvfn:mspbne-is, can be written $J(\weights) = \boldsymbol{\delta}(\weights)^\top W(\weights)^\inv \boldsymbol{\delta}(\weights)$.
When applying the product rule
{{{c}}}
\begin{align*}
  \partial_i J(\weights)
  &= 2 (\partial_i \boldsymbol{\delta}(\weights))^\top \wvec(\weights) + \boldsymbol{\delta}(\theta)^\top  \partial_i W(\weights)^\inv \boldsymbol{\delta}(\theta) \\
   \partial_i \boldsymbol{\delta}(\weights)
  &= \Expected\bigg[\sum_{j=1}^\numgvfs \rho_j   \partial_i\phivec_{j,\weights} \tderror_j + \phivec_{j,\weights}  \partial_i\tderror_j \bigg] \\ 
   \partial_i W(\weights)^\inv
  &= - W(\weights)^\inv  \partial_i W(\weights) W(\weights)^\inv 
  = -2 W(\weights)^\inv \Expected\bigg[\sum_{j=1}^\numgvfs (\partial_i\phivec_{j,\weights}) \phivec_{j,\weights}^\trans \bigg] W(\weights)^\inv 
\end{align*}
{{{c}}}
Recall that $\wvec(\weights) = W(\weights)^\inv\boldsymbol{\delta}(\weights)$, and that $W(\weights)$ is symmetric, giving
\begin{align*}
 \boldsymbol{\delta}(\theta)^\top \partial_i W(\weights)^\inv \boldsymbol{\delta}(\theta)
 &= -2\boldsymbol{\delta}(\theta)^\top W(\weights)^\inv \Expected\bigg[\sum_{j=1}^\numgvfs (\partial_i\phivec_{j,\weights})\phivec_{j,\weights}^\trans \bigg] W(\weights)^\inv  \boldsymbol{\delta}(\theta)\\
  &= -2\wvec(\weights)^\trans \Expected\bigg[\sum_{j=1}^\numgvfs (\partial_i\phivec_{j,\weights}) \phivec_{j,\weights}^\trans \bigg] \wvec(\weights)\\
  &= -2\wvec(\weights)^\trans \Expected\bigg[\sum_{j=1}^\numgvfs \phivec_{j,\weights} (\partial_i\phivec_{j,\weights})^\trans \bigg] \wvec(\weights)
\end{align*}
{{{c}}}
   The last line follows from the fact that the transpose of a scalar is equal to the scalar. Here we transpose the whole expression, leading to a transpose of the outer-product inside the sum.
  Additionally,
{{{c}}}
\begin{align*}
 \partial_i \boldsymbol{\delta}(\weights)^\top \wvec(\weights) 
  &= \Expected\bigg[\sum_{j=1}^\numgvfs \rho_j   \tderror_j (\partial_i\phivec_{j,\weights}) + \rho_j\phivec_{j,\weights}  \partial_i\tderror_j \bigg]^\trans  \wvec(\weights)\\ 
&=   \Expected\bigg[\sum_{j=1}^\numgvfs \rho_j \tderror_j (\partial_i\phivec_{j,\weights})^\trans \bigg] \wvec(\weights)
    + \Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \partial_i\tderror_j  \phivec_{j,\weights}^\trans  \bigg] \wvec(\weights)
\end{align*}
{{{c}}}
Grouping the terms with $(\partial_i\phivec_{j,\weights})$, we get
{{{c}}}
\begin{align*}
&\Expected\bigg[\sum_{j=1}^\numgvfs \rho_j \tderror_j (\partial_i\phivec_{j,\weights})^\trans \bigg] \wvec(\weights) - \wvec(\weights)^\trans \Expected\bigg[\sum_{j=1}^\numgvfs \phivec_{j,\weights}(\partial_i\phivec_{j,\weights})^\trans \bigg] \wvec(\weights)\\
&= \Expected\bigg[\sum_{j=1}^\numgvfs \Big( \rho_j \tderror_j - \wvec(\weights)^\trans\phivec_{j,\weights} \Big)(\partial_i\phivec_{j,\weights})^\trans\wvec(\weights)\bigg] \\
&= \boldsymbol{\psi}_i(\weights)
\end{align*}
{{{c}}}
where the last follows from the definition of $\nabla_\weights \boldsymbol{\psi}(\weights)$, which is the gradient vector composed of partial derivatives $\boldsymbol{\psi}_i(\weights)$. Therefore,
{{{c}}}
\begin{align*}
  \partial_i J(\weights)
  &= 2 \partial_i \boldsymbol{\delta}(\weights)^\top \wvec(\weights) + \boldsymbol{\delta}(\theta)^\top  \partial_i W(\weights)^\inv \boldsymbol{\delta}(\theta) \\
&= 2\boldsymbol{\psi}_i(\weights) + 2\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \partial_i\tderror_j \phivec_{j,\weights}^\trans  \wvec(\weights)  \bigg]
\end{align*}
 which proves Equation \eqref{eq_gtd2}. Now we can further simplify the second term, using the fact that $\phivec_{j,\weights} = \nabla_\weights V_{j,\weights}$, giving
\begin{align*}
\nabla_\weights \tderror_j = \nabla_\weights c_{j,\weights} + \gamma_j \phivec_{j,\weights}' - \phivec_{j,\weights}
.
\end{align*}
Now notice that 
\begin{align*}
\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \nabla_\weights\tderror_j \phivec_{j,\weights}^\trans  \wvec(\weights)  \bigg] 
&= \Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \big(\nabla_\weights c_{j,\weights} + \gamma_j \phivec_{j,\weights}' - \phivec_{j,\weights}\big) \phivec_{j,\weights}^\trans  \wvec(\weights)  \bigg] \\
&= -\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \phivec_{j,\weights}\phivec_{j,\weights}^\trans \bigg]\wvec(\weights) + \Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \big(\nabla_\weights c_{j,\weights} + \gamma_j \phivec_{j,\weights}'\big) \phivec_{j,\weights}^\trans  \wvec(\weights)  \bigg] 
\end{align*}
Because $\wvec(\weights) = W(\weights)^\inv\boldsymbol{\delta}(\weights)$, 
\begin{align*}
\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \phivec_{j,\weights}\phivec_{j,\weights}^\trans \bigg]\wvec(\weights) 
=
W(\weights)\wvec(\weights) = \boldsymbol{\delta}(\weights)
\end{align*}
Putting this all together, we get that
\begin{align*}
  -\tfrac{1}{2} \nabla_\weights J(\weights)
  &= -\boldsymbol{\psi}_i(\weights) - \Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j  \nabla_\weights\tderror_j \phivec_{j,\weights}^\trans\wvec(\weights) \bigg]\\
  &= -\boldsymbol{\psi}(\weights) + \boldsymbol{\delta}(\weights) -\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \big(\nabla_\weights c_{j,\weights} + \gamma_j \phivec_{j,\weights}'\big) \phivec_{j,\weights}^\trans  \wvec(\weights)  \bigg] 
\end{align*}
{{{c}}}
completing the proof.
#+end_proof

The resulting Recurrent GTD algorithm explicitly learns a second set of weights $\wvec$, to perform this update. In our implementation, we use a particular form of composition, namely that the cumulant for a GVF is a linear weighting of the predictions of some of the other GVFs on the next time step. If we let $c(i, j)$ indicate the weight on the $i$th GVF in the cumulant for the $j$th GVF, then we get that $\nabla_\weights c_{j,t} =   \sum_{i=1}^\numgvfs c(j,i) \phivec_{i,t}'$. 

The full *Recurrent GTD* update is 

\begin{align}
\svec_t &\gets f_{\weights_t}(\svec_{t-1}, \xvec_t) \nonumber\\
\svec_{t+1} &\gets f_{\weights_t}(\svec_{t}, \xvec_{t+1}) \nonumber\\
\phivec_{t,j} &\gets \nabla_\weights \svec_{t,j} \hspace{2.0cm} \triangleright \text{ Compute sensitivities using truncated BPTT}  \nonumber\\
\phivec'_{t,j} &\gets \nabla_\weights \svec_{t+1,j}  \nonumber\\
\rho_{t,j} &\gets \frac{\pi_j(a_t | \obs_t)}{\mu(a_t | \obs_t)}  \nonumber\\
\vvec_t &= \nabla^2 \svec_t \wvec_t \hspace{2.0cm} \triangleright \text{ Computed using R-operators, see Appendix \ref{sec:gvfn:gradbtt}} \nonumber\\
  \psivec_t &= \sum_{j=1}^{\numgvfs} ( \rho_{j,t}\delta_{j,t} - \phivec_{j,t}^\trans  \wvec_t)  \vvec_t \label{eq_rgtd_gen}\\
  \weights_{t+1} &= \weights_{t} + \alpha_t \bigg[ \sum_{j=1}^{\numgvfs}  \rho_{j,t} \tderror_{j,t} \phivec_{j,t} - \rho_{j,t} \bigg[\nabla_\weights c_{j,t} + \gamma_{j,t+1} \phivec'_{j,t}  \bigg] \phivec_{j,t}^\trans \wvec_t - \psivec_t  \bigg] \nonumber\\
   \wvec_{t+1} &= \wvec_t + \beta_t \bigg[ \sum_{j=1}^{\numgvfs}  \rho_{j,t} \Big(\tderror_{j,t} - \phivec_{j,t}^\trans \wvec_t\Big) \phivec_{j,t} \bigg] \nonumber
\end{align}

** Computing gradients of the value function back through time
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:gradbtt
:END:

In this section, we show how to compute $\phivec_t$, which was needed in the
algorithms. Recall from Section ref:sec:gvfn:algs that
we set $V^{(j)}(\svec_{t+1}) = \svec_{t+1,j}$, and using
$\Feats_{t+1}\defeq\twovec{\svec_{t}}{\xvec_{t+1}}$ let
  $\svec_{t+1,j} =
  \sigma\left( \Feats_{t+1}^\top\weights^{(j)}\right)$ for
  some activation function $\sigma:\RR\rightarrow\RR$. 
  For both Backpropagation Through Time or Real Time Recurrent Learning,
  it is useful to take advantage of the following formula for /recurrent sensitivities/
{{{c}}}
\begin{align*}
  \pd{V^{(i)}(S_{t+1})}{\weights_{(k,j)}} &= \actdot(\Feats_{t+1}^\trans \weights^{(i)}) \biggr(\biggr(\pd{\Feats_{t+1}}{\weights_{(k,j)}} \biggr)^\trans \weights^{(i)} + (\Feats_{t+1})_j \krondelta_{i,k}\biggr) \\
  &= \actdot(\Feats_{t+1}^\trans \weights^{(i)}) \left(\biggr[\pd{V^{(1)}(S_{t})}{\weights_{(k,j)}}, ... ,\pd{V^{(n)}(S_{t})}{\weights_{(k,j)}},\zerovec^\top\biggr] \weights^{(i)} + (\Feats_{t+1})_j \krondelta_{i,k}\right) 
\end{align*}
{{{c}}}
where $\krondelta$ is the Kronecker delta function and $\actdot(\cdot)$ is shorthand for the derivative of $\sigma$ w.r.t its scalar input. Given this formula, BPTT or RTRL can simply be applied.

For Recurrent GTD---though not for Recurrent TD---we additionally need to compute the Hessian back in time, for the Hessian-vector product. The Hessian for each value function is a $\numgvfs(\featuresize)\times \numgvfs(\featuresize)$ matrix; computing the Hessian-vector product naively would cost at least $O((\featuresize + \numgvfs)^2 \numgvfs^2)$ for each GVF, which is prohibitively expensive. We can avoid this using R-operators also known as Pearlmutter's method citep:&pearlmutter1994fast. 

The R-operator $\Roperator\{\cdot\}$ is defined as 
\begin{equation*}
  \mathcal{R}_\wvec\biggr\{\gvec(\weights)\biggr\} \defeq \frac{\partial \gvec(\weights + r \wvec)}{ \partial r} \biggr\rvert_{r=0}
\end{equation*}
for a (vector-valued) function $\gvec$ and satisfies 
\begin{equation*}
  \mathcal{R}_\wvec\biggr\{\nabla_\weights f(\weights)\biggr\} = \nabla^2_\weights f(\weights) \wvec.
\end{equation*}
{{{c}}}
Therefore, instead of computing the Hessian and then producting with $\wvec_t$, this operation can be completed in linear time, in the length of $\wvec_t$. 

Specifically, for our setting, we have
\begin{align*}
  &\mathcal{R}_w\biggr\{\actdot(\Feats_t^\top\weights)[\nabla_\weights \Feats_t^\trans \weights + \Feats_t^\trans \nabla_\weights\weights]\biggr\}\\
  & \quad = \pd{}{r}\biggr(\actdot(\Feats_t^\top(\weights + r \wvec)[\nabla_\weights \Feats_t^\trans (\weights + r\wvec) + \Feats_t^\trans \nabla_\weights(\weights+r\wvec)]\biggr) \biggr\rvert_{r=0}
\end{align*}
{{{c}}}
To make the calculation more managable we seperate into each partial for every node k and associated weight j.
{{{c}}}
\begin{align*}
  \pd{V^{(i)}(S_{t+1}, \weights)}{\weights_{(k,j)}} &= \actdot(\Feats_{t+1}^\trans \weights^{(i)}) (\eta_{t+1})_{i,k,j} \\
  (\eta_{t+1})_{i, k, j} &= ((\valuedtheta_t)_{k,j}^\trans \weights^{(i)} + (\Feats_{t+1})_j \delta_{i,k}) \\
  (\valuedtheta_t)_{k,j} &= \left[\pd{V^{(1)}(S_{t})}{\weights_{(k,j)}}, ... ,\pd{V^{(n)}(S_{t})}{\weights_{(k,j)}},\zerovec^\top\right]^\top\\
  \valuedr_t &= \left[\pd{V^{(1)}(S_{t})}{r}, ... ,\pd{V^{(n)}(S_{t})}{r},\zerovec^\top\right]^\top \\
\end{align*}
\begin{align*}
  \RopValueVect &= \left[\mathcal{R}_w\biggr\{\pd{V^{(1)}(S_{t-1})}{\weights_{(k,j)}}\biggr\}, ..., \mathcal{R}_w\biggr\{\pd{V^{(\numgvfs)}(S_{t-1})}{\weights_{(k,j)}}\biggr\},\zerovec^\top\right]^\top \\
  \mathcal{R}_w\left\{\pd{V^{(i)}(S_{t+1}, \weights)}{\weights_{(k,j)}}\right\} &= \frac{\partial^2 V^{(i)}(S_{t+1}, \weights + r\secweights)}{\partial r \partial \weights_{(k,j)}} \biggr\rvert_{r=0} \\
  &= \actdotdot\biggr(\Feats_{t+1}^\trans (\weights^{(i)} + r\secweights_i)\biggr) \biggr(\valuedr_t^\trans (\weights^{(i)} + r\secweights_i) + \Feats_{t+1}^\trans \secweights_i\biggr) (\eta_{t+1})_{i,k,j} \\
  &\phantom{{}=}
   + \actdot\biggr(\Feats_{t+1}^\trans (\weights^{(i)} + r\secweights_i)\biggr)\biggr(\RopValueVect^\trans (\weights^{(i)} + r\secweights)
   % &\phantom{{}= + \actdot(x_{t+1}^\trans (\weights^{(i)} + r\secweights_i))}
  + (\valuedtheta_t)_{k,j}^\trans w_i
  + (\valuedr_t)_j \krondelta_{k,i}\biggr) \biggr\rvert_{r=0} \\
  &= \actdotdot\biggr(\Feats_{t+1}^\trans \weights^{(i)}\biggr) \biggr(\valuedr_t^\trans (\weights^{(i)}) + \Feats_{t+1}^\trans \secweights_i\biggr) (\eta_{t+1})_{i,k,j} \\
  &\phantom{{}=}
   + \actdot\biggr(\Feats_{t+1}^\trans \weights^{(i)}\biggr) \biggr(\RopValueVect^\trans \weights^{(i)}
   % &\phantom{{}=}
  + (\valuedtheta_t)_{k,j}^\trans \secweights_i + (\valuedr_t)_j \krondelta_{k,i}\biggr)\\
  \pd{V^{(i)}(S_{t})}{r} &= \actdot(\Feats_t^\trans \weights^{(i)})(\valuedr_{t-1}^\trans \weights^{(i)} + \Feats_t^\trans w_i)
  % \krondelta_{k,i} &\defeq \text{Kronecker Delta} 
\end{align*}

** TD(\(\lambda\)) for GVFNs
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:tdlambda
:END:

For many of the experiments we used Recurrent TD with no back-propagation through time $p=1$. This algorithm only adjusts parameters to minimize immediate TD error. In many cases, this was sufficient, but at times it was slow and increasing $p$ improved learning. Another strategy is to use traces to obtain credit assignment back-in-time. The TD-error on this step can be attributed to state values back-in-time, with the \textbf{TD($\boldsymbol{\lambda}$) algorithm} 
{{{c}}}
\begin{align}
\svec_t &\gets f_{\weights_t}(\svec_{t-1}, \xvec_t) \nonumber\\
\svec_{t+1} &\gets f_{\weights_t}(\svec_{t}, \xvec_{t+1}) \nonumber\\
\gvec_{t,j} &\gets \nabla_{\weights_j} f_{\weights_t}(\svec_{t-1}, \xvec_{t}) && \triangleright \text{ gradient given $\svec_{t-1}$, no BPTT} \nonumber\\
\evec_{t,j} &\gets \gvec_{t,j} + \gamma_{t,j} \lambda \evec_{t-1,j} && \triangleright \text{ eligibility trace, $0 \le \lambda \le 1$} \nonumber\\
\delta_{t,j} &\gets C_{t+1}^{(j)} + \gamma_{t+1, j} \svec_{t+1,j} - \svec_{t,j}   \nonumber\\
\weights_{t+1,j} &\gets \weights_{t,j} + \alpha_t \tderror_{t,j} \evec_{t,j} \label{eq_td_lambda}
\end{align}
{{{c}}}
Notice the difference to Recurrent TD and Recurrent GTD, that the weights for each GVF are updated independently. This difference arises because the gradient computations for back-in-time, for the sensitivities, is what couples the updates. Without these sensitivities, the immediate gradient of the value $\gvec_{t,j}$ is independent for each GVF. 

** Summary

This chapter focused on the characterization of the mean squared projected bellman network error (MSPBNE) and subsequent derivations of algorithms based on this objective function. Specifically, we defined the Bellman network operator and the subsequent restrictions on the cumulants (i.e. acyclic composite connections). Equipped with the Bellman network operator and proving it is a contraction under some standard assumptions, the mean squared projected bellman network error can be defined. The final sections of the chapter derived the recurrent GTD and TD algorithms, and provided some insight into how to calculate various components with temporal sensitivities.
* Empirically Exploring Hand Designed GVFNs
:PROPERTIES:
:CUSTOM_ID: chap:gvfn:empirical
:END:


In this chapter, I empirically explore GVFNs with hand-designed GVFs. These empirical investigations provide the initial evidence that GVFNs can learn without gradients calculated through BPTT. Through this empirical experimentation I show the importance of choosing GVFs as a means to develop predictive targets, explore the application of GVFNs on time-series forecasting, and finally show recurrent TD to be enough to learn in Ring World a Cycle World.

# - Initial evidence to hypothesis
# - Show the importance of choosing the right predictive targets
# - Show that R-GTD not necessary (as proposed by [[cite:&silver2013gradient]]) and can be replaced by R-TD
** Experiments in Forecasting
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:exp-forecasting
:END:

In this section, we compare GVFNs and RNNs on two time series prediction datasets, particularly to ask 1) can GVFNs obtain comparable performance and 2) do GVFNs allow for faster learning, due to the regularizing effect of constraining the state to be predictions.[fn:: All code for these experiments can be found at [[https://github.com/mkschleg/GVFN]]] We investigate if they allow for faster learning both by examining learning speed as well as robustness to truncation length in BPTT.

\paragraph{Datasets}

We consider two time series datasets previously studied in a comparative
analysis of RNN architectures by [[cite:&bianchi2017recurrent]]: the Mackey-Glass
time series (previously introduced), and the Multiple Superimposed Oscillator.

The single-variate *Mackey-Glass (MG)* time series dataset is a synthetic data set generated from a time-delay differential equation:
{{{c}}}
\begin{equation}
  \partialderivative{y(t)}{t} = \alpha\frac{y(t-\tau)}{1+y(t-\tau)^{10}} - \beta y(t)\label{eq:mg}
  .
\end{equation}
{{{c}}}
We follow the learning setup in [[cite:&bianchi2017recurrent]]: we set $\tau=17$,
$\alpha=0.2$, $\beta=0.1$, and we take integration steps of size $0.1$. We forecast the target variable $y$ twelve steps into the
future, starting from an initial value $y(0)=1.2$. We generate $\nsamples = 600,000$ samples.

The *Multiple Superimposed Oscillator (MSO)* synthetic time series citep:&jaeger2004harnessing is defined by the sum of four sinusoids with unique frequencies

\begin{equation}
  y(t) = \sin(0.2t)+\sin(0.311t)+\sin(0.42t)+\sin(0.51t). \label{eq:mso}
\end{equation}
{{{c}}}
The resulting oscillator has a long period of $2000\pi \approx 6283.19$. Because we generate data using $t\in\Naturals$, the oscillator effectively never returns to a previously seen state. These attributes make prediction difficult with the MSO, as the model cannot rely on memory alone to make good predictions. We generate $\nsamples = 600,000$ samples and make predictions with a forecast horizon of $h=12$.

\paragraph{Experiment Settings}

The focus in this work is on online prediction, and so we report online prediction error. At each step $t$, after observing $o_t = y(t)$, the RNN (or GVFN) makes a prediction $\hat{y}_t$ about the target $y_t$, which is the observation 12 steps into the future, $y_t = y(t+h)$. The magnitude of the squared error $(\hat{y}_t - y_t)^2$ depends on the scale of $y_t$. To provide a more scale invariant error, we normalize by the mean of the target---a mean predictor. Specifically, for each run, we report average error over windows of size 10000 with the mean predictor is computed for each window. This results in $\nsamples/10000$ normalized squared errors, where $\nsamples$ is the length of the time series. We repeat this process 30 times, and average these errors across the 30 runs, and take the square root, to get a Normalized Root Mean Squared Error (NRMSE).

We fixed the values for hyperparameters as much as possible, using the previously reported value for the RNN and reasonable defaults for the GVFN. The stepsize is typically difficult to pick ahead of time, and so we sweep that hyperparameter for all the algorithms.  We attempted to make the number of hyperparameters swept comparable for all methods, to avoid an unfair advantage. We do not tune the truncation length, as we report results for each truncation length $p\in \{1, 2, 4, 8, 16, 32\}$ for all the algorithms.

\paragraph{Algorithm Details}

The GVFN consists of a single layer of size 32 and 128 (for MG and MSO respectively), corresponding to horizon GVFs. As described in Section ref:sec:gvfn:case-study, each GVF has a constant continuation $\gamma^{(j)} \in [0.2,0.95]$ and cumulant $C_{t}^{(j)}=\frac{1-\gamma^{(j)}}{y^{\text{max}}_{t}}y(t)$, where
$y^{\text{max}}_t$ is an incrementally-computed maximum of the observations
$y(t)$ up to time $t$. The GVFs are generated to linearly cover the range
$[0.2,0.95]$. This set is chosen as one of the simplest options that can be used
without much domain knowledge. It is likely not the optimal set of GVFs for the
GVFN, but represents a reasonable default choice.
The GVFN is followed by
a fully-connected layer with relu activations to produce a non-linear
representation, which is linearly weighted to predict the target.
The GVFN layer uses a linear activation, with clipping between [-10,
10], to help ensure state features remain bounded;
again, this represented a simple rather than optimized choice.

The GVFN was trained using Recurrent TD with a constant learning rate and a batch size of 32. The weights for the fully-connected relu layer and the weights for the linear output  are trained using ADAM, to minimize the mean squared error between the prediction at time $t$ and target $y(t+h)$. We swept the stepsize hyperparameters: the learning rate for the GVFN $\alpha_{\text{\tiny GVFN}} = N\cdot10^{-k}$ for $N\in\{1,5\}$, $k\in\{3,\ldots,6\}$, and the learning rate for the fully-connected and output layers $\alpha_{\text{pred}} =N\cdot10^{-k}$ for $N\in\{1,5\}$, $k\in\{2,\ldots,5\}$.

We compare to RNNs, LSTMs, and GRUs [fn:: We use standard implementations found in Flux cite:&innes:2018.]. The network architecture is similar to the GVFN for all recurrent models. The RNN size is set to 32 for MG and 128 for MSO, while the GRU and LSTM have 8 hidden units for MG and 128 for MSO. Notice how the GRU and LSTM have fewer hidden units than the RNN and GVFN for the MG experiment. This roughly accounts for the increased complexity of the LSTMs and GRUs as compared to the GVFN and RNN. While this was needed to make all the models competitive in MG, we found the GVFNs performed well in MSO even with the same number of hidden units as the GRU and LSTMs.
We trained these models using p-BPTT---specifically with the ADAM optimizer with a batch size of 32---to
minimize the mean squared error between the prediction at time $t$ and $y(t+h)$. We swept the learning rate $\alpha = 2^{-k}$ with $k \in \{1,\ldots,20\}$.
\begin{figure}[t!]
  \center
  \includegraphics[width=0.95\textwidth]{plots/gvfn/timeseries/trunc_comb_3.pdf}
  \caption{
    Truncation sensitivity for the (\textbf{left}) Mackey-Glass and (\textbf{right}) Multiple Superimposed Oscillator datasets. Errors are calculated using the normalized root mean squared error (NRMSE) averaged over the last 10k steps for the training results $\pm$ 1 standard error over 30 independent runs.
  }\label{fig:timeseries_sens}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.95\textwidth]{plots/gvfn/timeseries/learning_curves.pdf}
  \caption{
    Learning curves for the (\textbf{top}) Mackey-Glass and (\textbf{bottom}) Multiple Superimposed Oscillator datasets. We are reporting the normalized root mean squared error (NRMSE) normalized to the performance of the windowed average baseline. We use the average of 30 independent runs $\pm$ the standard error.
  }\label{fig:timeseries_lc}
\end{figure}

Finally, we also compare to RNNs with the 128 GVFs as auxiliary tasks. The augmented RNN has the same architecture as above, but with an additional set of output heads. The additional GVF heads are the same as those used by the GVFN, and are trained with TD. The gradient information from the GVFs is back-propagated through the network, influencing the representation. The augmented RNN was tuned over the same values as the RNN. The goal for adding this baseline is to gauge if there is an important difference in using the GVFs to directly constrain the state, as opposed to indirectly as auxiliary tasks. It further ensures that the RNN is given the same prior knowledge as the GVFN---namely the pertinence of these predictions---to avoid the inclusion of prior knowledge as a confounding factor.

All RNNs and GVFNs include a bias unit, as part of the input as well as in all layers. All methods have similar computation per step, particularly as they are run with the same truncation levels $p$.

\paragraph{Results}
We first show overall results across the truncation level in p-BPTT in Figure \ref{fig:timeseries_sens}. Three results are consistent across both datatsets: 1) GVFNs can obtain significantly better performance than RNNs with small $p$; 2) GVFNs are surprisingly robust to truncation level, providing almost the same performance across $p$; and 3) auxiliary tasks in the RNN do not provide consistent benefits across models and datasets. GVFNs provide a strict improvement on the MSO dataset. The result on MG is more nuanced. As truncation levels increase, the RNN's performance significantly improves and then passes the GVFN. This might suggest some bias in the specification of the GVFs. As is typical with regularization or imposing an inductive bias, it can improve learning---here allowing for much more stable learning with small $p$---but can prevent the solution from reaching the same prediction accuracy. In some cases, if we are fortunate, the inductive bias is strictly helpful, constraining the solution in the right way so as to incur minimal bias but improve learning. In MSO, it's possible the GVF specification was more appropriate and in MG less appropriate.


To gain more detailed insight into the behavior of the algorithms across truncation levels, we show learning curves for $p \in \{1, 8, 32\}$ in Figure \ref{fig:timeseries_lc}. All the approaches learn more slowly for $p =1$, but the RNNs are clearly impacted more significantly. In MSO, the GVFN has a clear advantage in terms of learning speed. This is not true in MG, where once $p \ge 8$, the RNN performs better and learns faster. The GVFN objective here may actually be difficult to optimize, but it allows the agent to make progress constructing a useful state, whereas the signal from the error to the targets is insufficient.

** Investigating Performance under Longer Temporal Dependencies

\begin{figure}[t]
  \center
  \includegraphics[width=0.95\textwidth]{plots/gvfn/compass_world/trunc_acc_comb_2.pdf}
  \caption{Results averaged over 30 runs $\pm$ one standard error. The dashed lines correspond to each RNN type augmented with auxiliary tasks, namely here the terminating horizon GVFs. The plots on the \textbf{(left)} are for a constant learning rate swept in range $\{0.1\times1.5^i; i \in [-10, 5]\} \cup \{1.0\}$. The plots on the \textbf{(right)} are for the ADAM optimizer with learning rate swept in range $\{0.01\times 1.5^i; i \in \{-18, -16, \ldots, 0\}\}$. The \textbf{(top)} row shows sensitivity over truncation measured by the average root mean squared value error (RMSVE) over the final 200000 steps of training. The \textbf{(bottom)} row shows learning curves for $p=4$ for prediction accuracy. We check if the prediction is correct by predicting the color of five with the highest GVF output, where the GVF prediction corresponds to a probability of facing that wall. When averaged over a window (10000 steps in our case) this results in a percentage of correct predictions during that time span.
  } \label{fig:compass}
\end{figure}

In this section, we investigate the utility of constraining states to be predictions, for an environment with long temporal dependencies. We use Compass World, introduced in Section \ref{GVFNs} (see Figure \ref{fig:compass_world_env}), which can have long temporal dependencies, because the random behavior can stay in the center of the world for many steps, observing only the color white.
The observation is encoded with two bits per color: one to indicate the agent observes that color, and the other to indicate another color is observed. The behavior policy chooses randomly between moving one-step forward; turning right/left for one step; moving forward until the wall is reached ({\em{leap}}); or randomly selecting actions for $k$ steps ({\em{wander}}). The full observation vector is encoded based on which action was taken, and includes a bias unit.

We chose five hard-to-learn GVFs with predictions corresponding to the wall the agent is facing. These predictions are not learnable without constructing an internal state. These five questions correspond to leap questions. The leap question is defined as having a cumulant of 1 in the event of seeing a specific wall (orange, yellow, red, blue, green), and a continuation function defined as $\gamma = 0$ when any color is observed---when the agent is facing a wall---and $\gamma = 1$ otherwise.


We use the same architecture for both RNNs and GVFNs; the main difference is that for the GVFN we constrain the hidden state to be GVF predictions. The GVFN uses 40 GVFs: 8 GVFs per color. The 8 GVFs for a color correspond to \textbf{Terminating Horizon} GVFs. This means that they have a cumulant of 1 when seeing that color, and zero otherwise; they have a $\gamma = 1- 2^k$ for one of 8 $k \in \{-7, -6, \ldots, -1\}$; they terminate---$\gamma$ becomes zero---when any color is observed; and the policy is to always go forward. These GVFs are similar to the horizon GVFs in time series prediction, except that termination occurs when a wall is reached and the policy is off-policy.
The RNN similarly uses 40 hidden units for the recurrent layer. For RNNs, we use the hyperbolic tangent and the sigmoid function for GVFNs. We used sigmoids instead for GVFNs, because the returns are always nonnegative; otherwise, these two activations represent a similar architectural choice.

We found treating the input action $a_t$ specially significantly improved performance of both the RNN and GVFN. This is done by specifying separate weight vectors $\{w_a \in \mathbb{R}^n ; \forall a \in \mathcal{A}\}$ for each action the agent can take. The hidden state is then calculated as $\svec_{t+1} = \sigma(w_{a_t}^\trans[\xvec_{t+1}, \svec_t])$, where $\sigma$ is the activation function. For the GRUs and LSTMs, this architectural modification is not straightforward; instead we pass the action as a one-hot encoding.

All the approaches share the same structure following the recurrent layer. The state $\svec_t$ is passed to a 32-dimensional hidden layer with relu activation, and then is linearly weighted to produce the predictions for the five hard-to-learn GVFs: $\hat{\mathbf{y}}_t = \text{relu}(\svec_t^\top \mathbf{F}) \mathbf{W}$ where $\mathbf{F} \in \RR^{40 \times 32}$ and $\mathbf{W} \in \RR^{32 \times 5}$. All methods include a bias unit on every layer.


The performance for increasing $p$, as well as learning curves for $p = 8$, are show in Figure \ref{fig:compass}. Again, we obtain a several clear conclusions. 1) The GVFN is again highly robust to truncation level, reaching almost perfect accuracy with $p =1$. 2) The GVFN can learn noticeably faster with smaller $p$, such as $p = 4$, and the differences disappear for larger $p$. 3) The auxiliary tasks do not provide near the same level of benefit as the GVFN, though unlike the time series results, there does in fact seem to be some benefit. 4) All the methods are improved when using ADAM---especially the LSTMs and GRUs---though GVFNs are effective even with constant stepsizes.

** Investigating Poorly Specified GVFNs
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:emp:poorlyspecified
:END:

In the previous Compass World and Forecasting experiments, the GVFNs were robust to truncation. In fact, computing one-step gradients was sufficient for good performance. A natural question is when we can expect this to fail. We hypothesize that this robustness to truncation relies on appropriately specifying the GVFs in the GVFN. Poorly specified GVFs could both (a) make it so that the GVFN is incapable of constructing a state that can accurately predict the target and (b) make training difficult or unstable. In this section, we test this hypothesis by testing several choices for the GVFs in the GVFN in Compass World.

We consider three additional GVFN specifications: two that include intentional (but realistic) misspecifications and one that should be an improvement on the Terminating Horizon GVFN. The first misspecification, which we call the \textbf{Horizon} GVFN, causes the hidden states to have widely varying magnitudes. These GVFs are similar to the Terminating Horizon GVFs, except that they do not include termination when a color is observed. This means the true expected returns can be quite large, up to $\tfrac{1}{1-\gamma}$ (e.g.,  $\frac{1}{1-0.99} = 100$) if the agent is already immediately in front of the wall with that color. The policy is to go forward, and so if the agent is already facing the wall and receives a cumulant of 1, it will see a 1 forever onward, resulting in a return of $\sum_{i=0}^\infty \gamma^i = \tfrac{1}{1-\gamma}$.

The second misspecification provides a minimal set of sufficient predictions, but ones that are harder to learn. A natural choice for this is to use the five hard-to-learn predictions themselves, which is clearly sufficient but may be ineffective because we cannot learn them quickly enough to be a useful state. We call this the \textbf{Naive} GVFN, because it naively assumes that representability is enough, without considering learnability.

Finally, we also consider a specification that could improve on the more generic Terminating Horizon GVFN, that we call the \textbf{Expert Network}. This network also has 40 GVFs, but ones that are hand-designed for Compass World. This GVFN is a modified version of the TD network designed for Compass World citep:&sutton2005temporaldifference. The GVFs are defined similarly for the 5 colours. There are 3 myopic GVFs: a myopic GVFs consists of a myopic termination ($\gamma = 0$ always) and a cumulant of the color bit. Each myopic GVF has a persistent policy, which takes one action forever. Since there are three actions there are three myopic GVFs. These myopic GVFs indicate whether the agent is right beside the color (ahead, to the left or to the right). There is 1 leap GVF where the policy goes forward always, the cumulant is again the color bit and $\gamma=1$ except when a color is observed, giving $\gamma =0$. There are 2 GVFs with a persistent policy (left, right) with myopic termination and a cumulant of the previous leap GVF's. These compositional GVFs let the agent know if they were to first turn right (or left) and then go forward, would they see the color. There are 2 leap GVFs with cumulants of the myopic GVFs. Finally, there is 1 GVF with uniform random policy with $\gamma = 0$ at a wall event and $\gamma = 0.5$ otherwise.

As a baseline, we also include what we call a \textbf{Forecast} network, which uses $k$-horizon predictions for the hidden state instead of GVFs. The architecture of the Forecast network is otherwise the same as the GVFN. We use a set of horizons $\mathcal{K} = \{1, 2, \ldots, 8\}$, for each of the non-white observations, resulting in a hidden state size of 40. To train these networks online we keep a buffer of $p+\max(\mathcal{K})$ observations, using the first $p$ observations in the BPTT calculation and the next $k$ observations to determine the targets of the network. We then recover the most recent hidden state to train the evaluation GVFs as we would with the RNN and GVFN architectures. More specifically, at time step $t$, we update state $\svec_{t-k}$ with observations $\mathbf{o}_{t-k+1}, \ldots, \mathbf{o}_{t}$.


\begin{figure}[t]
  \center
  \includegraphics[width=0.95\textwidth]{plots/gvfn/compass_world/gvfn_lc.pdf}
  \caption{
  Learning curves for \textbf{(dashed)} $p=1$ and \textbf{(solid)} $p=8$ for various GVFN specifications and the Forecast networks. The GVFN is labeled TermHorizon, to highlight that it is composed of terminating horizon GVFs. Learning rates were chosen as in Figure \ref{fig:compass}, where the left plot corresponds to using a constant stepsize and the right to using the ADAM optimizer. The errors were averaged over 30 independent runs, to get the final learning curves $\pm$ standard error.}\label{fig:compass_poor}
\end{figure}


Learning curves for all the GVFN specifications, as well as the Forecast network, with $p = 1$ and $p = 8$, are reported in Figure \ref{fig:compass_poor}. The results indicate that the specification can have a big impact. The two misspecified GVFNs perform noticeably worse than the Terminating Horizon GVFN. As expected, the Naive GVFN is eventually able to learn, with enough steps, $p = 8$ and the ADAM optimizer. It is sufficient to obtain a good state, but poor learnability prevents it from playing a useful role. The Horizon GVFN, which has potentially high magnitude GVF predictions, is closer in performance to the Terminating Horizon GVFN, but clearly worse. The Expert GVFN, on the other hand, can get to a lower error, though it does not have a clear advantage in terms of learning speed or robustness to $p$; this slower learning could again be potentially due to the fact that these expert GVFs were more difficult to learn than the simpler terminating horizon GVFs. Finally, the Forecast network performed very poorly. This is not too surprising in this environment. When considering a $k$-horizon prediction, the target is often zero, with the occasional one. This is generally a hard learning problem, as the resulting prediction loss does not provide a useful constraint. These results clearly show specifying the GVFs used to constrain the hidden state is an important consideration when using GVFNs, and could be the difference between learnable and not learnable representations.

** Comparing Recurrent GTD and Recurrent TD

TD networks with a simple TD network update rule---no backprop through time---have been shown to have divergence issues on a simple six-state domain, called Ringworld~citep:&tanner2005temporal. In fact, Gradient TD networks citep:&silver2013gradient  were introduced precisely to solve this problem. Because GVFNs are a strict generalization of TD networks, we can set the GVFN to get the same problematic setting if we use a simple TD update (RTD with $p=1$). This raises a natural question of if Recurrent TD (RTD) similarly has divergence issues, and if we need to use Recurrent GTD (RGTD).

In all of our experiments so far, we have opted for the simpler RTD algorithm, rather than the full gradient algorithm RGTD, because empirically we found little difference between the two. RTD, unlike the simple TD update rule, does in fact compute gradients back-in-time, and so should be a more sound update. Further, once we use truncated BPTT, even RGTD is providing a biased estimate of the gradient. But nonetheless RTD---which is built on the semi-gradient TD update---does drop more of the gradient than RGTD. It is likely that RGTD is needed in some cases. But it is possible that for most settings, RTD provides a reasonable interim choice between the simple TD network learning rule, and the more complex RGTD.

In this section, we test RTD and RGTD on Ringworld, to see if they perform differently on this known problematic setting.  Note that for $p=1$, RTD reduces to the simple TD network learning rule, and so we expect poor performance.

Ring World is a six-state domain~citep:&tanner2005temporal where the agent can move left or right in the ring. All the states are indistinguishable except state six. The observation vector is simply a two bit binary encoding indicating if the agent is in state six or not. The agent behaves uniformly randomly. The goal is to predict the observation bit on the next time step. The environment itself is not too difficult for state-construction; rather a particular TD network causes divergence from the simple TD update rule. The corresponding GVFN consists of two chains of compositional GVFs: one chain for always go right and one chain for always going left. In the first chain, the first GVF is a myopic GVF, that has as cumulant the observed bit after taking action Right, with $\gamma =0$. This first GVF predicts the observation one step into the future. The second GVF has the first GVFs prediction as a cumulant after taking action Right, with $\gamma = 0$. This second GVF predicts the observation two steps into the future. There are five GVFs in each chain, for a total of 10 GVFs in the GVFN.

\begin{figure}[t]
  \center
  \begin{subfigure}{0.55\textwidth}
    \includegraphics[width=\textwidth]{plots/gvfn/ring_world/learning_curve_rgtd.pdf}
  \end{subfigure}
  \caption{
    Learning curves for $p=1$ and $p=2$ averaged over 10 runs with fixed window smoothing of 1000 steps, in the Ringworld environment. Learning rates chosen from a sweep over $\alpha \in \{0.1\times1.5^i; i\in\{-10, -9, \ldots, 6\}\}$ for the RNN and learning rates $\alpha \in \{0.1\times1.5^i; i\in\{-6, -9, \ldots, 8\}\}$ and $\beta = \{0.0, 0.01\}$ corresponding to RTD and RGTD respectively. All approaches needed only $p = 2$ to learn, including the baseline RNN included for comparison.
  }\label{fig:ring}
\end{figure}

Figure \ref{fig:ring} shows the results of the Ring World experiments for truncation $p=1$ and $p=2$. The GVFNs for both RTD and RGTD needed only $p \ge 2$ to learn effectively. We also include a baseline RNN of the same architecture, that indicates that the GVFN specification does negatively impact performance. But, with even just $ p = 2$, any convergence issues seem to disappear. In fact, RTD and RGTD perform very similarly. The fact that Ringworld is not problematic for RTD is by no means a proof that RTD is sufficient, especially since Ringworld was designed to be a counterexample for the simple TD network update not for RTD. But, it is one more datapoint that RTD and RGTD perform similarly. In future work, we will be investigating a counterexample for RTD, to better understand when it might be necessary to use RGTD.

** Summary

This chapter contains some initial evidence to the effect of using GVFs to constrain the state of a recurrent network. While this provides some evidence for the /Prediction Representation hypothesis/, we do not perform a explicit test of this hypothesis. While these experiments show the initial promise of the GVFN approach to learning a state-update function, they are not extensive. In all the empirical results, the agent's were trained in a sequential manner without experience replay buffers. This possibly limited the performance possible for both the RNNs and GVFNs. The results also relied on hand designed GVFs. While several networks used simple heuristics to construct a collection of GVFs, it is yet to be tested whether these results generalize beyond the simple bit domains and synthetic forecasting domains used here.

* Discovery of GVFNs through Generate and Test
:PROPERTIES:
:CUSTOM_ID: chap:gvfn:discovery
:END:

In Chapter ref:chap:gvfn:empirical, I explored several hand designed network configurations. While a necessary first step to judge the GVFN's potential, discovering the predictive questions used to construct the state is essential to more aptly apply GVFNs to large complex problems. In this chapter, I consider the discovery question for GVFNs. While this chapter only serves as the tip of the iceberg of what is possible with discovery, I aim to provide a baseline and structure to approach the question in future work. I also describe several approaches to discovery used throughout the predictive learning literature and discuss how they might apply to the GVFN architecture directly.

Previous approaches to discovery in predictive representations have focused on finding a set of predictions that would enable the agent to answer all predictive questions accurately. This objective is trying to find a sufficient statistic of the history for all predictions, and has been discussed in various forms citep:&subramanian2022approximate. This is the approach typically taken in PSRs and a usual criteria when approaching a POMDP problem. This criteria falls naturally from the POMDP specification, where the assumption is there is a true underlying latent state which the agent can determine from enough interactions with the system.  We conjecture that finding such a state is not feasible in large complex problems, and searching for such a state would be a poor use of a finite set of computational resources. Instead, the agent should focus on finding a set of questions which is useful for the agents overarching goals---for example, maximizing the return in the control problem.

In the following section, we describe several prior approaches to discovery applicable to the GVFN framework, develop a simple approach to a discovery framework for future testing, and discuss various ways of specifying GVFs by hand for the GVFN.

** Previous Approaches

There are two main families of approaches to discovery of GVFs for GVFNs: generate-and-test and gradient descent.


*Generate and test*
is a natural algorithmic approach when considering a search problem through a complex unordered (or not obviously ordered) space. The core of the approach is to propose GVFs through a generator and approximate their utility for the downstream task through a proxy measure. This approach has been used for representation discovery citep:&mahmood2013representation,javed2020learning. The simplest setting where such a generate-and-test approach could be used is time series forecasting, as the predictions are on-policy and so policies do not have to be proposed by the generated. Further, practitioners can apply their prior knowledge in creating the cumulant and continuation functions considered by the generator. There are, however, some simple strategies for generating policies, which we discuss in Section ref:sec:gvfn:simple-disc.

A generate and test algorithm has been developed for TD networks [[cite:&makino2008online]]. The process of discovery involves creating new predictions built entirely from existing structures: senses or predictions. By building new predictions from existing predictions, it facilitates the creation of compositional structures. The system proposed in citeA:&makino2008line determines when a node (i.e. a prediction or sense) should be expanded on using three criteria. They then expand these nodes in specific ways to ask a broad set of compositional questions. TD networks do not include policies---rather they include action primitives---so the approach does not directly extend. However, the idea of iteratively creating such compositional structures does extend. For example, in this work, the expert network considered in Section ref:sec:gvfn:emp:poorlyspecified was composed of compositional GVFs. Compositional GVFs could be generated simply by using existing GVFs as the cumulant for the new GVF.


*Meta-gradient descent* uses gradient descent to learn meta-parameters that affect learning performance. The meta-parameters could correspond to initialization of a model for later fine tuning [[cite:&finn2017modelagnostic]], a set of GVF auxiliary tasks to improve representation learning in Atari cite:&veeriah2019discovery or parameterized options [[cite:&bacon2017option]]. This approach splits the problem into two optimization problems: an inner problem and an outer problem. The inner optimization consists of the usual control or prediction procedure, where the agent seeks to maximize the discounted return or lower prediction error. The outer optimization calculates gradients through this procedure, with respect to the meta-parameters.

For example, to learn a set of GVFs as auxiliary tasks, citeauthor:&veeriah2019discovery (citeyear:&veeriah2019discovery) parameterized the cumulant and continuation functions. They did not need to parameterize the policies for the GVFs as they assumed on-policy prediction: the policy \(\pi\) for the GVF is the current policy. These meta-parameters are optimized in the outer loop to produce auxiliary tasks that improve control performance in the inner loop. For our setting, we could similarly parameterize GVF questions, including the policy. This meta-gradient approach was reasonably effective for discovering GVFs as auxiliary tasks, though the procedure is expensive and has some trainability issues. Nonetheless, it is a reasonable direction for pursuing discovery for GVFNs.

** Investigating a Simple Generate and Test Strategy for GVF Discovery
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:simple-disc
:END:


\begin{wrapfigure}{R}{0.5\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{plots/gvfn/discovery/evaluator_generator.pdf}
  \caption{The discovery framework.}\label{fig:gvfn:discovery}
\end{wrapfigure}
We base this simple discovery framework on algorithms described for representation
search citep:&mahmood2013representation focusing on two main components: an
evaluator, and a generator. The evaluator is responsible for testing GVFs and
removing unused GVFs. The generator proposes new GVFs from a set of possible
GVFs. We summarize our framework in Figure ref:fig:gvfn:discovery. The key questions are how GVFs are evaluated and how new ones proposed. Our goal here is simply to demonstrate one avenue for discovery in GVFNs, rather than to develop an algorithm for discovery; we therefore opt for what we believe are some of the simplest choices.


To evaluate the usefulness of a GVF we look at the magnitude of the associated weight in the external tasks using the GVFN. We assume the state vector is used linearly to make predictions, with $\theta_j$ corresponding to state $s_j$ and so to the $j$th GVF. We evaluate all the GVFs every $N \in \Naturals$ steps and prune the lowest $\epsilon \in [0,1]$ percentage, i.e., prune $\lfloor n \epsilon \rfloor$ least useful GVFs of the full set of $\numgvfs$ GVFs. Other criteria have been proposed for evaluation, such as using traces of the weight magnitudes and considering internal weights citep:&mahmood2013representation. As mentioned above, we opt for the simplest choice that is still reasonably effective.


We generate new GVFs randomly from a set of GVF primitives. We define a set of basic types of cumulants, continuations and policies from which to randomly sample. For continuations, we consider
\textit{myopic discounts} ($\gamma = 0$),
\textit{horizon discounts} ($\gamma \in (0,1)$) and
\textit{terminating discounts} (the discount is set to $\gamma \in (0,1]$ everywhere, except for at an
event, which consists of a transition $(o, a, o')$).
For cumulants, we consider
\textit{stimuli cumulants} (the cumulant is one of the observations,
or taking on 0 or 1 if the observation fulfills some criteria (e.g. a threshold))
and \textit{compositional cumulants} (the cumulant is the prediction of another GVF).
We also use \textit{random cumulants} (the cumulant is a random number generated from a zero-mean Gaussian with a random variance sampled from a uniform distribution); we do not expect these to be useful, but rather use it to define what we call a dysfunctional GVF to test pruning.
For the policies, we propose \textit{random policies} (an action is chosen at random) and
\textit{persistent policies} (always follows one action).

The resulting GVF primitives consist of a triplet $(c, \gamma, \pi)$ where each is randomly chosen from these basic types. For example, a randomly generated GVF could consist of a myopic continuation, a stimuli cumulant on observation bit one and a random policy. This would correspond to predicting the first component of the observation vector on the next step, assuming a random action is taken. As another example, a randomly generated GVF could consist of a termination continuation with $\gamma = 0.9$, a stimuli cumulant which is 1 when the observation is zero and is otherwise zero otherwise and a persistent policy with action forward. This GVF corresponds to predicting the likelihood of seeing the observation change from active (`1') to inactive (`0'), given the agent persistently moves forward, within the horizon of about $(1-\gamma)^\inv = 10$ steps.

We could also have considered parameterized continuations, cumulants and policies and randomly sample from that set. This set, however, is large. The GVF primitives can be seen as a prior over the full set of GVFs, which is too large from which to randomly generate. Without this prior we expect the discovery approach to still work but to take even longer than the experiments we present here.

We evaluate the performance of our system on two experiments in Compass World citep:&sutton2005temporaldifference. Both experiments use the five hard-to-learn GVFs as the targets for the GVFN, introduced in Section \ref{section:experiments:compassworld}. These questions correspond to a question of ``which wall will I hit if I move forward forever?''.
The first experiment, Figure \ref{fig:compass_disc} (left), provides a sanity check that the evaluation strategy prunes dysfunctional representational units. We initialize the GVF network with 200 GVFs: 45 used to form the expert crafted TD network citep:&sutton2005temporaldifference, and 155 defective GVFs predicting noise $\sim \mathcal{N}(0,\sigma^2)$. We report the learning curve and pruned GVFs over 12 million steps. The second experiment, Figure \ref{fig:gvfn:compass_disc} (right), uses the full discovery approach to find a representation useful for learning the evaluation GVFs. We report the learning curves of the evaluative GVFs over 100 million steps.

These experiments have many similarities to the experiments above, but there is one key differences worth noting. Instead of using RTD or RGTD, we used TD($\lambda$); see Appendix \ref{app_tdlambda} for the update equations. We found that this was sufficient to learn the expert network specification in a reasonable number of steps, and is significantly simpler than the other algorithms. Note that we did not use this algorithm in the above comparisons with RNNs, for two reasons. First in the cases where the target was not a return, it is not possible to use eligibility traces, as they are designed for predicting expected returns. Second, as far as we are aware, the eligibility trace calculation for neural networks with several output nodes has not been formally derived nor tested.
%Another difference is in the included baselines. We include two. The first is performance of learning without any recursive state (from raw observations), and the second is a strategy that does random generation once ($\epsilon = 0.0$).

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{plots/gvfn/discovery/combined_cull_gen.pdf}
  \caption{\textbf{(left)} Pruning predictive units occurs every million steps with no regeneration $\alpha=0.001, \lambda=0.9, \epsilon=0.1, \sigma^2 = 1$ \textbf{(right)} Learning curves of the evaluative GVFs $N = 1000000, \epsilon=\text{labeled}, \alpha=0.001, \lambda=0.9, n = 100$, over 5 runs with standard error denoted by the shaded region.
    }\label{fig:gvfn:compass_disc}
\end{figure}

The results indicate that even a simple generate and test approach can be effective for discovery in GVFNs. The first figure shows that the pruning approach gradually removes the dysfunctional GVFs, without pruning the expert GVFs. Eventually, once the agent has mostly removed all the dysfunctional GVFs, it is then forced to prune the expert GVFs and prediction performance begins to drop. Of course, in practice, the agent would not prune all its GVFs; in this experiment we simply continue the pruning until the end to avoid biasing when we stop the agent.
The second plot shows that iteratively pruning and generating new GVFs significantly improves on using an initial random set. For $\epsilon = 0.2$, which means about \(20\%\) of GVFs are pruned in each pruning phase, the prediction error continues to decrease until it almost reaches 0 and is almost as good as the set of hand-design GVFs used in previous experiments.

The goal of this experiment was to answer: is it possible to discover useful GVFs for a GVFN, even in simple settings? A negative answer would mean that GVFNs might have limited applicability. A  demonstration that it is possible provides some evidence that this is a tractable problem for which even simple solutions can help us make traction. This demonstration, however, by no means shows an ideal or even efficient algorithm and there is ample room for improvement. Primarily, the random generation strategy does not take into account the current set of proposed predictions, potentially resulting in redundancy. A more principled method would look to generate a wide variety of predictions dependent on the current set of predictions.
%This would involve measuring how related GVF questions are from their specification is not particularly straightforward.
Another issue is the proxy used to determine a prediction's usefulness. Currently, the system will prune GVFs that are not directly useful, even if they are the cumulant for a useful GVF. The cumulant for the useful GVF is replaced by a new random GVF. This could reduce the quality of the predictive state or cause other instabilities within the GVFN. A simple approach is to define usefulness based also on compositional utility, not just on utility for the prediction task. The usefulness of a GVF should be higher if it is used by a GVF that is itself heavily relied on for accurate predictions, versus if it is only used by less useful GVFs.

** Heuristics to specify GVFNs
Through testing GVFNs in several domains we have developed some rules of thumb for choosing GVFs which can be used today. In our time-series experiments, we found selecting GVFs with constant \(\gammaj{j} \in [1 - 2^{-j}]\) to be surprisingly effective across the settings with fixed policies---namely the time series datasets. This is encouraging as these specifications on the surface seem simpler to discover than something as complex as the Expert network in Compass World. A set of discounts selected linearly across a range was also effective. We also found that including GVFs which have a pseudo-termination at a known event (known due to expert knowledge) and a cumulant which is only active at this event improved learning performance considerably (see the performance of the Terminating-Horizon network in Section ref:sec:gvfn:emp:poorlyspecified).


* DONE Composite General Value Functions
CLOSED: [2023-02-22 Wed 14:34]
:PROPERTIES:
:CUSTOM_ID: chap:composite
:END:

In Chapter ref:chap:gvfn:empirical, I explored several hand designed network configurations. This lead to several observations, including compositional GVFs have considerable representational power if the predictive questions can be answered. In the following Chapter, I focus on the open question of "what do GVFs composed together in chains predictively represent?" While I don't answer this question conclusively for all possible chains GVFs, I provide explorations for two important types of composite GVFs: those which have constant discounts, and those which have terminating discounts.


# Reinforcement learning is built on predicting the effect of behavior on future observations and rewards. Many of our algorithms learn predictions of a cumulative sum of (discounted) future rewards, which is used as a bedrock for learning desirable policies. While reward has been the primary predictive target of focus, TD models [[cite:&sutton1995td]] lay out the use of temporal-difference learning to learn a world model through value function predictions. Temporal-difference networks [[cite:&tanner2005thesis;&sutton2005temporaldifference]] take advantage of this abstraction and build state and representations through predictions. [[citeauthor:&sutton2011horde]] ([[citeyear:&sutton2011horde]]) and [[citeauthor:&white2015developing]] (citeyear:&white2015developing) further the predictive perspective by developing a predictive approach to building world knowledge through general value functions (GVFs).

Learning predictions of any real-valued signal the agent has access to also opens the possibility of asking compositional predictive questions [[cite:&white2015developing]]. A compositional question is one whose target is dependent on another prediction internal to the agent. Compositions expand the possible range of predictive questions we can specify as a GVF [[cite:&sutton2005temporaldifference;&rafols2006temporal;&zheng2021learning;&white2015developing;&schlegel2021general]]. While this may suggest the GVF framework is limited in what questions can be asked, the limitations are necessary so the predictions can be trained /independent of span/ [[cite:&vanhasselt2015learning]]. Learning independent of span means the target can be learned using online algorithms regardless of the effective horizon of the prediction. Adding layers of compositional questions have improved the learning in predictive representations [[cite:&rafols2006temporal;&schlegel2021general]], and improved the performance of deep reinforcement learning through auxiliary tasks [[cite:&zheng2021learning]]. In the automatic specification of learning targets compositions are thought to provide a way for the agent to build complexity [[cite:&schlegel2021general;&veeriah2019discovery;&zheng2021learning;&kearney2022what]], but often these architectures don't leverage compositions for stability concerns [[cite:&tanner2005temporal;&tanner2005td;&schlegel2017stable;&schlegel2021general]]. 


As well as improving behavior empirically, compositions can provide semantic depth. An excellent example of this can be seen in option-extended temporal difference networks [[cite:&rafols2006temporal]], and later explored again in [[cite:&schlegel2021general]]. The example is centered in an environment where the agent has a low-powered visual sensor and needs to learn its directionality from the painted walls. Each cardinal direction has a different colored wall. The first layer of predictions the agent makes is to predict what color it will observe if it were to drive straight. The second layer are myopic predictions which ask what the first layer's prediction will be after turning clockwise (or counter-clockwise). The second layer allows the agent to predict which walls are to its sides as well as the wall in the direction the agent is facing. These predictions cannot be specified in the usual GVF framework, but can be easily constructed through compositions. While this may be ``repeated information'' in a sense, the extra learning objectives makes the learning properties of the predictive representation better as compared to other specifications [[cite:&schlegel2021general]].

As algorithms for the automatic discovery of complex question networks continue to push the boundaries of what questions are considered by the agent, the properties of compositions should be better studied. When searching for what to learn the questions an agent eventually retains will be dependent on the agent's ability to learn the predictions. While it is clear questions that naturally diverge (say setting the discount $\gamma=1$) should be avoided, other problems, such as the scale of a target, could be equally as problematic when using function approximation (i.e. end-to-end neural networks). This could mean important predictions are disregarded because the agent is unable to learn the answer without proper strategies to normalize the prediction's magnitude. Better strategies for learning and normalizing predictive targets will come from understanding the effective discount schedule (or emphasis) compositional predictions will have on the targets.

In this report, we consider the effect of compositions on the sequence of discounts, and relegate the effect of off-policy importance weights to future work. We first analyze the sequence of discounts over any number of compositions and constant discounts. We then analyze this sequence to better understand how it emphasizes parts of the data stream. Surprisingly, the effective discount for constant discount compositions have a form which can be described analytically. While this does not include the full spectrum of discount functions, it provides a first step towards understanding compositions. Next we look at simulations using more complex state-dependent discount functions using a simple consistent sequence and two timeseries datasets. In these simulations we focus on the effect of applying the same discount function a large number of times, looking to see if the shape of the returns become regular over the compositions. Finally, several future directions and questions are posed.
{{{c}}}
{{{c}}}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/compgvfs/seq_plus_cw.pdf}
  \caption{(\textbf{left}) The effective discount for $n$ compositions
    normalized by the maximum value found in section
    \ref{sec:compgvfs:analyze}. (\textbf{middle, right}) The cycle world
    simulations, with top graph as the cumulant and subsequent plots
    $n$ compositions with constant and terminating discounts
    respectively.} \label{fig:compgvfs:seq-cw}
\end{figure}

** Analyzing the sequence 
:PROPERTIES:
:CUSTOM_ID: sec:compgvfs:analyze
:END:

In this section, we restrict to the setting where we have an infinite sequence of sensor readings $\mathbf{x} = \{x[0], x[1], \ldots,
x[t], \ldots, x[\infty]\}$ where $x[i] \in [x_{\text{min}} , x_{\text{max}}]$ and a constant discount $\gamma$. The return of this signal starting at a time step $t$ is
{{{c}}}
$V[t] = \sum_{k=0}^\infty x[k] \gamma[k-t]$
{{{c}}}
where $\gamma[k] = \gamma^{k-1}$ for $k >= 1$ and $0$ otherwise. This framing of the return is slightly different from the typical presentation. Specifically, we reinterpret the return as a convolution beteween $\gamma$ and $x$ [fn:: In digital signal processing [[cite:&oppenheim2010discrete]] often the convolution, in this case $\gamma$, is mirrored across $t$ and the inifinte sequence of sensor readings is $\mathbf{x} = \{x[-\infty], \ldots, x[t], \ldots, x[\infty]\}$. The corresponding convolution would be $V[t] = \sum_{k=-\infty}^\infty x[k] \gamma[t-k]$ which would change how we define the sequence of $\gamma$. To be consistent with the reinforcement learning literature, we don't follow this here and instead implicitly define $\gamma$ as the mirrored version and only consider the sequence starting at $k=0$.]
and shift the discount sequence over the sensor readings. This implicitly defines an infinite sequence of predictions $V[t]$. In the above equation, if we replace the sequence $x$ with the sequence of predictions $V$, we get a new set of predictions and for any number of compositions $n$ we have
{{{c}}}
$V^n[t] = \sum_{k=0}^\infty V^{n-1}[k] \gamma[k-t]$.
{{{c}}}
Expanding this equation we can define the general sequence of effective discounts for $n$ compositions and the corresponding return as
{{{c}}}
{{{c}}}
\[
  \gamma^n[k] = \begin{cases}
    0 \quad \mbox{ if } k < n \\
    \frac{\prod_{i=1}^{n-1} (k-i)}{(n-1)!} \gamma^{k - n}
  \end{cases} \quad\quad V^n[t] = \sum_{k=0}^\infty x[k] \gamma^n[k - t]
\]
{{{c}}}
where $\gamma^1[k] = \gamma[k]$ defined above and $V^n[t]$ is the target of the $n$th composition at timestep $t$. For any value $n$ there are two sequences multiplied together. The original discounting shifted by the number of applications $\gamma^1[k-n]$ and a diverging series
{{{c}}}
\[
  Q^n[k] = \frac{\prod_{i=1}^{n-1} (k-i)}{(n-1)!} = \frac{\Gamma(k)}{\Gamma(k-n+1)\Gamma(n)}
\]
{{{c}}}
where $\Gamma(k) = (k-1)!$ for $k \in \mathbb{Z}$ is known as the Gamma function, and can be used to analyze the function with $k \in \mathbb{R}$.

We know for any particular application of the convolution $\gamma$ on a series with known domain $[x_{\text{min}} , x_{\text{max}}]$ the value function can take values bounded by $V^1[t] \in [\frac{x_{\text{min}}}{1-\gamma}, \frac{x_{\text{max}}}{1-\gamma}]$. This extends to $n$ compositions in a straightforward way where the range of the value function becomes $V^n[t] \in [\frac{x_{\text{min}}}{(1-\gamma)^n}, \frac{x_{\text{max}}}{(1-\gamma)^n}]$. While normalizing the value function to take values within in the range $[0,1]$ has been used in various settings [[cite:&schlegel2021general]], as we add more compositions we see the effective range of values shrinking considerably.

Given the effective discounting sequence above, we can begin to piece together the which observations are emphasized in the predictions. The first 100 steps of the effective discount function for several values of $n$ can be seen in figure ref:fig:compgvfs:seq-cw. These sequences are normalized to be in the range $[0,1]$ for a visual comparison. The emphasis becomes increasingly spread as $n$ increases, with the peak of this function moving further to the future at a consistent rate.

To find the maximum value we take the derivative of the log of the sequence with respect to $k$ getting
{{{c}}}
\[\frac{\delta}{\delta k} \ln \gamma^n[k] = \psi(k) - \psi(k-n+1) + \ln\gamma\]
{{{c}}}
where $\psi(z+1) = H_{z} - C$ is the digamma function, $H_{z} = \sum_{i=1}^z \frac{1}{i} \leq \int_{1}^z \frac{1}{x} dx = ln(z)$ is the Euler harmonic number, and $C$ is the Euler-Mascheroni constant. Using the approximation above, we can find where we should expect the maximal value is (to an approximation) $k = hn - (h-1)=h(n-1)+1$, where $h=\frac{1}{1-\gamma}$ is sometimes known as the horizon of discount $\gamma$. Of course this is an approximation from above and the real value falls in $k \in [h(n-1), h(n-1) + 1)]$.

** Empirical observations

While we can describe the effective discount for composing constant discount predictions, the same techniques are difficult to apply to a non time-invariant discount (i.e. state-dependent discounts [[cite:&white2015developing;&white2017unifying;&sutton2011horde]]). Instead, in this section we look at the ideal returns of various signals using constant discounting and a terminating discounting functions. We use three datasets moving from highly synthetic to real-world robot sensori-motor data. The goal of this section is to show the non-intuitive behavior of compositions to motivate further analysis and exploration. All code can be found at \url{https://github.com/mkschleg/CompGVFs.jl}. Below $\gamma = 0.9$ unless otherwise stated.
{{{c}}}
{{{c}}}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/compgvfs/mso_cb.pdf}
  \caption{(\textbf{left two}) Returns of the multiple sinusoidal
    oscillator (MSO) synthetic data set with constant and terminating
    discount respectively. The gray vertical lines are where the
    return terminates. (\textbf{right two}) Returns of Critterbot
    data set over the light3 sensor with constant and terminating
    discount respectively.} \label{fig:compgvfs:critterbot}
\end{figure}
{{{c}}}
{{{c}}}
The first series is based off the cycle world, where the agent observes a sequence of a single active bit followed by $9$ inactive bits, where the length of the sequence is $m=10$. The cumulant is the observation itself, and in this report we learn using TD($\lambda=0.9$) with learning rate $\alpha=0.1$ and an underlying tabular representation where each component is the place in the sequence. We learn two chains of compositions. The first is that of the continuous discounting described above, and the second is a series of discounts which terminate (i.e. $\gamma[t]=0$) when the observation is active. The predictions of a single run can be seen in figure ref:fig:compgvfs:seq-cw. For the constant discount, as the number of compositions increases we see the prediction sequence converge to what looks to be a sinusoid with frequency of $10$, and amplitude driven by the analysis above. We expect this to be the case following from the central limit theorem. For the terminating discount, the wave form is more interesting. The first layer of predictions look very similar to the constant discount with amplitude shifted by $\frac{\gamma^{m}}{1 - \gamma^{m}}$. But as there are more compositions the effect seems to be the prediction is at its height farther away from the active bit. As the agent gets closer to the observation, the sequence of summed values is shorter leading to smaller values. Given the sequence we use it is easy to mistake this as the agent creating a trace of the cumulant, but we must remember the prediction is about future cumulants.

Next we use a subset of the Critterbot dataset [[cite:&modayil2014multitimescale;&white2015developing]], focusing on light sensor 3 in figure ref:fig:compgvfs:critterbot. This gives a sequence of spikes similar to the cycle world sequence and a long pause in-between consistent saturations of the light sensor. We are able to see with the current setting the predictions look more like shifted and spread spikes. But with many more compositions, the return reverts to a similar form as before. The terminating discounts (with termination at sensor saturation $x[t+1] > 0.99$) provides a nice demonstration of how the returns are predicting the signal, just with a decaying prediction instead of the usual growing prediction. The results are similar in the multiple sinusoidal oscillator [[cite:&jaeger2004harnessing]]. We use a slightly different terminating discount where the return terminates when the previous normalized prediction is $y^{n-1}[t+1] > 0.9$ rather than when the observation is saturated. While there are decays as the MSO sequence peaks, as we increase the depth of the composition, these periods are less frequent. Deep compositions may indicate parts of the sequence where there are fewer saturations in the original sequence.

** Future Directions

This work suggests a number of interesting research directions and questions. While we mostly analyzed the sequence on discrete steps and applications of the filter, the general form does lend itself to continuous and complex values of $n$ and $k$. In a similar vein, we focused on real valued exponential discounting while several discounting schemes exist which could be applied to our formulation. We are particularly interested in complex discounting [[cite:&deasis2018predicting]] and hyperbolic discounting [[cite:&ainslie1992hyperbolic;&fedus2019hyperbolic]]. Applying a diverse set of discounting schemes in compositions provide an interesting way to extend the power of value functions while maintaining learnability through efficient algorithms like temporal-difference learning. 

The approach used in this paper is unable to analyze state-dependent discount functions. One way around this might be in analyzing truncated sequences and taking an expectation over a distribution of sequence lengths. This might lead to a expected effective discounting sequence, but how this will interact with an underlying Markov process is unclear. This is an important next step for understanding the effects of compositions in general value functions, and could also help in analyzing off-policy compositions.

Finally, the return can be re-interpreted as a convolution over the infinite sequence of observations. While this interpretation was only used to better the notation in this manuscript, further connections to convolutions and digital signal processing should be explored. Better filter designs might inspire different discounting schedules to squeeze more information from the data stream. We also have only analyzed these convolutions in the time domain. The frequency domain might give us more insight into how consistent signals like the cycle world dataset will be effected by compositions. 

* IN-PROGRESS [#B] Importance Resampling for Prediction in Reinforcement Learning
:PROPERTIES:
:CUSTOM_ID: chap:resampling
:END:

One lingering issue that plagues not only GVFNs but the community of learning answers to predictive questions generally is the variance induced by off-policy prediction through importance sampling. There have been several alternatives to straight importance sampling ratios, many of which have been proposed for a more standard policy evaluation setting (i.e. where the target and behavior policies are close together on the simplex). In this Chapter, I propose importance resampling as a off-policy prediction algorithm to mitigate the update variance of importance sampling while also being a consistent estimator.

** Introduction

An emerging direction for reinforcement learning systems is to learn many predictions, formalized as value function predictions contingent on many different policies. The idea is that such predictions can provide a powerful abstract model of the world. Some examples of systems that learn many value functions are the Horde architecture composed of General Value Functions (GVFs) \citep{sutton2011horde,modayil2014multi}, systems that use options \citep{sutton1999between,schaul2015universal}, predictive representation approaches \citep{sutton2005temporal,schaul2013better,silver2017thepredictron} and systems with auxiliary tasks \citep{jaderberg2017reinforcement}. 
Off-policy learning is critical for learning many value functions with different policies, because it enables data to be generated from one behavior policy to update the values for each target policy in parallel. 

The typical strategy for off-policy learning is to reweight updates using importance sampling (IS). For a given state $\state$, with action $\action$ selected according to behavior $\mu$, the IS ratio is the ratio between the probability of the action under the target policy $\pi$ and the behavior: $\frac{\tpolicy(\action|\state)}{\bpolicy(\action|\state)}$. The update is multiplied by this ratio, adjusting the action probabilities so that the expectation of the update is as if the actions were sampled according to the target policy $\pi$. Though the IS estimator is unbiased and consistent \citep{kahn1953, rubinstein2016simulation}, it can suffer from high or even infinite variance due to large magnitude IS ratios, in theory \citep{andradottir1995} and in practice \citep{precup2001offpolicy,mahmood2014weighted,mahmood2017multi}.


There have been some attempts to modify off-policy prediction algorithms to mitigate this variance.\footnote{There is substantial literature on variance reduction for another area called off-policy policy evaluation, but which estimates only a single number or value for a policy (e.g., see \citep{thomas2016data}). The resulting algorithms differ substantially, and are not appropriate for learning the value function.} 
Weighted IS (WIS) algorithms have been introduced \citep{precup2001offpolicy,mahmood2014weighted, mahmood2015off}, which normalize each update by the sample average of the ratios. These algorithms improve learning over standard IS strategies, but are not straightforward to extend to nonlinear function approximation. In the offline setting, a reweighting scheme, called importance sampling with unequal support \citep{thomas2017importance}, was introduced to account for samples where the ratio is zero, in some cases significantly reducing variance.  
Another strategy is to rescale or truncate the IS ratios, as used by V-trace \citep{espeholt2018impala} for learning value functions and Tree-Backup \citep{precup2000eligibility}, Retrace \citep{munos2016safe} and ABQ \citep{mahmood2017multi} for learning action-values. Truncation of IS-ratios in V-trace can incur significant bias, and this additional truncation parameter needs to be tuned. 

An alternative to reweighting updates is to instead correct the distribution before updating the estimator using weighted bootstrap sampling: resampling a new set of data from the previously generated samples \citep{smith1992a, arulampalam2002}. Consider a setting where a buffer of data is stored, generated by a behavior policy. Samples for policy $\tpolicy$ can be obtained by resampling from this buffer, proportionally to $\frac{\tpolicy(\action|\state)}{\bpolicy(\action|\state)}$ for state-action pairs $(\state,\action)$ in the buffer. 
In the sampling literature, this strategy has been proposed under the name Sampling Importance Resampling (SIR) \citep{rubin1988using, smith1992a, gordon1993}, and has been particularly successful for Sequential Monte Carlo sampling \citep{gordon1993,skare2003improved}. Such resampling strategies have also been popular in classification, with over-sampling or under-sampling typically being preferred to weighted (cost-sensitive) updates \citep{lopez2013aninsight}.
%which samples data from the prior distribution according to the probability mass function (PMF) with sample probabilities $p(x_i) \propto \rho(x_i)$. 
%This is a variant on the well known bootstrap resampling procedure \citep{efron1982jackknife, smith1992a} where instead of sampling the data $x_i$ uniformly, the SIR algorithm does a weighted resampling according to the above PMF. 

A resampling strategy has several potential benefits for off-policy prediction.[fn:: We explicitly use the term prediction rather than policy evaluation to make it clear that we are not learning value functions for control. Rather, our goal is to learn value functions solely for the sake of prediction.] Resampling could even have larger benefits for learning approaches, as compared to averaging or numerical integration problems, because updates accumulate in the weight vector and change the optimization trajectory of the weights. For example, very large importance sampling ratios could destabilize the weights. This problem does not occur for resampling, as instead the same transition will be resampled multiple times, spreading out a large magnitude update across multiple updates. On the other extreme, with small ratios, IS will waste updates on transitions with very small IS ratios. By correcting the distribution before updating, standard on-policy updates can be applied. The magnitude of the updates vary less---because updates are not multiplied by very small or very large importance sampling ratios---potentially reducing variance of stochastic updates and simplifying learning rate selection. We hypothesize that resampling (a) learns in a fewer number of updates to the weights, because it focuses computation on samples that are likely under the target policy and (b) is less sensitive to learning parameters and target and behavior policy specification.

In this work, we investigate the use of resampling for online off-policy prediction for known, unchanging target and behavior policies. We first introduce Importance Resampling (IR), which samples transitions from a buffer of (recent) transitions according to IS ratios. These sampled transitions are then used for on-policy updates. We show that IR has the same bias as WIS, and that it can be made unbiased and consistent with the inclusion of a batch correction term---even under a sliding window buffer of experience. We provide additional theoretical results characterizing when we might expect the variance to be lower for IR than IS. We then empirically investigate IR on three microworlds and a racing car simulator, learning from images, highlighting that (a) IR is less sensitive to learning rate than IS and V-trace (IS with clipping) and (b) IR converges more quickly in terms of the number of updates.




* TODO [#B] [0/5] Conclusions and Final Remarks

This thesis set out to uncover some of the key discrepancies between how recurrent models work in the supervised learning setting and the continual reinforcement learning setting. In this chapter, I will summarize the main contributions made to agent-state construction in the continual RL setting. I will discuss potential future directions and propose open questions and directions for developing methods of agent-state construction based on the work presented in this thesis.

** TODO Summary of Contributions

- The work presented in this thesis can be seen in two parts
1. Understanding how action embeddings effect state construction
2. Developing a predictive approach in recurrent networks using general value functions
   - Defining a structure to test this approach against RNNs
   - Developing algorithms to learn with the induced constraints
   - Comparing a series of predictive methods and "free" recurrent architectures in a set of continual prediction tasks
   - Discussing possible directions for discovering important predictive questions for representation
   - Exploring the consequences of composite GVFs and their associated targets
   - Proposing an algorithm for estimating off-policy predictions.

** TODO Future directions and work
*** TODO Open Problems for Recurrent Architectures in RL
:PROPERTIES:
:CUSTOM_ID: sec:arnn:open-problems
:END:

Recurrent architectures are often taken off-the-shelf from the supervised learning setting for use in reinforcement learning. While this has been moderately successful, the RL problem poses challenges not often considered by supervised learning. Below we discuss three interesting properties of an RL system, and how they affect learning using recurrent networks.

*Practical Online Recurrent Learning:*

In reinforcement learning, it is desirable to learn as much about the most recent experience before selecting an action (i.e. to learn online and incrementally). Learning efficiently online enables adapting behavior in real time and scaling to massive data-streams and architectures. This puts pressure on the learning system to update the weights within a set amount of time so the system can act citep:&sutton2011horde;&white2015developing, which is often not a concern in the supervised setting. In settings where an agent must move around its environment independently, the on-board computational system can be heavily constrained by the power of the processor as well as limited energy from the battery. An algorithm whose computational and memory complexity scales independently of the sequence length (without the quadratic complexity on size of the network as real time recurrent learning citep:&williams1989learning) and could be applied online-incrementally would be a major breakthrough in using recurrent architectures for RL and computationally constrained systems generally. A detailed discussion on relevant literature is in Appendix \ref{app:ltd}.

*Active Data Collection Matters:*

Imagine an agent in a hallway with recognizable observations only at the beginning and end of the hallway, much like our TMaze environments. The agent must learn a state update which spans at least the length of the hallway. But this is in the best case scenario when the agent prioritizes making it to the end of the hallway. In reality, our agents will randomly explore the hallway until the end, often extending the length of the sequence the agent needs to learn over. The interaction between the agent's behavior (or exploration) and the difficulty of training under partial observability with a recurrent agent is currently unexplored. Active data collection strategies could mitigate the length of long-temporal dependencies, which would show massive improvements in our agent's learning efficiency and ability.

\begin{wrapfigure}[20]{r}{0.4\textwidth}
  \includegraphics[width=0.9\textwidth]{./plots/arnns/figures/dirtmaze_learn_intervention.pdf}
  \caption{Average success over the intervention taking the go forward activation and starting in the eastward position. {\bf (Naive Strategy)} Using the evaluated intervention over 60k steps for training, {\bf (Hand Designed)} a sequence of hand designed interventions to build up to the final evaluation intervention over 60k steps.} \label{fig:dirtmaze_inter}
\end{wrapfigure}


To show the potential of active data collection, we will briefly revisit the Directional TMaze. Start with an agent who has learned the base task of the Directional TMaze environment. If we force the agent to take specific actions and to start in a specific orientation at the beginning of the episode, we could feasibly teach the agent to artificially extend the horizon of its policy without increasing the length of the training sequence. See Figure \ref{fig:dirtmaze_inter} for preliminary results of how behavior can extend the horizon of the agent's policy. In this experiment, trained multiplicative agents are paced through a set of interventions. The naive strategy uses epsilon greed after forcing the agent to step forward twice down the hallway. The hand designed sequence, instead guides the agent through a series of forced actions to build to the final desired policy. This simple experiment shows the potential for slowly extending the temporal horizon of a policy without adjusting the truncation value by intervening on the agent's behavior.


*Insight Beyond Learning Curves:*


Learning curves provide little understanding of an agent's learning process and this likely limits algorithmic progress in partially observable settings. Unfortunately, such metrics can't be used to address more complicated questions about the agent and its behavior. While searching for SOTA is admirable, deeper questions about the internal learned structures and behaviors of our agents are often, but not always, ignored. Analyzing the internal dynamics of an agent with recurrent architectures is uniquely challenging in reinforcement learning. Some challenges include (see Appendix \ref{app:understanding} for details):
# \begin{itemize} % [topsep=0pt,itemsep=-1ex]
#   \setlength\itemsep{-0.0em}
#   \setlength\topsep{0.0pt}
- Generating data for evaluating and analyzing representation learning is an especially difficult problem for agents with recurrence. The data generated must be coherent trajectories the agent may potentially experience in the environment, meaning a (or several) data generating policy must be selected to provide coverage over the space of agent-environment interactions
- Current tools for analyzing state representations are designed for NLP citep:&karpathy2015visualizing;&ming2017understanding and are ill-suited for analyzing the link between the environment, the agent's state, and the behavior policy.
- Analyzing the behavior of our agents through performance metrics leaves many questions unanswered: How the agent might be behaving? When does the agent make a long-term decision? In what circumstances might the agent's policy fail?
# \end{itemize}

# \section{Insight Beyond Learning Curves} \label{app:understanding}
Learning curves showing the agent's performance, usually through episodic return or prediction error, over the agent's lifetime has been the primary method algorithms are compared. Unfortunately, such metrics can't be used to address more complicated questions about the agent and its behavior. While searching for SOTA is admirable, deeper questions about the internal learned structures and behaviors of our agents are often, but not always, ignored. Analyzing the internal dynamics of an agent with recurrent architectures is uniquely challenging in reinforcement learning.

One challenge is how data is generated. Unlike SL whose data is usually a dataset designed ahead of time, RL generates data through interactions with an environment whose underlying dynamics are likely unaccessible to the system designer. While randomly generated data, in combination with tools from NLP citep:&karpathy2015visualizing;&ming2017understanding, can give us some insight into how our agent's perform, see section \ref{sec:learnability}, extending the analysis to larger domains could leave large parts of the agent-environment interactions unseen. 

While we provide some representational analysis in the prediction setting, further results in the control setting would be even more beneficial. Unfortunately, many of the analysis tools we considered require the ``correct'' target at a given time. In the control setting, even when the underlying dynamics of an environment can be fully specified (say in lunar lander) a notion of what the right action is at a given time can be extremely difficult to discover. Future work should go into analyzing the link between histories, agent state, environment state, and behavior.

Even when analyzing the behavior of our agent, using the performance metric as the primary measure is deeply flawed. This type of analysis fails to address questions such as: How the agent might be behaving? When does the agent make a long-term decision? In what circumstances might the agent's policy fail? Analyzing the agent as a non-linear coupled system with the environment through a series of dynamical equations could lead to further insight on conditions which lead the agent to behave in certain ways or when certain decisions are made by the agent. cite:&beer2003dynamics develop a series of questions and experiments to analyze an artificial agent from this perspective in a simple catcher like domain. While these tools would be difficult to apply to real-world problems, using them in simulations could provide useful insight into a full description of the agent's learned policy.

Because of the above challenges there are several lingering questions about these types of agents left unanswered in these domains. 1) Under what conditions will the agent's policy fail in an environment? 2) How robust are the policies to out-of-distribution events and how does this effect the hidden state?  3) What algorithms do the learning process discover to solve the domains reliably? 4) Is the model stable over a long training period or in a continual domain? 5) When does the agent make a decision, and does the agent stick to this decision? We believe answering these questions and more can lead to better understanding of recurrent agents as well as pathways to better algorithms for training such agents.


**** Architectural Choices
:PROPERTIES:
:CUSTOM_ID: sec:arnn:arch-choice
:END:
Below are several architectural choices we made which should be empirically explored in future work:

*Cell architecture:* As exemplified in this paper, the architectural choices made in the supervised learning setting may not be the best suited for the RL setting. Here we focused on one simple architectural choice, how to incorporate action into the state, but show sometimes massive improvements in the networks ability to predict and control. Sections \ref{sec:combining} and \ref{app:sec:learning_bias} explore this further, but future work should investigate novel RL based cell types.
  
*The woes of the experience replay buffer:* Current deep learning, including recurrent architectures, in reinforcement learning include the need for an experience replay buffer. While a learning algorithm which overcomes this limitation would likely be preferable, in the short term cohesive strategies for combining an experience replay with recurrent architectures should be empirically explored. There are two major approaches currently: 1) using the stale traces, or 2) warming up the agent from the beginning (or some number of time steps prior) of an episode citep:&hausknecht2015deep. We use a third strategy here (using gradient information to refresh the hidden state to minimize the objective), but found little difference between this and the stale approach. For much more insight and discussion on this choice see cite:&kapturowski2019recurrent.
  
*Target networks and state:* How should we initialize the hidden state for a target network? In this paper, we used the state stored (in the experience replay buffer) for the main model.
  
*To continually learning, or to not:* The trade-offs between a continually learning and non-continually learning agent is extremely important for both recurrent and feed-forward architectures. In this paper, we chose to report results when the agent is still learning, but only on stationary domains. Any learning system should track the environment as it changes, or as the agent experiences novel states. Recurrent architectures have an added problem that the hidden state is an accumulation of the agent's entire history, meaning novel observations could irreparably harm the hidden state if it is not reset at regular intervals (i.e. in an episodic domain). This will potentially harm the agent's performance for its entire lifetime if it is unable to adjust it's weights accordingly. This is not the case in a feed-forward network, where novel observations will not effect the long-term behavior of an agent in known parts of the state space. This effect is understudied in the literature for recurrent agents, but is an important aspect of deploying recurrent RL agents in real world systems.

*Objectives matter:* It is known that some objective functions are more learnable in both the fully and partially observable settings citep:&mozer1991induction;&vanhasselt2015learning. Auxiliary tasks are often also used to augment the networks objective function citep:&jaderberg2017reinforcement, or to constrain the learned state citep:&schlegel2021general. Which objective functions, auxiliary tasks, and learning algorithms are more best when applied to training a recurrent networks?
*** TODO Understanding the intersection between predictive and non-predictive agent-state construction
*** TODO Discovery and the form of GVF prediction targets
*** TODO Beyond a synchronous predictive network 

* Postamble                                                          :ignore:

#+begin_export latex
\printbibliography
#+end_export

* Appendix :ignore:

#+begin_export latex
\appendix
#+end_export

* TODO [#B] Details and extended ideas for incorporating actions in recurrent networks


* Graveyard                                                        :noexport:
** Background graveyard
The environment evolves according to unknown dynamics which the agent has influence through its actions.


in response to the action and other environmental properties unknown to the agent. While there is a constant (likely dense) stream of experience the agent perceives, there are regularities on which the agent can use to predict and control in the future. The regularities can take many forms, but must be learned through experience. We call the learned internal representation of recognized regularities as *agent-state*.

# The *environment* is large. So large that the agent will never perceive all the experience afforded to it through its lifetime. This world is likely stochastic and non-stationary, but is filled with regularities for the agent to learn and use to 
# # , with many hidden states to determine its properties.

# The environment can be the whole world the agent may experience, or a subsection of this world that is partitioned for the agent by a *practitioner*.

The *practitioner* is you and me. Specifically, a practitioner is one who is creating the *agent* for an environment. The agent observes the environment from an egocentric perspective, meaning the agent doesn't observe the full set of hidden properties of the environment and must maintain its current beliefs of the world internally through what is sometimes referred to as *agent-state* (we use state and agent-state interchangeably, making sure to emphasize when discussing the environment state). In MI research, the goal is to construct agents which can behave in an environment, usually to accomplish a specific goal set by the practitioner. We seek solutions, algorithms, and systems which can do solve goals with as little intervention by the practitioner as possible {{{citeplease bitterlesson}}}. This form of machine intelligence has been discussed before, even using it to form a centralized definition of intelligence and how to create such an intelligence {{{citeplase richintelligence}}} [[cite:&silver2021reward]].

While the practitioner can set a large set of goals for the agent, we are particularly interested in an agent's ability to predict and control its datastream, which the construction of agent state is an important aspect of. As previously stated, this thesis focuses on learning agent state using recurrent networks and investigate several assumptions developed in other problem settings and how they apply to the RL problem.

*An Example*
One common example we continuously tread in this thesis is that of a small robot on wheels behaving in a room. This is a common example throughout AI and is also known as the vacuum robot {{{citeplease ModernAI}}}, or lovingly known as the critterbot [[cite:&sutton2011horde;&modayil2014multitimescale;&white2015developing]]. While we don't explore the full robotic setting, we often use simulations which approximate this setting to ask fundamental questions about the learning process.
** Perception and Partial Observability in Reinforcement Learning (Background Part 2?)

From here on we will primarily consider the setting where the agent observes its world through limited senses. This setting is often known as the partially observable setting in reinforcement learning. In this thesis, we focus on partial observability in terms of the agent-centric observations, emphasizing the discussion held in 


- State, credit assignment/search through the functional space
- Environment State, Agent State, Representations
- Working towards a better definition of what we want from state -> Better path of discovery for new algorithms which learn state.
- Focus is on understanding prior methods through empirical investigations, developing these methods using modern tools, and making recommendations for the future.
*** Problem Formulation


We consider a partially observable setting, where the observations are a function of an unknown, unobserved underlying state.
The dynamics are specified by transition probabilities \(\Pfcn = \States \times \Actions \times \States \rightarrow [0,\infty)\) with state space \(\States\) and action-space \(\Actions\). On each time step the agent receives an observation vector \(\obs_t \in \Observations \subset \Reals^\obssize\), as a function \(\obs_t = \obs(\state_t)\) of the underlying state \(\state_t \in \States\). The agent only observes \(\obs_t\), not \(\state_t\), and then takes an action \(\action_t\), producing a sequence of observations and actions: \(\obs_{0}, a_{0}, \obs_{1}, a_1, \ldots\).

The goal for the agent under partial observability is to identify a state representation \(\svec_t \in \RR^\numgvfs\) which is a sufficient statistic (summary) of past interaction, for targets \(y_t\). More precisely, such a /sufficient state/ ensures that \(y_t\) given this state is independent of history \(\hvec_t = \obs_0, a_{0}, \obs_1, a_1, \ldots, \obs_{t-1}, a_{t-1}, \obs_{t}\),
{{{c}}}
{{{c}}}
\begin{equation}
  p(y_{t} | \svec_t) = p(y_{t} | \svec_t, \hvec_t)
\end{equation}
{{{c}}}
{{{c}}}
or so that statistics about the target are independent of history, such as \(\mathbb{E}[Y_{t} | \svec_t] = \mathbb{E}[Y_{t} | \svec_t, \hvec_t]\).
Such a state summarizes the history, removing the need to store the entire (potentially infinite) history.

**** Sufficient State


# The formulation defined above uses a less stringent definition of sufficient state than used in previous work citep:&littman2002predictive;&subramanian2020approximate. We presume that the agent has a limited set of targets of interest, and needs to find a sufficient summary of the agent's history for just those targets. For example, a potential set of targets is the observation vector on the next time step.

# Sufficient state in subjective vs objective approaches.



# Previous approaches to discovery in predictive representations have focused on finding a set of predictions that would enable the agent to answer all predictive questions accurately. This objective is trying to find a sufficient statistic of the history for all predictions, and has been discussed in various forms citep:&subramanian2020approximate. This is the approach typically taken in PSRs and a usual criteria when approaching a POMDP problem. This criteria falls naturally from the POMDP specification, where the assumption is there is a true underlying latent state which the agent can determine from enough interactions with the system.  We conjecture that finding such a state is not feasible in large complex problems, and searching for such a state would be a poor use of a finite set of computational resources. Instead, the agent should focus on finding a set of questions which is useful for the agents overarching goals---for example, maximizing the return in the control problem.

*** TODO [#B] Discovery, search, and credit assignment

Many parts of the literature use three terms indistinguishabaly when discussing a notion of state-construction or state-discovery.
- Differences in how we want to search (g&t, heuristic search, gradient descent)
- Differences through the space we want to search through: Predictions vs Parameter space
- Differences through how the search is finalized: Learn answerable predictions vs freeze random features.

*** DONE [#A] Using RNNs in Reinforcement Learning (discovery through gradient descent)
CLOSED: [2022-12-19 Mon 12:41]
*** TODO [#B] Other methods for discovering state in RL
*** TODO Learning Long-temporal dependencies from Online Data
# Learning long-temporal dependencies is the primary concern of both RL and SL applications of recurrent networks. While great work has been done to coalesce around a few potential architectures and algorithms for SL settings, these are often found lacking in the online-incremental RL context citep:&sodhani2019toward;&rafiee2020eye;&schlegel2020general discussed in section \ref{sec:open_problems}. Not only do agents need to learn from the currently stored data (i.e. in an experience replay buffer), they must also continually incorporate the newest information into their decisions (i.e. update online and incrementally). The importance of learning state from an online stream of data has been heavily emphasized in the past through predictive representations of state citep:&littman2002, temporal-difference networks citep:&sutton2005 and GVF networks citep:&schlegel2020general, and in modeling trace patterning systems citep:&rafiee2020eye. From a supervised learning perspective, several problems like saturating capacity and catastrophic forgetting are cited as the most pressing for any parametric continual learning system citep:&sodhani2019toward. Below we suggest a few alternative directions needing further exploration in the RL context.

# The current standard in training recurrent architectures in RL is truncated BPTT. This algorithm trades off the ability to learn long-temporal dependencies with computation and memory complexity. Currently, the system designer must set the length of temporal sequences the agent needs to model (as would be needed for truncated BPTT to be effective citep:&mozer1995focused;&ke2018sparse;&tallec2018;&rafiee2020eye). Setting this length is a difficult task, as it interacts with the underlying environment and the agent's exploration strategy (see section \ref{sec:open_problems} for more details). As the truncation parameter increases it is known that the gradient estimates become wildly variant citep:&pascanu2013difficulty;&sodhani2019toward, which can make learning slow.

# An alternative to (truncated) BPTT is real time recurrent learning (RTRL) citep:&williams1989learning. Unfortunately RTRL is known to suffer high computational costs for large networks. Several approximations have been developed to alleviate these costs citep:&tallec2018;&mujika2018approximating, but these algorithms often struggle from high variance updates making learning slow. The approximation to the RTRL influence matrix proposed by cite:&menick2020practical shows significant promise in sparse recurrent networks, even outperforming BPTT when trained fully online. cite:&ke2018sparse propose a sparse attentive backtracking credit assignment algorithm inspired by hippocampal replay, showing evidence the algorithm has beneficial properties of both BPTT and truncated BPTT. The focused architecture was often able to compete with the fully connected architecture on length of learned temporal sequence and prediction error on several benchmark tasks. Another line of search/credit assignment algorithms is generate and test citep:&kudenko1998feature;&mahmood2013representation;&dohare2021continual;&samani2021learning. These search algorithms aren't as tied to their initialization as other systems as they intermittently inject randomness into their search to jump out of local minima. Many of these approaches combine both gradient descent and generate and test to gain the benefits of both. While a full generate and test solution is possible, finding the right heuristics to generate useful state objects quickly could be problem dependent.

# Learning long-temporal dependencies through regularizing objectives on the state has shown promise in alleviating the need for unrolling the network over long-temporal sequences. cite:&schlegel2020general use GVFs to make the hidden state of a simple RNN predictions about the observations showing potential in lightening the need for BPTT. This approach is sensitive the GVF parameters to use as targets on the state of the network. Predictive state recurrent neural networks citep:&downey2017a combine the benefits of RNNs and predictive representations of state citep:&littman2002 in a single architecture. They show improvement in several settings, but don't explore the model when starved for temporal information in the update. Another approach is through stimulating traces, as shown by cite:&rafiee2020eye, where traces of observations are used to bridge the gap between different stimuli. Instead of traces, an objective which learns the expected trace citep:&van2021expected of the trajectory could provide similar benefits as a predictive objective. One can even change the requirements on the architecture in terms of final objectives. cite:&mozer1991induction propose to predict only the contour or general trends of a temporal sequence, reducing the resolution considerably. Value functions are another object which takes an infinite sequence and reduces resolution to make the target easier to predict citep:&sutton1995td;&sutton2011horde;&modayil2014multitimescale;&vanhasselt2015learning.

# It is also possible to reduce or avoid the need for BPTT for modeling long-temporal sequences by adjusting the internal mechanisms of the recurrent architecture. Echo-state Networks citep:&jaeger2002adaptive are one possible direction. Related to the generate and test idea, echo-state networks rely on a random fixed ``reservoir'' network, where predictions are made by only adjusting the outgoing weights. Because the recurrent architecture is fixed, no gradients flow through the recurrent connections meaning no BPTT is needed to estimate the gradients. Unfortunately, these networks are dependent on their initializations making them hard to deploy in practice. cite:&mozer1995focused propose a focused architecture design, where recurrent connections are made more sparsely (even just singular connections). This significantly reduces the computational complexity of RTRL and allows for a focused version of BPTT.

# Transformers citep:&vaswani2017attention are a widely used alternative to recurrent architectures in natural language processing. Transformers have also shown some success in reinforcement learning but either require the full sequence of observations at inference and learning time citep:&mishra2018simple;&parisotto2020stabilizing or turn the RL problem into a supervised problem using the full return as the training signal citep:&chen2021decision. Because of these compromises, it is still unclear if transformers are a viable solution to the state construction problem in continual reinforcement learning.

*** Graveyard                                                    :noexport:
**** TODO [#B] Long Temporal Abstractions vs embodied state

- What is the difference between temporal abstractions and an embodied state?
- What kinds of abstractions do we want in an embodiement/agent-state?
- Where does the body come into the equation, and do predictions fit within an embodied perspective of an agent's state?
  Yes, state must be constructed in a hierarchy. Predictive state in the form of GVFs should be deeply linked to the agent's body not the agent's world. Mixing notions of temporal abstractions for the world and temporal abstractions for the body is a mistake. They happen at difference parts of the hierarchy using difference symbols of question asking possibly using difference mechanisms for answering predictive questions.
- Where does history play into this approach?
  History is another temporal abstraction imho and an extremely important part of the state.
  



** IN-PROGRESS [2/15] Old Background

In this chapter, I will provide relevant background details pertaining to this thesis. This includes material related to reinforcement learning (RL) (Section ref:sec:bg:RL), temporal abstractions (Section [[ref:sec:bg:temporal-abstractions]]), function approximation and artificial neural networks (ANNs) (Section ref:sec:bg:func-approx), and tensors and low-rank decompositions ref:sec:bg:tensor. Further background more specific to individual chapters will be introduced in the relevant sections.

In this thesis, we take the perspective that an agent is situated inside its environment and observes its world from an egocentric perspective. While this is not a particularly novel interpretation of the machine intelligence problem, it is worthwhile to clarify the terms we will use throughout intuitively before moving onto formal descriptions.

The *environment* is large, with many hidden states to determine its properties. The *agent* is the system deployed in the environment. The environment can be the whole world the agent may experience, or a subsection of this world that is partitioned for the agent by a *practitioner*. The *practitioner* is you and me. Specifically, a practitioner is one who is creating the *agent* for an environment. The agent observes the environment from an egocentric perspective, meaning the agent doesn't observe the full set of hidden properties of the environment and must maintain its current beliefs of the world internally through what is sometimes referred to as *agent-state* (we use state and agent-state interchangeably, making sure to emphasize when discussing the environment state). In MI research, the goal is to construct agents which can behave in an environment, usually to accomplish a specific goal set by the practitioner. We seek solutions, algorithms, and systems which can do solve goals with as little intervention by the practitioner as possible. This form of machine intelligence has been discussed before, even using it to form a centralized definition of intelligence [[cite:&silver2021reward]].

While the practitioner can set a large set of goals for the agent, we are particularly interested in an agent's ability to predict and control its datastream, which the construction of state is an important aspect of. As previously stated, this thesis focuses on learning state using recurrent networks and investigate various assumptions made by the supervised learning community and how they apply to the RL problem.

*An Example*
One common example we continuously tread in this thesis is that of a small robot on wheels behaving in a room. This is a common example throughout AI and is also known as the vacuum robot {{{citeplease ModernAI}}}, or lovingly known as the critterbot [[cite:&sutton2011horde;&modayil2014multitimescale;&white2015developing]]. While we don't explore the full robotic setting, we often use simulations which approximate this setting to ask fundamental questions about the learning process.


*** DONE Reinforcement Learning
CLOSED: [2023-01-23 Mon 13:59]
:PROPERTIES:
:CUSTOM_ID: sec:bg:RL
:END:

The primary problem setting considered in this thesis is reinforcement learning. In short, a reinforcement learning (RL) agent seeks to maximize a reward signal by acting in the world. In this thesis, we are concerned with two learning problems in reinforcement learning. Specifically, we focus on the prediction and control problem, but each share the same general framework. The agent-environment interaction consists of a stream of data (from the agent's senses), coming in at a consistent rate into the agent's central control systems. In most reinforcement learning, the agent-environment boundary is placed inside the agent's nervous system where parts of the agent's body which are defined through evolution are external to the learning process, and those that are learned and modified through an agent's lifetime are a part of the learning process. This enables RL researchers to focus on the core problem of learning a policy to maximize reward.

More mathematically grounded, the agent observes the sequence \(\obs_1, \action_1, \reward_2, \obs_2, \ldots, \obs_t, \action_t, \reward_{t+1}, \obs_{t+1}\) in its lifetime. The observation \(\obs_t\) is the agent's window into the world through various sensing parts of its body. These can include a camera for vision, microphone for audio, lidar to measure distance from other objects, and many other analog-to-digital conversion technologies. The agent then selects an action \(\action_t\) which is passed to the agent's actuators or sub-level control system. By performing this action, the agent receives a reward \(\reward_{t+1}\) and another observation \(\obs_{t+1}\).

The agent-environment interaction can be formalized as a partially observable Markov decision processes (POMDP). The underlying dynamics are defined by a tuple \((\EnvStates, \Actions, \Pmat, f_\obs, \Rewards)\). Given a state \(\envstate \in \EnvStates\) and \(\action \in \Actions\) the environment transitions to a new state \(\envstate^\prime \in \EnvStates\) according to the state transition probability matrix \(\Pmat \defeq \EnvStates \times \Actions \times \EnvStates \rightarrow [0,\infty)\) with a reward given by \(\Rewards \defeq \EnvStates \times \Actions \rightarrow \Reals\). The observations can then be defined as a lossy function over the state \(\obs_t \defeq f_\obs(\envstate_t) \in \Reals^\obssize\), and the reward is \(\reward_t \defeq f_\reward(\envstate_0, \envstate_1, \ldots, \envstate_t) \in \Reals\). This thesis concerns itself primarily with the discrete action setting, where the set of actions is a finite discrete set of values \(\action \in \Actions \defeq [A_1, A_2, \ldots, A_n]\).

The agent has several canonical internal components. A *policy* is a mapping from states to actions \(\pi: \EnvStates \rightarrow \Actions\) and defines a way of interacting with the environment. Most often a policy defines a probability distribution over the space of Actions conditioned on the agent's state \(\pi(a|s)\defeq\text{The probability of selecting action $a$ in state $s$}\). A *value function* is a prediction of the future cumulated (discounted) reward the agent will obtain by following a policy. Specifically,
{{{c}}}
\[
V(\State) = \Expected_\pi[ G_t | s_t = \State, a \sim \pi(\cdot| S)]
\]
{{{c}}}
{{{c}}}
with a state-action value function defined similarly
\[
q(\State, \Action) = \Expected_\pi [ G_t | s_t = \State, a_t = \Action].
\]
This thesis uses both state value functions and state-action value functions to do prediction and control. In the following two sections we will go into the specifics of the prediction problem and the control problem.


**** Graveyard                                                  :noexport:


In this paper we perform experiments in two settings: prediction and control. For prediction, general value functions (GVFs) define the targets [[cite:&sutton2011horde;&white2015developing]]. A GVF is a tuple containing a cumulant \(c_{t+1} = f_c(o_t, a_t, o_{t+1}, r_{t+1}) \in \Reals\), a continuation function \(\gamma_{t+1} = f_\gamma(o_t, a_t, o_{t+1}) \in [0, 1]\), and a history \(\hvec_t = [\action_0, \obs_1, \action_1, \obs_2, \action_2, \ldots, \obs_t]\) conditioned policy \(\pi(\action_t|\hvec_t) \in [0,\infty)\). The goal of the agent is to learn a value function which estimates the expected cumulative return under \(\pi\),
{{{c}}}
\[
\Expected_\pi\left[ G_t^c | H_t = \hvec_t \right] \quad\quad\text{ where } G_t^c \defeq c_{t+1} + \gamma_{t+1} G_{t+1}^c
.
\]
{{{c}}}


# To estimate the value function we use off-policy semi-gradient TD(0) citep:&sutton1988learning;&tesauro1995temporal (see ref:sec:bg:td0 for details). For the control setting we learn a policy which maximizes the discounted sum of rewards or return \(G_t \defeq \sum_{i=0}^\infty \gamma^{i} \reward_{i+t+1}\). In this paper, we use Q-learning citep:&watkins1992q to construct an action-value function and take actions according to an epsilon-greedy strategy.

*** IN-PROGRESS On and Off-policy Prediction in Reinforcement Learning
:PROPERTIES:
:CUSTOM_ID: sec:bg:offpolicy
:END:


The prediction problem in RL is that of learning value functions effectively and efficiently. This process can be used to improve an agent's policy through value iteration {{{CITEPLEASE}}}, or to learn temporal abstractions of the sensorimotor stream through options or general value functions (see Section ref:sec:bg:temporal-abstractions for more details). A value function can be learned either on-policy or off-policy through temporal difference learning. In this section, we will be introducing the on and off-policy prediction problem as used throughout this text. To see a more complete treatment with respect to the deep reinforcement learning setting see Chapter ref:chap:resampling.

As introduced above, a *value function* is a prediction of the future cumulative (discounted) reward received by following a policy \(\tpolicy\),
\[
V_\tpolicy(\State) = \Expected_\pi[ G_t | s_t = \State, a \sim \tpolicy(\cdot| S)]
\]
where \(G_t = \sum_{i=1}^{\infty} \gamma^{i-1} r_{t+i} \) is the return. The operator \(\mathbb{E}_{\tpolicy}\) indicates an expectation with actions selected according to policy $\tpolicy$. GVFs encompass standard value functions, where the cumulant is a reward. Otherwise, GVFs enable predictions about discounted sums of others signals into the future, when following a target policy \(\tpolicy\). These values are typically estimated using parametric function approximation, with weights \(\theta \in \RR^d\) defining approximate values \(\Value_\theta(\state)\). 

The simplest algorithm to learn the value function is through Monte-Carlo sampling. The brief of the algorithm is to get samples of the return starting in state $\State$ following policy $\tpolicy$, which are then averaged to receive the return. This algorithm only requires the environment to be episodic (i.e. clear terminations) and converges to the true value function as the number of samples grows.

Another way to learn the value function is by taking advantage of the Bellman equation through dynamic programming.

Both of the above algorithms enforce restrictions on the types of problems addressable. Temporal-difference learning combines advantages of both the above algorithms, alleviating some of constraints imposed.

The off-policy prediction problem is equally concerned with learning value functions of policy $\tpolicy$, but must use data generated from a separate behavior policy $\bpolicy$.

# We consider the problem of learning General Value Functions (GVFs) \citep{sutton2011horde}. The agent interacts in an environment defined by a set of states 
# $\States$, a set of actions $\Actions$ and Markov transition dynamics, with probability $\Pfcn(\state'|\state,\action)$ of transitions to state $\state'$ when taking action $\action$ in state $\state$. A GVF is defined for policy $\pi: \States \!\times \!\Actions \!\rightarrow\! [0,1]$, cumulant $\cumul: \States\! \times \!\Actions \!\times\! \States\! \rightarrow\! \RR$ and continuation function $\gamma: \States \!\times\! \Actions \!\times \!\States \rightarrow [0,1]$, with $\cumulr_{t+1} \defeq  \cumul(\stater_t, \actionr_t, \stater_{t+1})$ and  $\gamma_{t+1} \defeq  \gamma(\stater_t, \actionr_t, \stater_{t+1})$ for a (random) transition $(\stater_t, \actionr_t, \stater_{t+1})$. The value for a state $s \in \States$ is
# {{{c}}}
# \begin{align*}
#   \Value(\state) \defeq \mathbb{E}_\pi\left[ G_t | \stater_t = \state \right] &
#   % MARTHAC: lets noti nclude this ugly equation, since we dont need it
# %= \mathbb{E}_\pi\Big[ \sum_{i=1}^\infty \Big( \prod_{j=0}^{i-1} \gamma_{t+j} \Big)  \cumulr_{t+i}  | \stater_t = \state \Big]
# &\text{where }  G_t \defeq \cumulr_{t+1} + \gamma_{t+1} \cumulr_{t+2} + \gamma_{t+1} \gamma_{t+2} \cumulr_{t+3} + \ldots
# .
# \end{align*}
# {{{c}}}


In off-policy learning, transitions are sampled according to behavior policy, rather than the target policy. 
To get an unbiased sample of an update to the weights, the action probabilities need to be adjusted. Consider on-policy temporal difference (TD) learning, with update \(\alpha_t\delta_t\nabla_\theta \Value_{\theta}(s)\) for a given \(S_t = s\), for learning rate \(\alpha_t \in \RR^+$ and TD-error $\delta_t \defeq C_{t+1} + \gamma_{t+1}\Value_{\theta}(S_{t+1}) -  \Value_{\theta}(s)\). If actions are instead sampled according to a behavior policy \(\bpolicy: \States \times \Actions \rightarrow [0,1]\), then we can use importance sampling (IS) to modify the update, giving the off-policy TD update $\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\theta}(s)$ for IS ratio $\rho_t \defeq \frac{\tpolicy(\actionr_t | \stater_t)}{\bpolicy(\actionr_t | \stater_t)}$.  Given state $\stater_t = \state$, if $\bpolicy(a | s) > 0$ when $\tpolicy(a | s) > 0$, then the expected value of these two updates are equal. To see why, notice that
{{{c}}}
\begin{equation*}
  \mathbb{E}_\mu\left[\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\theta}(s) |S_t = s\right]
  =  \alpha_t\nabla_\theta \Value_{\theta}(s)\mathbb{E}_\mu\left[\rho_t\delta_t |S_t = s\right]
\end{equation*}
which equals $\mathbb{E}_\pi\left[\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\theta}(s) |S_t = s\right]$ because
{{{c}}}
\begin{align*}
\mathbb{E}_\mu\left[\rho_t\delta_t |\stater_t = \state\right] 
% &= \sum_{\action \in \Actions} \mu(\action | \state) \mathbb{E}\left[\rho_t\delta_t |\stater_t = \state, \actionr_t = \action \right]\\ 
&= \sum_{\action \in \Actions} \mu(\action | \state) \frac{\tpolicy(\action | \state)}{\bpolicy(\action | \state)} \mathbb{E}\left[\delta_t |\stater_t = \state, \actionr_t = \action \right]
% &= \sum_{\action \in \Actions} \tpolicy(\action | \state) \mathbb{E}\left[\delta_t |\stater_t = \state, \actionr_t = \action \right] \\
= \ \mathbb{E}_\pi\left[\delta_t |\stater_t = \state\right].
\end{align*}

Though unbiased, IS can be high-variance. A lower variance alternative is Weighted IS (WIS). For a batch consisting of transitions $\{(s_i, a_i, s_{i+1}, c_{i+1}, \rho_i)\}_{i=1}^n$, batch WIS uses a normalized estimate for the update.
For example, an offline batch WIS TD algorithm, denoted WIS-Optimal below, would use update \(\alpha_t \frac{\rho_t}{\sum_{i=1}^n \rho_i} \delta_t\nabla_\theta \Value_{\theta}(s)\).

Obtaining an efficient WIS update is not straightforward, however, when learning online and has resulted in algorithms in the SGD setting (i.e. $n=1$) specialized to tabular \citep{precup2001offpolicy} and linear functions cite:&mahmood2014weighted;&mahmood2015off.
We nonetheless use WIS as a baseline in the experiments and theory.

*************** TODO Finish Off-policy background section
*************** END

Per decision corrections

Lifetime corrections

Semi-gradient Temporal-difference learning

**** Deadly triad

Off-policy, function approximation, bootstrapping 

*** DONE Control
CLOSED: [2023-01-25 Wed 11:12]

The bread and butter problem for reinforcement learning research is the control problem. The control problem is the process of searching (or learning) a policy which the agent can use to decide actions. There are many possible approaches for control in reinforcement learning, from value-based control (through q-learning) to direct policy optimization through policy gradient and actor critic methods. In this thesis, we are primarily concentrated on value-based control as a means to study the perception of reinforcement learning agents (see ref:sec:bg:perception for more details).

We again start with a value function, this time a state-action value function, as defined above
\[
q(\State, \Action) = \Expected_\optpolicy [ G_t | s_t = \State, a_t = \Action].
\]
where \(\optpolicy\) is the optimal policy. The goal of the agent is to search through the space of policies to maximize the total return the agent will receive from any state, or in other words to find the optimal policy \(\optpolicy\). In this thesis, our control experiments are restricted to Q-learning [[cite:&watkins1992qlearning;&mnih2015humanlevel]], an off-policy technique which learns the optimal policy. Q-learning, in its simplest form, is defined by the following set of updates
\begin{align*}
\delta_{t+1} &= Q_\theta (S_t, A_t) - (R_{t+1} + \gamma \max_a (Q_\theta(S_{t+1}, A_{t+1}))) \\
\Delta \theta &= \delta_{t+1} \nabla_\theta Q_\theta(S_t, A_t)
\end{align*}
See Sections ref:sec:bg:func-approx and ref:sec:bg:perception for details on how to apply this method when using deep learning function approximation and recurrent neural networks respectively.


*** TODO Function approximation in Reinforcement Learning
:PROPERTIES:
:CUSTOM_ID: sec:bg:func-approx
:END:
*** IN-PROGRESS Temporal Abstractions
:PROPERTIES:
:CUSTOM_ID: sec:bg:temporal-abstractions
:END:

A key concept used in this thesis is to leverage *temporal abstractions* to improve behavior in an agent, but before we can go about using these abstractions we must discuss different types of temporal abstractions and the mechanisms needed to use them in a general continual reinforcement learning agent.

**** General Value Functions

The first, and most general, of the temporal abstractions found in the RL landscape are general value functions (GVFs). A general value function is a value function whose cumulant (replacing the reward), discount, and policy are functions of the incoming observation or any signal the agent has (including other predictions, see Chapter ref:chap:composite). The core construction of the GVF is the same as a value function, now taking into account the generality. We define the GVF question more formally below.

# A GVF question is a tuple $(\tpolicy, \cumulant, \gamma)$ composed of a policy $\tpolicy: \Hist \times \Actions \rightarrow [0, \infty)$, cumulant
# $\cumulant: \Hist \times \Actions \times \Hist \rightarrow \RR$ and continuation function\footnote{The original GVF definition assumed the continuation was only a function of $H_{t+1}$. This was later extended to transition-based continuation \citep{white2017unifying}, to better encompass episodic problems. Namely, it allows for different continuations based on the transition, such as if there is a sudden change from $\hvec_t$ to $\hvec_{t+1}$. We use this more general definition for this reason, and because the cumulant itself is already defined on the three tuple $(\hvec_t, a_t, \hvec_{t+1})$.} $\gamma: \Hist \times \Actions \times \Hist \rightarrow [0,1]$, also called the discount. On time step t, the agent is in $H_t$, takes actions $A_t$, transitions to $H_{t+1}$ and observes\footnote{Throughout this document, unbolded uppercase variables are random variables; lowercase variables are instances of that random variable; and bolded variables are vectors. When indexing into a vector on time step $t$, such as $\hvec_t$, we double subscript as $\hvec_{t,j}$ for the $j$th component of $\hvec_t$.} cumulant $C_{t+1}$ and continuation $\gamma_{t+1}$. The answer to a GVF question is defined as the value function, $V: \Hist \rightarrow \RR$, which gives the expected, cumulative discounted cumulant  from any history $\hvec_t \in \Hist$. The value function which can be defined recursively with a Bellman equation as
# {{{c}}}
# \begin{align}
#   V(\hvec_t) &\defeq \expect*{ C_{t+1} + \gamma_{t+1} V(H_{t+1}) | H_t = \hvec_t, A_{t} \sim \pi(\cdot | \hvec_t)} \label{eq_bewh}\\
#   &= \sum_{\action_t \in \Actions} \pi(\action_t | \hvec_t) \sum_{\hvec_{t+1} \in \Hists} \Pr(\hvec_{t+1} | \hvec_t, \action_t) \left[\cumulant(\hvec_t, a_t, \hvec_{t+1}) + \gamma(\hvec_t,a_t,\hvec_{t+1}) V(\hvec_{t+1}) \right] \nonumber
#  .
# \end{align}
# {{{c}}}
# The sums can be replaced with integrals if $\Actions$ or $\Observations$ are continuous sets. We assume that $\Hist$ is a finite set, for simplicity; the definitions and theory, however, can be extended to infinite and uncountable sets.


- (General) Value Functions
- Options
- TD and Option Models
- Anticipation
- Predictive coding? -> Spatial abstraction

- Value Functions, TD Models, Option Models, and GVFs oh my!

**** Options
**** 

*** IN-PROGRESS Perception and Partial Observability in Reinforcement Learning
:PROPERTIES:
:CUSTOM_ID: sec:bg:perception
:END:


In the previous section, we discussed the fully observable reinforcement learning problem. While this is used in some chapters in the following thesis, the main focus here is the partial observability setting. The 

We consider a partially observable setting, where the observations are a function of an unknown, unobserved underlying state.
The dynamics are specified by transition probabilities \(\Pfcn = \States \times \Actions \times \States \rightarrow [0,\infty)\) with state space \(\States\) and action-space \(\Actions\). On each time step the agent receives an observation vector \(\obs_t \in \Observations \subset \Reals^\obssize\), as a function \(\obs_t = \obs(\state_t)\) of the underlying state \(\state_t \in \States\). The agent only observes \(\obs_t\), not \(\state_t\), and then takes an action \(\action_t\), producing a sequence of observations and actions: \(\obs_{0}, a_{0}, \obs_{1}, a_1, \ldots\).

The goal for the agent under partial observability is to identify a state representation \(\svec_t \in \RR^\numgvfs\) which is a sufficient statistic (summary) of past interaction, for targets \(y_t\). More precisely, such a /sufficient state/ ensures that \(y_t\) given this state is independent of history \(\hvec_t = \obs_0, a_{0}, \obs_1, a_1, \ldots, \obs_{t-1}, a_{t-1}, \obs_{t}\),
{{{c}}}
{{{c}}}
\begin{equation}
  p(y_{t} | \svec_t) = p(y_{t} | \svec_t, \hvec_t)
\end{equation}
{{{c}}}
{{{c}}}
or so that statistics about the target are independent of history, such as \(\mathbb{E}[Y_{t} | \svec_t] = \mathbb{E}[Y_{t} | \svec_t, \hvec_t]\).
Such a state summarizes the history, removing the need to store the entire (potentially infinite) history.

*** TODO Learning Long-temporal Dependencies

Learning long-temporal dependencies is the primary concern of both RL and SL applications of recurrent networks. While great work has been done to coalesce around a few potential architectures and algorithms for SL settings, these are often found lacking in the online-incremental RL context cite:&sodhani2020training;&rafiee2022eyeblinks;&schlegel2021general. 
# discussed in section \ref{sec:open_problems}.
Not only do agents need to learn from the currently stored data (i.e. in an experience replay buffer), they must also continually incorporate the newest information into their decisions (i.e. update online and incrementally). The importance of learning state from an online stream of data has been heavily emphasized in the past through predictive representations of state cite:&littman2002predictive, temporal-difference networks [[cite:&sutton2005temporaldifference]] and GVF networks [[cite:&schlegel2021general]], and in modeling trace patterning systems [[cite:&rafiee2022eyeblinks]]. From a supervised learning perspective, several problems like saturating capacity and catastrophic forgetting are cited as the most pressing for any parametric continual learning system [[cite:&sodhani2020training]]. Below we suggest a few alternative directions needing further exploration in the RL context.

The current standard in training recurrent architectures in RL is truncated BPTT. This algorithm trades off the ability to learn long-temporal dependencies with computation and memory complexity. Currently, the system designer must set the length of temporal sequences the agent needs to model (as would be needed for truncated BPTT to be effective [[cite:&mozer1995focused;&ke2018sparse;&tallec2018unbiased;&rafiee2022eyeblinks]]). Setting this length is a difficult task, as it interacts with the underlying environment and the agent's exploration strategy
# (see section \ref{sec:open_problems} for more details).
As the truncation parameter increases it is known that the gradient estimates become wildly variant [[cite:&pascanu2013difficulty;&sodhani2020training]], which can make learning slow.

An alternative to (truncated) BPTT is real time recurrent learning (RTRL) cite:&williams1989learning. Unfortunately RTRL is known to suffer high computational costs for large networks. Several approximations have been developed to alleviate these costs [[cite:&tallec2018unbiased;&mujika2018approximating]], but these algorithms often struggle from high variance updates making learning slow. The approximation to the RTRL influence matrix proposed by cite:&menick2020practical shows significant promise in sparse recurrent networks, even outperforming BPTT when trained fully online. citeauthor:&ke2018sparse (citeyear:&ke2018sparse) propose a sparse attentive backtracking credit assignment algorithm inspired by hippocampal replay, showing evidence the algorithm has beneficial properties of both BPTT and truncated BPTT. The focused architecture was often able to compete with the fully connected architecture on length of learned temporal sequence and prediction error on several benchmark tasks. Another line of search/credit assignment algorithms is generate and test [[cite:&kudenko1998feature;&mahmood2013representation;&dohare2022continual;&samani2021learning]]. These search algorithms aren't as tied to their initialization as other systems as they intermittently inject randomness into their search to jump out of local minima. Many of these approaches combine both gradient descent and generate and test to gain the benefits of both. While a full generate and test solution is possible, finding the right heuristics to generate useful state objects quickly could be problem dependent.

Learning long-temporal dependencies through regularizing objectives on the state has shown promise in alleviating the need for unrolling the network over long-temporal sequences. citeauthor:&schlegel2021general (citeyear:&schlegel2021general) use GVFs to make the hidden state of a simple RNN predictions about the observations showing potential in lightening the need for BPTT. This approach is sensitive the GVF parameters to use as targets on the state of the network. Predictive state recurrent neural networks [[cite:&downey2017predictive]] combine the benefits of RNNs and predictive representations of state [[cite:&littman2002predictive]] in a single architecture. They show improvement in several settings, but don't explore the model when starved for temporal information in the update. Another approach is through stimulating traces, as shown by [[cite:&rafiee2022eyeblinks]], where traces of observations are used to bridge the gap between different stimuli. Instead of traces, an objective which learns the expected trace [[cite:&hasselt2021expected]] of the trajectory could provide similar benefits as a predictive objective. One can even change the requirements on the architecture in terms of final objectives. [[cite:&mozer1991induction]] propose to predict only the contour or general trends of a temporal sequence, reducing the resolution considerably. Value functions are another object which takes an infinite sequence and reduces resolution to make the target easier to predict [[cite:&sutton1995td;&sutton2011horde;&modayil2014multitimescale;&vanhasselt2015learning]].

It is also possible to reduce or avoid the need for BPTT for modeling long-temporal sequences by adjusting the internal mechanisms of the recurrent architecture. Echo-state Networks [[cite:&jaeger2002adaptive]] are one possible direction. Related to the generate and test idea, echo-state networks rely on a random fixed "reservoir" network, where predictions are made by only adjusting the outgoing weights. Because the recurrent architecture is fixed, no gradients flow through the recurrent connections meaning no BPTT is needed to estimate the gradients. Unfortunately, these networks are dependent on their initializations making them hard to deploy in practice. [[citeauthor:&mozer1995focused]] ([[citeyear:&mozer1995focused]]) propose a focused architecture design, where recurrent connections are made more sparsely (even just singular connections). This significantly reduces the computational complexity of RTRL and allows for a focused version of BPTT.

Transformers [[cite:&vaswani2017attention]] are a widely used alternative to recurrent architectures in natural language processing. Transformers have also shown some success in reinforcement learning but either require the full sequence of observations at inference and learning time [[cite:&mishra2018simple;&parisotto2020stabilizing]] or turn the RL problem into a supervised problem using the full return as the training signal [[cite:&chen2021decision]]. Because of these compromises, it is still unclear if transformers are a viable solution to the state construction problem in continual reinforcement learning.

*** IN-PROGRESS Recurrent Neural Networks
:PROPERTIES:
:CUSTOM_ID: sec:bg:rnns
:END:

Recurrent neural networks (RNNs) have been established as an important tool for learning predictions of data with temporal dependencies. They have been primarily used in language and video prediction [[cite:&mikolov2010recurrent;&wang2016largercontext;&saon2017english;&wang2018eidetic;&oh2015actionconditional]], but have also been used in traditional time-series forecasting [[cite:&bianchi2017recurrent]] and RL [[cite:&onat1998recurrent;&bakker2002reinforcement;&wierstra2007solving;&hausknecht2015deep;&heess2015memorybased;&zhu2018improving;&igl2018deep]]. Many specialized architectures have been developed to improve learning with recurrence. These architectures are designed to better learn long temporal dependence and avoid saturation including, Long-short Term Memory units (LSTMs) [[cite:&hochreiter1997long]], Gated Recurrent Units (GRUs)
[[cite:&cho2014properties;&chung2014empirical]], Non-saturating Recurrent Units (NRUs) [[cite:&chandar2019nonsaturating]], and others. Most modern RNN architectures integrate information through additive operations. However, some work has also examined multiplicative updating
[[cite:&sutskever2011generating;&wu2016multiplicative]] which follows from what were known as Second-order RNNs [[cite:&goudreau1994firstorder]].

In this Section we will outline the methods used to estimate gradients in recurrent neural networks in this thesis (Section ref:, discuss three major architectures applied in this thesis, and finally outline how we use recurrent neural networks in the reinforcement learning problem.
**** TODO Estimating Gradients for Recurrent Neural Networks
:PROPERTIES:
:CUSTOM_ID: sec:bg:rnns:grads
:END:
**** TODO Various Recurrent Neural Network Architectures
:PROPERTIES:
:CUSTOM_ID: sec:bg:rnns:arch
:END:

There are several known problems with simple recurrent units (and to a lesser extent other recurrent cells). The first is known as the vanishing and exploding gradient problem [[cite:&pascanu2013difficulty]]. In this, as gradients are multiplied together (via the chain rule in BPTT) the gradient can either become very large or vanish into nothing. In either case, the learned networks often cannot perform well and a number of practical tricks are applied to stabilize learning [[cite:&bengio2013advances]]. The second problem is called saturation. This occurs when the weights \(\weights\) become large and the activations of the hidden units are at the extremes of the transfer function. While not problematic for learning stability, this can limit the capacity of the network and make tracking changes in the environment dynamics more difficult [[cite:&chandar2019nonsaturating]].

# We focus our experiments around the simple recurrent cells (RNNs) and GRUs.
Long-short term memory cells (LSTM) were developed to address the issues with modeling long-temporal dependencies.

Gated-recurrent units (GRU) are a modification from the LSTM cell which maintains performance in many settings, improves ease of use, and improves computational footprint.

*************** TODO [#B] Gated Recurrent Units, LSTMs
*************** END

**** TODO Recurrent Neural Networks for learning agent-state
:PROPERTIES:
:CUSTOM_ID: sec:bg:rnns:rl
:END:

For effective prediction and control, the agent requires a state representation \(\agentstate_t\) that is a sufficient statistic of the past: \(\Expected\left[ G^c_t | \agentstate_t \right] = \Expected\left[G^c_t | \agentstate_t, \history_t\right]\). When the agent learns such a state, it can build policies and value functions without the need to store any history. For example, for prediction, it can learn \(V(\agentstate_t) \approx \Expected\left[ G^c_t | \agentstate_t \right]\).


An RNN provides one such solution to learning \(\agentstate_t\) and associated state update function. The simplest RNN is one which learns the parameters \(\weights \in \Reals^\numparams\) recursively
{{{c}}}
\[
  \agentstate_t = \sigma(\weights \xvec_t + \bvec)
\]
{{{c}}}
where \(\xvec_t = [\obs_t, \agentstate_{t-1}]\) and \(\sigma\) is any non-linear transfer function (typically tanh). While concatenating information (or doing additive operations) has become standard in RNNs, another idea explored earlier in the literature and in more modern cells is using multiplicative operations
{{{c}}}
\[
  (\agentstate_t)_i = \sigma\left(\sum_{j=1}^M \sum_{k=1}^N\weights_{ijk} (\obs_t)_j (\agentstate_{t-1})_k + \bvec_i\right) \quad\quad \triangleright \text{ where } \weights \in \Reals^{|\agentstate| \times |\obs| \times |\agentstate| }.
\]
{{{c}}}
Using this type of operation was initially called second-order RNNs [[cite:&goudreau1994firstorder]], and was also explored in one of the first landmark successes of RNNs [[cite:&sutskever2011generating]] in a character-level language modeling task.


RNNs are typically trained through the use of back-propagation through time [[cite:&mozer1995focused]]. This algorithm effectively unrolls the network through the sequence and calculates the gradient as if it was one large network with shared weights.
This unrolling is often truncated at some number of steps \(\tau\). While this alleviates computational-cost concerns, the learning performance can be sensitive to the truncation parameter [[cite:&pascanu2013difficulty]]. When calculating the gradients through time for a specific sample, we define our loss as
{{{c}}}
\[
  \mathcal{L}_{t}(\weights) = \sum_{i}^{N} (v_i(\agentstate_t(\weights)) - y_{t, i})^2
\]
{{{c}}}
where \(N\) is the size of the batch, and \(y\) is the target defined by the specific algorithm. This effectively means we are calculating the loss for a single step and calculating the gradients from that step only.

*** IN-PROGRESS Tensors and Low-Rank Decompositions
:PROPERTIES:
:CUSTOM_ID: sec:bg:tensor
:END:

The simplest, albeit slightly inaccurate, way to describe and use a tensor is as a multi-dimensional array of numbers (either real or complex) which transform under coordinate changes in predictable ways. In this paper, we will be considering tensors as multi-dimensional arrays using Einstein summation notation. The ith, jth, kth component of an order-3 tensor will be denoted with lower indices \(\weightmat_{ijk} \in \Reals\) with associated dimension size denoted with corresponding uppercase letters as \(\weightmat \in \Reals^{I\times J\times K}\).

Like matrices, tensors have a number of decompositions which can prove useful. For example, every tensor can be factorized using canonical polyadic decomposition (CP decomposition), which decomposes an order-N tensor \(\weightmat \in \Reals^{I_1 \times I_2 \times \ldots \times I_N}\) into N matrices as follows
{{{c}}}
\begin{align*}
  \weightmat_{i_1, i_2, \ldots} &= \sum_{r=1}^R \lambda_r \weightmat^{(1)}_{i_1, r}  \weightmat^{(2)}_{i_2, r}  \ldots \weightmat^{(N)}_{i_N, r} \\
  &= \lambda_r \weightmat^{(1)}_{i_1, r}  \weightmat^{(2)}_{i_2, r} \ldots \weightmat^{(N)}_{i_N, r} \quad \triangleright \text{Explicit summation over $r\in\{1,\ldots,R\}$.}
\end{align*}
{{{c}}}
{{{c}}}
where \(\weightmat^{(j)} \in \Reals^{I_j \times R}\), and \(R\) is the rank of the tensor. This is a generalization of matrix rank decomposition, and exists for all tensors with finite dimensions.

Working with tensors takes a bit more care in deciding which fibers (generalization of row and column) the product should be over. One type of product is known as the n-mode product which is defined as follows 
{{{c}}}
\[
  (\weightmat \times_n \vvec)_{i_1, i_2, \ldots, i_{n-1}, j, i_{n+1}, \ldots i_{N}}
      = \weightmat_{i_1, i_2, \ldots, i_{n-1}, i_n, i_{n+1}, \ldots i_{N}} \vvec_{j, i_n}
\]
{{{c}}}
where \(\vvec \in \Reals^{J, I_n}\).

An important property, which will be used later in this thesis (see Chapter ref:chap:arnn is some simplifications we can make when considering n-mode products with their rank decomposition. In this thesis, we only consider order 3 tensors and all further calculations will use order 3 tensors for simplicity. Specifically, \(\weightmat \in \Reals^{IJK}\), with CP-decomposition \(\weightmat_{ijk} = \lambda_{r}a_{ir}b_{jr}c_{kr}\) and vector over a strand \(\vvec^{M} = \vvec^{(1, M)} \in \Reals^{1 \times M}\)).
{{{c}}}
\begin{align*}
  (\weightmat \times_2 \vvec^{J} \times_3 \vvec^{K})_{i,1,1}
  &= \sum_{k=1}^K \left(\sum_{j=1}^J\weightmat_{ijk} \vvec^{J}_{1j}\right) \vvec^{K}_{1k} \\
  &= \sum_{k=1}^K\sum_{j=1}^J \left(\sum_{r=1}^R\lambda_{r}a_{ir}b_{jr}c_{kr}\right) \vvec^{J}_{1j} \vvec^{K}_{1k}\\
  &= \sum_{r=1}^R \lambda_{r} a_{ir}
    \left(\sum_{j=1}^J b_{jr}\vvec^{J}_{1j}\right)
    \left(\sum_{k=1}^K c_{kr}\vvec^{K}_{1k}\right)\\
  &=  \sum_{r=1}^R \lambda_{r} a_{ir}\left(\vvec^{J} \Bmat \odot \vvec^{K} \Cmat\right)_{1r} \\
  \weightmat \times_2 \vvec^{J} \times_3 \vvec^{K}
  &= \boldsymbol{\lambda} \Amat \left(\vvec^{J}\Bmat \odot \vvec^{K}\Cmat\right)^\trans
     \quad \triangleright \boldsymbol{\lambda}_{i,i} = \lambda_i
\end{align*}

Similarly to CP decomposition, Tucker rank decomposition can be used to create a similar operation. Tucker rank decomposition decomposes an order-N tensor \(\weightmat \in \Reals^{I_1 \times I_2 \times \ldots \times I_N}\) into N matrices another order-N tensor \(G \in \Reals^{R_1 \times R_2 \times \ldots \times R_N}\) as follows

\begin{align*}
  \weightmat_{i_1, i_2, \ldots i_N} &= \sum_{r_1=1}^{R_1} \sum_{r_1=1}^{R_1} \ldots
  \sum_{r_1=1}^{R_1} g_{r_1 r_2 \ldots r_N} \weightmat^{(1)}_{i_1, r_1}
  \weightmat^{(2)}_{i_2, r_2}  \ldots \weightmat^{(N)}_{i_N, r_N}.
\end{align*}

With similar simplifications to CP decomposition,

\begin{align*}
  (\weightmat \times_2 \vvec^{J} \times_3 \vvec^{K})_{i,1,1}
  &= \sum_{k=1}^K \left(\sum_{j=1}^J\weightmat_{ijk} \vvec^{J}_{1j}\right) \vvec^{K}_{1k} \\
  &= \sum_{k=1}^K\sum_{j=1}^J \left(\sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_{ip} b_{jq} c_{kr}\right) \vvec^{J}_{1j} \vvec^{K}_{1k}\\
  &= \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_{ip}
    \left(\sum_{j=1}^J b_{jq}\vvec^{J}_{1j}\right)
    \left(\sum_{k=1}^K c_{kr}\vvec^{K}_{1k}\right)\\
  &= \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_{ip} \left(\vvec^{J}  \Bmat\right)_{1q} \left(\vvec^{K} \Cmat\right)_{1r} \\
  \weightmat \times_2 \vvec^{J} \times_3 \vvec^{K}
  &= G \times_1 \Amat^\trans \times_2 \left(\vvec^{J}\Bmat\right)^\trans \times_3 \left(\vvec^{K}\Cmat\right)^\trans \\
  &= \Amat \left[\left(G ^\trans \times_2 \left(\vvec^{J}\Bmat\right)^\trans\right) \left(\vvec^{K}\Cmat\right)^\trans \right].
\end{align*}

One interesting property of this operation is now each of the dimensions can have a separately tuned rank, giving the system designer more discretion on where to focus representational resources.

Using a lower rank approximation of a multiplicative operation has been derived before several times. A multiplicative update was used to make action-conditional video predictions in Atari [[cite:&oh2015actionconditional]].  This operation also appears in a lower-rank approximation defined by Predictive State RNN hidden state update [[cite:&downey2017predictive]], albeit never performed as well as the full rank version. We find similarly that both factorizations perform below the full tensor version (i.e. the multiplicative). We don't report results for the Tucker rank decomposition as it performed similarly to the CP decomposition. 

*** TODO Summary

