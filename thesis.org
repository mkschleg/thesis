#+title: Exploring the effectiveness of Recurrent Networks in Reinforcement Learning
#+FILETAGS: :THESIS:
#+author: Matthew Schlegel
#+STARTUP: overview
#+STARTUP: nolatexpreview
#+OPTIONS: toc:nil
#+OPTIONS: title:nil
#+OPTIONS: ':t
#+LATEX_CLASS: thesis
#+LATEX_HEADER: \input{variables.tex}
#+MACRO: c #+latex: %
#+MACRO: citeplease *[CITEPLEASE: $1, $2, $3, $4, $5, $6]*

* Preamble                                                           :ignore:
#+begin_comment
Preamble for UofA thesis. Needed to make thesis compliant. I use this in my candidacy as well, with specific
details commented out for brevity. This makes:
- title page
- abstract page
- table of contents
- list of tables
- list of figures

and sets formatting up for main text.
#+end_comment

#+BEGIN_EXPORT LaTeX

\renewcommand{\onlyinsubfile}[1]{}
\renewcommand{\notinsubfile}[1]{#1}

\preamblepagenumbering % lower case roman numerals for early pages
\titlepage % adds title page. Can be commented out before submission if convenient

\subfile{\main/tex/abstract.tex}

\doublespacing % possible options \truedoublespacing, \singlespacing, \onehalfspacing

%%%%%%%
% Additional files for thesis
%%%%%% 

% Below are the dedication page and the quote page. FGSR requirements are not
% clear on if you can have one of each or just one or the other. They do say to
% ask your supervisor if you should have them at all.
%
% The CS Department links to a comparison of pre- and post-Spring 2014 thesis
% guidelines (https://www.ualberta.ca/computing-science/graduate-studies/current-students/dissertation-guidelines)
% The comparison document lists an optional dedication page, but no quote page.

\subfile{\main/tex/preface.tex}
\subfile{\main/tex/dedication.tex}
\subfile{\main/tex/quote.tex}
\subfile{\main/tex/acknowledgements.tex}


\singlespacing % Flip to single spacing for table of contents settings
               % This has been accepted in the past and shouldn't be a problem
               % Now the table of contents etc.
               
\tableofcontents
\listoftables  % only if you have any
\listoffigures % only if you have any

% minimal support for list of plates and symbols (Optional)
%\begin{listofplates}
%...            % you are responsible for formatting this page.
%\end{listofplates}
%\begin{listofsymbols}
%...            % You are responsible for formatting this page
%\end{listofsymbols}
               
% A glossary of terms is also optional
\printnoidxglossaries
               
% The rest of the document has to be at least one-half-spaced.
% Double-spacing is most common, but uncomment whichever you want, or 
% single-spacing if you just want to do that for your personal purposes.
% Long-quoted passages and footnotes can be in single spacing
\doublespacing % possible options \truedoublespacing, \singlespacing, \onehalfspacing

\setforbodyoftext % settings for the body including roman numeral numbering starting at 1

#+END_EXPORT






* Introduction
:PROPERTIES:
:CUSTOM_ID: chap:introduction
:END:


The goal of machine intelligence research is to design an agent which interacts with its world through a continuous stream of interactions, consistently getting better through feedback, either provided or self-guided. Continual reinforcement learning [[cite:&ring1994continual;&khetarpal2022continual]] is one paradigm in which we can design such systems. Similar to reinforcement learning (RL), a continual RL agent seeks to maximize the cumulative sum of rewards---which provides a simple yet effective path towards learning behavior. The main attribute of continual RL is that there is an unending stream of sensorimotor interactions which the agent must sift through online. This is different from many instantiations of reinforcement learning where the agent often gets to the goal or the end of a maze and then is sent back to the beginning to continue learning, in continual RL the agent never reaches a point where it is reset. The agent must also balance computation to make decisions within a small time frame while also learning from recent observations. These attributes make the continual RL problem challenging, but applicable to a wide range of real-world applications.

Continual RL systems in the real-world will inevitably have to make decisions when the information available is insufficient to make an optimal choice. For example, an autonomous car driving on the highway. An autonomous car can have an abundance of sensors to perceive the cars around it---say to ensure other cars are currently within their lane or following the speed limit--or measure its own internal properties such as temperature, total fuel, and speed. While it seems the car has sufficient information at an instantaneous observation to predict with confidence, there are still many properties of the world obscured to the agent. If the neighboring car's wheel pops off because the bolts weren't tightened properly or if the driver two cars over drops their coffee and swerves towards the agent. These environment attributes could result in conditions in which the agent could never predict the future given its senses. In situations like these, where the agent has improper context, the world appears non-stationary: the world responds differently when the agent acts according to the same set of senses.

In the face of missing information about the environments underlying properties (also known as partial observability), the main approach developed in machine learning is to summarize the past to provide the agent context which is missing from its current senses. In the autonomous car example, an agent can keep a history to measure how erratic other drivers have been. The agent could notice a wobble in the car with the faulty wheel or erratic driving from the driver juggling hot coffee. Using this information the agent could preemptively move away from dangerous scenarios to minimize the risk of an accident. In the past, artificial intelligence research focused on outlining all possible scenarios the agent could face in what are known as expert systems. These systems faced challenges in that humans were notoriously poor at outlining why they made a decision in a given scenario. The goal of representation learning---which is embodied by the current wave of deep learning---is to construct compact memories of the past in order to aid decision making.

In RL, the most widely used approaches for representation learning are based on ideas developed for supervised learning (SL). Recurrent neural networks (RNNs) have been established as an important tool for learning predictions of data with temporal dependencies. They have been primarily used in language and video prediction [[cite:&mikolov2010recurrent;&wang2016largercontext;&saon2017english;&wang2018eidetic;&oh2015actionconditional]], but have also been used in traditional time-series forecasting [[cite:&bianchi2017recurrent]]. Recurrent networks have also been applied to the RL problem [[cite:&onat1998recurrent;&bakker2002reinforcement;&wierstra2007solving;&hausknecht2015deep;&heess2015memorybased;&zhu2018improving;&igl2018deep]], but often these applications focus on how to fit the RL problem into what will work with the standard recurrent training approaches. Recently, transformers [[cite:&vaswani2017attention]] have become a widely used alternative to recurrent architectures in natural language processing. Transformers have also shown some success in reinforcement learning but either require the full sequence of observations at inference and learning time [[cite:&mishra2018simple;&parisotto2020stabilizing]] or turn the RL problem into a supervised problem using the full return as the training signal [[cite:&chen2021decision]]. This is again similar to the application of recurrent networks where the problem is modified to fit the architecture.


While we can--and arguably should--use the intuitions built in the SL setting for continual RL problems, these intuitions must be tested and re-justified as the problem setting incorporates ways of behaving in an environment or if there is no-longer a well defined dataset with constrained histories---like in continual reinforcement learning. In this thesis, we ask
#+BEGIN_QUOTE
What algorithmic modifications are needed to make reinforcement learning methods more performant in partially observable, continual decision making tasks?
#+END_QUOTE
I use (continual) reinforcement learning to study recurrent architectures and provide novel observations and algorithms to learn representations in the RL setting. In the following sections, I outline the main contributions of this dissertation.

** Incorporating Actions in Recurrent Networks

This contribution (Chapter ref:chap:arnn) empirically evaluates architectural choices for encoding action in the update function of recurrent networks. The evaluation provides 1) simple strategies to drastically improve prediction and control in reinforcement learning that can be applied in a general way to recurrent networks and 2) clarity in what approaches have been applied to recurrent networks in the reinforcement learning literature. Evidence for the preferred strategy is provided through an in-depth analysis of the prediction setting in a small example domain and in a variate of control problems.

** A novel predictive approach to learning in the face of partial observability

In this contribution, I develop a novel approach to incorporate ideas from predictive state representations---an approach to representation learning that is designed for temporal partially observable prediction problems---and recurrent neural networks, called a general value function network (GVFN). The approach is shown to be competitive to RNNs in several continual learning and time-series domains without needing as much history when estimating gradients. This contribution is split into three chapters.
- Chapter ref:chap:gvfn introduces the architecture and relates it to recurrent neural networks and previous predictive approaches.
- Chapter ref:chap:gvfn:algs derives several learning algorithms for the architecture derived from an extension of the /Mean-Squared Projected Bellmen Network Error/.
- Chapter ref:chap:gvfn:empirical empirically compares the new architecture to standard recurrent neural networks in several continual learning prediction problems.

** Designing and generating predictive questions

This contribution investigates the set of predictive questions available to GVFNs and how these can be generated for the architecture discussed above. The impact of this contribution brings clarity to the set of predictive questions applicable to the GVFN architecture (Chapter ref:chap:composite) and progresses towards automatic deployment to a general set of partially observable settings (Chapter ref:chap:gvfn:discovery). These chapters use a collection of lemmas and empirical evidence to draw conclusions about the set of general value functions.

** Off-policy prediction using Resampling

This contribution (Chapter ref:chap:resampling) defines a new off-policy prediction algorithm using importance resampling. The resulting estimator is shown to be more sample efficient than importance sampling, and robust to settings with large importance sampling ratios, while still being consistent. This estimator is widely applicable to reinforcement learning when using a replay buffer, but also is consequential for the predictive approaches developed in this thesis. The evidence provided is through several empirical experiments and theorems with proofs provided in the appendix.

** More generous sections                                         :noexport:
*** Incorporating Actions in Recurrent Networks
This contribution (Chapter ref:chap:arnn) empirically evaluates architectural choices for encoding action in the update function of recurrent networks. The evaluation provides 1) simple strategies to drastically improve prediction and control in reinforcement learning that can be applied in a general way to recurrent networks and 2) clarity in what approaches have been applied to recurrent networks in the reinforcement learning literature. Evidence for the preferred strategy is provided through an in-depth analysis of the prediction setting in a small example domain and in a variate of control problems.
*** Developing a novel predictive approach to recurrent learning

I develop a novel approach to incorporate ideas from predictive state representations---an approach to representation learning that is designed for temporal partially observable prediction problems---and recurrent neural networks. I show this approach can envelop a large set of other predictive approaches: including predictive state representations, temporal-difference networks, and forecasting networks. This contribution includes several theorems and lemmas to support the architecture and relate it to the other predictive approaches.

*** Deriving algorithms to minimize the MSPBNE

This contribution develops the /Mean-Squared Predictive Bellmen Network Error/, extending prior work in temporal-difference networks [[cite:&silver2013gradient]], and then derives several algorithms to minimize such an objective. This contribution outlines several algorithms which can be readily used for training a general value function network. This contribution includes several theorems, lemmas, algorithms, and details to apply the derived algorithms to the GVFN architecture.

*** Evaluating GVFNs


*** Investigating the set of predictive questions

*** Generating for general value function networks

*** Off-policy prediction with Resampling

* Background


In this thesis, I take the perspective that an agent is situated inside its environment and observes its world from an egocentric perspective, continually [[cite:&ring1994continual;&ring1997child;&sutton2011horde]]. In this chapter, I provide the relevant general background. This includes background on reinforcement learning (RL) (including off-policy prediction and control), and learning under the constraint of partial observability. Specific background details related to certain solution methods will be presented closer to their relevant sections.

** Reinforcement Learning
CLOSED: [2023-02-21 Tue 11:50]

The problem setting considered in this thesis is (continual) reinforcement learning (RL). In short, a reinforcement learning agent seeks to maximize a reward signal by acting in the world. In this thesis, I am concerned with two learning problems in reinforcement learning. Specifically, I focus on the model-free prediction and control problem, but each share the same general framework. The agent-environment interaction consists of a stream of data (from the agent's senses), coming in at a consistent rate into the agent's central control systems. In most reinforcement learning, the agent-environment boundary is placed inside the agent's nervous system where parts of the agent's body which are defined through evolution are external to the learning process, and those that are learned and modified through an agent's lifetime are a part of the learning process. This enables RL researchers to focus on the core problem of learning a policy to maximize reward. Figure ref:fig:bg:rl-interaction depicts the agent-environment interaction loop in RL.

#+caption: Diagram of the agent-environment interaction as typically depicted in reinforcement learning.
#+name: fig:bg:rl-interaction
#+attr_latex: :width 0.8\linewidth
[[./plots/rl-diagram.pdf]]

In the agent's lifetime it observes its surroundings, takes actions, and receives rewards as the infinite sequence \(\obs_1, \action_1, \reward_2, \obs_2, \ldots, \obs_t, \action_t, \reward_{t+1}, \obs_{t+1}, \ldots\). The observation \(\obs_t\) is the agent's window into the world through various sensing parts of its body. These can include a camera for vision, microphone for audio, lidar to measure distance from other objects, and many other analog-to-digital conversion technologies. The agent then selects an action \(\Action_t\) which is passed to the agent's actuators or sub-level control system. By performing this action, the agent receives a reward \(\reward_{t+1}\) and another observation \(\obs_{t+1}\) determined by the dynamics of the environment.

The agent-environment interaction can be formalized as a partially observable Markov decision processes (POMDP). The underlying dynamics are defined by a tuple \((\EnvStates, \Actions, \Pmat, f_\obs, \Rewards)\). Given a state \(\envstate \in \EnvStates\) and \(\Action \in \Actions\) the environment transitions to a new state \(\envstate^\prime \in \EnvStates\) according to the state transition probability matrix \(\Pmat \defeq \EnvStates \times \Actions \times \EnvStates \rightarrow [0,\infty)\) with a reward given by \(\Rewards \defeq \EnvStates \times \Actions \rightarrow \Reals\). The observations can then be defined as a lossy function over the environment state \(\obs_t \defeq f_\obs(\envstate_t) \in \Reals^\obssize\), and the reward is \(\reward_t \defeq f_\reward(\envstate_0, \envstate_1, \ldots, \envstate_t) \in \Reals\). This thesis concerns itself primarily with the discrete action setting, where the set of actions is a finite discrete set of values \(\Action \in \Actions \defeq [A_1, A_2, \ldots, A_n]\).

The agent has several canonical internal components. A *policy* is a mapping from states to actions \(\pi: \EnvStates \rightarrow \Actions\) and defines a way of interacting with the environment. Most often a policy defines a probability distribution over the space of Actions conditioned on the agent's state \(\pi(a|\envstate)\defeq\text{The probability of selecting action $\Action$ in state $\envstate$}\). A *value function* is a prediction of the future cumulated (discounted) reward the agent will obtain by following a policy. Specifically,
{{{c}}}
\[
V(\EnvState) = \Expected_\pi[ G_t | \envstate_t = \EnvState, \Action \sim \pi(\cdot| \EnvState)]
\]
{{{c}}}
{{{c}}}
with a state-action value function defined similarly
\[
q(\EnvState, \Action) = \Expected_\pi [ G_t | \envstate_t = \EnvState, \Action_t = \Action].
\]
This thesis uses both state value functions and state-action value functions to do prediction and control. In the following sections I will extend this framework to the partial observable case, and go into the specifics of the prediction problem and the control problem.

** Prediction

*************** DONE Outline and lay foundation for prediction section :noexport:
CLOSED: [2023-02-21 Tue 15:44]
*************** END


The prediction problem in RL is that of learning value functions efficiently and accurately. This process can be used to improve an agent's policy through value iteration or policy iteration [[cite:&sutton2018reinforcement]], or to learn temporal abstractions of the sensorimotor stream through options or general value functions (see Section ref:sec:bg:temporal-abstractions for more details). A value function can be learned either on-policy or off-policy through temporal difference learning. In this section, I introduce the on and off-policy prediction problem as used throughout this text.

As introduced above, a *value function* is a prediction of the future cumulative (discounted) reward received by following a policy \(\tpolicy\),
\[
\Value_\tpolicy(\EnvState) = \Expected_\pi[ G_t | \envstate_t = \EnvState, a \sim \tpolicy(\cdot| \EnvState)]
\]
where \(G_t = \sum_{i=1}^{\infty} \gamma^{i-1} \reward_{t+i} \) is the return. The operator \(\mathbb{E}_{\tpolicy}\) indicates an expectation with actions selected according to policy $\tpolicy$. GVFs encompass standard value functions, where the cumulant is a reward. Otherwise, GVFs enable predictions about discounted sums of others signals into the future, when following a target policy \(\tpolicy\). These values are typically estimated using parametric function approximation, with weights \(\weights \in \RR^d\) defining approximate values \(\Value_\weights(\envstate)\). 

The simplest algorithm to learn the value function is through Monte-Carlo sampling. The brief of the algorithm is to get samples of the return starting in state $\EnvState$ following policy $\tpolicy$, which are then averaged to receive the expected return. You can use the trajectories to estimate the returns for either first-visit to a specific state or on every visit, see cite:&singh1996reinforcementa;&sutton2018reinforcement for more details. This algorithm only requires the environment to be episodic (i.e. clear terminations) and converges to the true value function as the number of rollouts grow.

Another approach to learning value functions is to take advantage of the Bellman equation through dynamic programming. The Bellman equation for the value function $\Value_\tpolicy(\EnvState)$
\begin{align*}
\Value^\pi(\EnvState) &= \Expected_\tpolicy[G_t | \envstate_t = \EnvState, \Action \sim \tpolicy(\cdot | \EnvState)] \\
&= \Expected_\tpolicy[\reward_t + \gamma G_{t+1} | \envstate_t = \EnvState, \Action \sim \tpolicy(\cdot | \EnvState)] \\
&= \overline{R}(\EnvState, \pi(\EnvState)) + \gamma \sum_{\EnvState^\prime} P(\EnvState^\prime | \EnvState, \Action \sim \tpolicy(\EnvState)) \Value^\tpolicy(\EnvState^\prime)
\end{align*}
where $\overline{R}(\EnvState, \tpolicy(\EnvState)$ is the expected one-step reward for policy $\tpolicy$ in state $\EnvState$. The algorithm uses the transition dynamics of the environment $\Pmat$ to iteratively calculate the value function through dynamic programming [[cite:&sutton2018reinforcement]].

Temporal-difference learning combines advantages of both these algorithms, eliminating the need for environment dynamics (as in dynamic programming) and episodic environments (as in Monte-Carlo sampling). For tabular settings, TD learning follows the update rule
\[
\hat{\Value}_{t+1}(\EnvState) \leftarrow \hat{V}_t(\EnvState) + \alpha (\reward_t + \gamma \hat{V}_t(\EnvState^\prime) - \hat{V}_t(\EnvState)).
\]
The target for the temporal-difference learning algorithm is known as the TD target \(\reward_t + \gamma \hat{\Value}_t(\EnvState^\prime)\). TD bootstraps using the previous estimate of the return on the next state \(\hat{\Value}_t(\EnvState^\prime)\) (like dynamic programming) while sampling transitions from the environment following \(\tpolicy\) (like Monte-Carlo sampling).

When using function approximation, the preferred approach is to follow the gradient taken of the value function with respect to the parameters of your function. This is known as the semi-gradient TD learning algorithm
\[
\weights_{t+1} \leftarrow \Value(\EnvState; \weights_t) + \alpha (\reward_t + \gamma \Value(\EnvState^\prime; \weights_t) - \Value(\EnvState; \weights)) \nabla_\weights \Value(\EnvState; \weights).
\]
This update can be seen as minimizing the mean squared TD objective \(\loss(\EnvState, \EnvState^\prime, r_t) = \Vert U_t - \Value(\EnvState; \weights) \Vert^2_2\) assuming the bootstrapped target \(U_t = \reward_t + \gamma \Value(\EnvState^\prime; \weights_t)\) has gradient \(\nabla_\weights U_t = 0\). 


*************** DONE Clean up history of learning value functions on-policy :noexport:
CLOSED: [2023-02-28 Tue 14:23]
*************** END

# Another way to learn the value function is by taking advantage of the Bellman equation through dynamic programming.

# Both of the above algorithms enforce restrictions on the types of problems addressable. Temporal-difference learning combines advantages of both the above algorithms, alleviating some of constraints imposed.

# The off-policy prediction problem is equally concerned with learning value functions of policy $\tpolicy$, but must use data generated from a separate behavior policy $\bpolicy$.

*************** DONE Temporal-difference learning for on-policy prediction :noexport:
CLOSED: [2023-02-28 Tue 14:23]
*************** END
*** Off-policy prediction

In off-policy prediction, transitions are sampled according to behavior policy, rather than the target policy. 
To get an unbiased sample of an update to the weights, the action probabilities need to be adjusted. Consider on-policy temporal difference (TD) learning, with update \(\alpha_t\delta_t\nabla_\theta \Value_{\weights}(\envstate)\) for a given \(\EnvState_t = \envstate\), for learning rate \(\alpha_t \in \RR^+\) and TD-error \(\delta_t \defeq R_{t+1} + \gamma_{t+1}\Value_{\weights}(\EnvState_{t+1}) -  \Value_{\weights}(\envstate)\). If actions are instead sampled according to a behavior policy \(\bpolicy: \EnvStates \times \Actions \rightarrow [0,1]\), then importance sampling (IS) is used to modify the update, giving the off-policy TD update $\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\weights}(\envstate)$ for IS ratio $\rho_t \defeq \frac{\tpolicy(\Action_t | \EnvState_t)}{\bpolicy(\Action_t | \EnvState_t)}$.  Given state $\EnvState_t = \envstate$, if $\bpolicy(\Action | \envstate) > 0$ when $\tpolicy(\Action | \envstate) > 0$, then the expected value of these two updates are equal. To see why, notice that
{{{c}}}
\begin{equation*}
  \mathbb{E}_\mu\left[\alpha_t\rho_t\delta_t\nabla_\weights \Value_{\weights}(\envstate) |\EnvState_t = \envstate\right]
  =  \alpha_t\nabla_\weights \Value_{\weights}(s)\mathbb{E}_\mu\left[\rho_t\delta_t |\EnvState_t = \envstate\right]
\end{equation*}
which equals $\mathbb{E}_\pi\left[\alpha_t\rho_t\delta_t\nabla_\weights \Value_{\weights}(\envstate) |\EnvState_t = \envstate\right]$ because
{{{c}}}
\begin{align*}
\mathbb{E}_\mu\left[\rho_t\delta_t |\EnvState_t = \envstate\right] 
% &= \sum_{\action \in \Actions} \mu(\action | \state) \mathbb{E}\left[\rho_t\delta_t |\stater_t = \state, \actionr_t = \action \right]\\ 
&= \sum_{\Action \in \Actions} \mu(\Action | \envstate) \frac{\tpolicy(\Action | \envstate)}{\bpolicy(\Action | \envstate)} \mathbb{E}\left[\delta_t |\EnvState_t = \envstate, \Action_t = \action \right]
% &= \sum_{\action \in \Actions} \tpolicy(\action | \state) \mathbb{E}\left[\delta_t |\stater_t = \state, \actionr_t = \action \right] \\
= \ \mathbb{E}_\pi\left[\delta_t |\EnvState_t = \envstate\right].
\end{align*}

Though unbiased, IS can be high-variance. A lower variance alternative is Weighted IS (WIS). For a batch consisting of transitions $\{(\envstate_i, \Action_i, \envstate_{i+1}, \reward_{i+1}, \rho_i)\}_{i=1}^n$, batch WIS uses a normalized estimate for the update.
For example, an offline batch WIS TD algorithm, denoted WIS-Optimal below, would use update \(\alpha_t \frac{\rho_t}{\sum_{i=1}^n \rho_i} \delta_t\nabla_\weights \Value_{\weights}(\envstate)\). Obtaining an efficient WIS update is not straightforward, however, when learning online and has resulted in algorithms in the SGD setting (i.e. $n=1$) specialized to tabular cite:&precup2001offpolicy and linear functions cite:&mahmood2014weighted;&mahmood2015off.

*************** DONE Fill in TDC update rule                      :noexport:
CLOSED: [2023-03-01 Wed 12:12]
*************** END

While the above objectives have been shown to effectively work in a wide range of problem settings, there are a series of known counter examples where these algorithms do not converge. This is due to what is known as the deadly-triad in off-policy semi-gradient TD: off-policy, function approximation, and bootstrapping. Removing any of these properties results in a convergent learning rule. Instead, minimizing an objective known as the /mean squared projected Bellmen error/ (MSPBE) converges to the TD fixed point. This objective minimizes the full Bellman error through a projection operator [[cite:&sutton2009fast;&maei2009convergent]]. Minimizing this objective results in several algorithms including one known as temporal-difference with corrections (TDC). For linear function approximation $V(\EnvState_t; \weights_t) = \weights_t^\trans \phi_t$ (where \(\phi_t\) is the features corresponding to state \(\EnvState_t\)
\begin{align*}
\weights_{t+1} &\leftarrow \weights_{t} + \alpha \delta_t \phi_t - \alpha \gamma \phi_{t+1} (\phi_t^\trans \secweights_{t}) \\
\secweights_{t+1} &\leftarrow \secweights_t + \beta(\delta_t - \phi_t^\trans \secweights_t) \phi_t \\
\end{align*}
where \(\alpha\) and \(\beta\) are learning rates which can also be set per time-step. This algorithm can also be derived when the value function is non-linear [[cite:&maei2009convergent]]. See Chapter ref:chap:gvfn:algs for a non-linear derivation with added constraints.

** Control in Reinforcement Learning
CLOSED: [2023-02-21 Tue 11:50]
:PROPERTIES:
:CUSTOM_ID: sec:bg:control
:END:
The bread and butter problem for reinforcement learning research is the control problem. The control problem is the process of searching (or learning) a policy which the agent can use to decide actions. There are many possible approaches for control in reinforcement learning, from value-based control (through q-learning) to direct policy optimization through policy gradient and actor critic methods. All the control experiments in this thesis use value-based control as a means to study the perception of reinforcement learning agents (see ref:sec:bg:perception for more details).

As defined above, a state-action value function
\[
\QValue(\envstate, a) = \Expected_\optpolicy [ G_t | \EnvState_t = \envstate, \Action_t = \Action].
\]
where \(\optpolicy\) is the optimal policy is the main object for value based control. The goal of the agent is to search through the space of policies to maximize the total return the agent will receive from any state, or in other words to find the optimal policy \(\optpolicy\). In this thesis, our control experiments are restricted to Q-learning [[cite:&watkins1992qlearning;&mnih2015humanlevel]], an off-policy technique which learns the optimal policy. Q-learning, in its simplest form, is defined by the following set of updates
\begin{align*}
\delta_{t+1} &= \reward_{t+1} + \gamma \max_a (Q(\EnvState_{t+1}, a)) - Q(\EnvState_t, \Action_t) \\
Q(\EnvState_t, \Action_t) &\leftarrow Q(\EnvState_t, \Action_t) + \alpha\delta_{t+1} 
\end{align*}

So far, the above update rule for q-learning is defined for the tabular setting. In section ref:sec:bg:deeprl, this algorithm is extended to the function approximation setting.

# See Sections ref:sec:bg:func-approx and ref:sec:bg:perception for details on how to apply this method when using deep learning function approximation and recurrent neural networks respectively.

** Perception and Partial Observability in Reinforcement Learning
:PROPERTIES:
:CUSTOM_ID: sec:bg:perception
:END:

A setting which is partially observable the observations are a function of an unknown, unobserved underlying state.
The dynamics are specified by transition probabilities \(\Pfcn = \EnvStates \times \Actions \times \EnvStates \rightarrow [0,\infty)\) with state space \(\EnvStates\) and action-space \(\Actions\). On each time step the agent receives an observation vector \(\obs_t \in \Observations \subset \Reals^\obssize\), as a function \(\obs_t = \obs(\envstate_t)\) of the underlying state \(\envstate_t \in \EnvStates\). The agent only observes \(\obs_t\), not \(\envstate_t\), and then takes an action \(\action_t\), producing a sequence of observations and actions: \(\obs_{0}, \action_{0}, \obs_{1}, \action_1, \ldots\).

The minimal set of histories \(\Hist\) enables the Markov property for the distribution over next observation
{{{c}}}
{{{c}}}
\begin{equation}
\!\Hist = \left\{ \hist_t \!=\! (\obs_0, \action_0, \ldots, \obs_{t-1}, \action_{t-1}, \obs_t) \ | \ \substack{\text{(Markov property)} \Pr(\obs_{t+1} | \hist_t, \action_t ) = \Pr(\obs_{t+1} | \obs_{-1} \action_{-1} \hist_t \action_t), \\ \text{ (Minimal history) }   \Pr(\obs_{t+1} | \hist_t ) \neq \Pr(\obs_{t+1} | \obs_1, \action_1, \ldots, \action_{t-1}, \obs_t )} \right\}
\end{equation}
{{{c}}}
The goal for the agent under partial observability is to identify a state representation \(\state_t \in \RR^\numgvfs\) which is a sufficient statistic (summary) of history \(\Hist\), for targets \(y_t\). More precisely, such a /sufficient state/ ensures that \(y_t\) given this state is independent of history \(\hist_t = \obs_0, \action_{0}, \obs_1, \action_1, \ldots, \obs_{t-1}, \action_{t-1}, \obs_{t}\),
{{{c}}}
{{{c}}}
\begin{equation}
  p(y_{t} | \State_t) = p(y_{t} | \State_t, \hist_t)
\end{equation}
{{{c}}}
{{{c}}}
or so that statistics about the target are independent of history, such as \(\mathbb{E}[Y_{t} | \state_t] = \mathbb{E}[Y_{t} | \state_t, \hist_t]\).
Such a state summarizes the history, removing the need to store the entire (potentially infinite) history.

In the next two sections, I detail recurrent neural networks (RNNs) and the algorithms used to train RNNs in SL and RL.

*** Recurrent Neural Networks
CLOSED: [2023-02-22 Wed 13:17]
:PROPERTIES:
:CUSTOM_ID: sec:bg:rnns
:END:

Recurrent neural networks (RNNs) have been established as an important tool for learning predictions of data with temporal dependencies. They have been primarily used in language and video prediction [[cite:&mikolov2010recurrent;&wang2016largercontext;&saon2017english;&wang2018eidetic;&oh2015actionconditional]], but have also been used in traditional time-series forecasting [[cite:&bianchi2017recurrent]] and RL [[cite:&onat1998recurrent;&bakker2002reinforcement;&wierstra2007solving;&hausknecht2015deep;&heess2015memorybased;&zhu2018improving;&igl2018deep]]. In this section, I will outline the three major architectures applied in this thesis. In the next section I will detail the algorithms deployed to train these architectures in SL and RL.

An RNN provides one such solution to learning \(\agentstate_t\) and associated state update function. The simplest RNN is one which learns the parameters \(\weights \in \Reals^\numparams\) recursively
{{{c}}}
\[
  \agentstate_t = \sigma(\weights \xvec_t + \bvec)
\]
{{{c}}}
where \(\xvec_t = [\obs_t, \agentstate_{t-1}]\) and \(\sigma\) is any non-linear transfer function (typically tanh). While concatenating information (or doing additive operations) has become standard in RNNs, another idea explored earlier in the literature and in more modern cells is using multiplicative operations
{{{c}}}
\[
  (\agentstate_t)_i = \sigma\left(\sum_{j=1}^M \sum_{k=1}^N\weights_{ijk} (\obs_t)_j (\agentstate_{t-1})_k + \bvec_i\right) \quad\quad \triangleright \text{ where } \weights \in \Reals^{|\agentstate| \times |\obs| \times |\agentstate| }.
\]
{{{c}}}
Using this type of operation was initially called second-order RNNs [[cite:&goudreau1994firstorder]], and was also explored in one of the first landmark successes of RNNs [[cite:&sutskever2011generating]] in a character-level language modeling task.

There are several known problems with simple recurrent units (and to a lesser extent other recurrent cells). The first is known as the vanishing and exploding gradient problem [[cite:&pascanu2013difficulty]]. In this, as gradients are multiplied together (via the chain rule in back-propagation through time) the gradient can either become very large or vanish into nothing. In either case, the learned networks often cannot perform well and a number of practical tricks are applied to stabilize learning [[cite:&bengio2013advances]]. The second problem is called saturation. This occurs when the weights \(\weights\) become large and the activations of the hidden units are at the extremes of the transfer function. While not problematic for learning stability, this can limit the capacity of the network and make tracking changes in the environment dynamics more difficult [[cite:&chandar2019nonsaturating]].

Many specialized architectures have been developed to improve learning with recurrence. These architectures are designed to better learn long-temporal dependences and avoid saturation [[cite:&hochreiter1997long;&cho2014properties;&chung2014empirical;&greff2017lstm;&chandar2019nonsaturating]]. The experiments presented in this work use three cell types. The first was the simple RNN introduced earlier in this section. The other cells used are Long short-term memory cells (LSTM) [[cite:&hochreiter1997long]], and gated-recurrent units (GRU) [[cite:&chung2014empirical]] which are standard cells used throughout sequence prediction in supervised learning. Long short-term memory cells (LSTM) were developed to address the issues with modeling long-temporal dependencies and the vanishing gradients problem observed in simple RNN cells. Gated-recurrent units (GRU) are a modification from the LSTM cell which maintains performance in many settings, improves ease of use, and improves computational footprint [[cite:&greff2017lstm]]. 

*************** CANCELLED LSTM Cell Architecture       :noexport:CANCELLED:
CLOSED: [2023-03-28 Tue 13:08]
*************** END

*************** CANCELLED GRU Cell Architecture        :noexport:CANCELLED:
CLOSED: [2023-03-28 Tue 13:08]
*************** END


*** Back-Propagation Through Time
CLOSED: [2023-03-28 Tue 13:03]
:PROPERTIES:
:CUSTOM_ID: sec:bg:bptt
:END:


In supervised learning, back-propagation through time (BPTT) [[cite:&mozer1995focused]] is the most often used algorithm for estimating the gradients of recurrent networks. In this section, BPTT is briefly introduced alongside some alternatives.
# This algorithm works by rolling the recurrent network back through the sequence used in the forward inference of the model, effectively treating the rolled out network as one deep neural network. RNNs are typically trained through the use of back-propagation through time (BPTT) [[cite:&mozer1995focused]].
This algorithm effectively unrolls the network through the sequence and calculates the gradient as if it was one large network with shared weights. When calculating the gradients through time for a specific sample using BPTT, the loss can be defined [fn:: You can also sum over the temporal dimension, passing in a sequence of labels to the loss. In this work, the loss is calculated with respect to the most recent timestep (or the end of sampled sequence) and calculate gradients back from this.] as
{{{c}}}
\[
  \mathcal{L}_{t}(\obs_1, \ldots, \obs_t, y_{t}, \weights) = \sum_{i}^{N} (v_i(\agentstate(\obs_1, \ldots, \obs_t, \weights)) - y_{t, i})^2
\]
{{{c}}}
where \(N\) is the size of the batch, and \(y\) is the target defined by the specific algorithm, and \(t is the current time.\). This will calculate the loss for a single step at the end of the sequence rolling back through the entire sequence to the beginning.

One might notice the above loss function requires growing computational and memory requirements as the agent interacts with the environment. To limit the computational and memory concerns, the current standard in training recurrent architectures in RL is (truncated) Back-propagation through time. \(p\)-BPTT truncates the unrolling of the network to some number of steps \(p\). While this alleviates computational-cost concerns, the learning performance can be sensitive to the truncation parameter [[cite:&pascanu2013difficulty]], particularly if the dependencies back-in-time are longer than the chosen \(p\)---as reaffirmed by the results in this thesis. The loss is slightly modified from above as
\[
  \mathcal{L}_{t}(\obs_t-p, \ldots, \obs_t, y_{t}, \weights) = \sum_{i}^{N} (v_i(\agentstate(\obs_t-p, \ldots, \obs_t, \weights)) - y_{t, i})^2.
\]

An alternative to (truncated) BPTT is real time recurrent learning (RTRL) cite:&williams1989learning. Unfortunately RTRL is known to suffer high computational costs for large networks. Several approximations have been developed to alleviate these costs [[cite:&tallec2018unbiased;&mujika2018approximating]], but these algorithms often struggle from high variance updates making learning slow. The approximation to the RTRL influence matrix proposed by cite:&menick2020practical shows significant promise in sparse recurrent networks, even outperforming BPTT when trained fully online. citeauthor:&ke2018sparse (citeyear:&ke2018sparse) propose a sparse attentive backtracking credit assignment algorithm inspired by hippocampal replay, showing evidence the algorithm has beneficial properties of both BPTT and truncated BPTT. The focused architecture was often able to compete with the fully connected architecture on length of learned temporal sequence and prediction error on several benchmark tasks. Another line of search/credit assignment algorithms is generate and test [[cite:&kudenko1998feature;&mahmood2013representation;&dohare2022continual;&samani2021learning]]. These search algorithms aren't as tied to their initialization as other systems as they intermittently inject randomness into their search to jump out of local minima. Many of these approaches combine both gradient descent and generate and test to gain the benefits of both. While a full generate and test solution is possible, finding the right heuristics to generate useful state objects quickly could be problem dependent.


** Prediction and Control in Deep Recurrent Reinforcement Learning
:PROPERTIES:
:CUSTOM_ID: sec:bg:deeprl
:END:


When applying deep recurrent neural networks to the RL setting, there are several components which have been shown improve learning [[cite:&mnih2015humanlevel]]. In this section, these components are introduced in a piecemeal way. In the empirical results of this thesis, all details are provided closer to their respective results.


*Loss Functions:* In all the following results, semi-gradient learning updates are used unless otherwise specified. Given a loss function of the form
\[\loss(\State, \Action, \State^\prime, \reward_t) = \Vert U_t - \QValue(\State, Action; \weights) \Vert\]
where the bootstrapped target is \(U_t = \reward_t + \gamma \max_a(\QValue(\State^\prime, a; \weights_t))\) for Watkins q-learning [[cite:&watkins1992qlearning]]. The semi-gradient learning update only considers the gradient with respect to \(\QValue(\State, Action; \weights)\) where the gradient of the target \(\nabla_\weights U_t = 0\). The norm of the loss function is the squared L2-norm \(\Vert \xvec \Vert^2_2 = \sum_i \xvec_i^2 \), unless otherwise specified. 

*Experience Replay Buffer:* The experience replay buffer is mechanism for re-using data [[cite:&lin1993reinforcement;&lin1992selfimproving]] and for inducing an almost independent, identically distributed set of examples with which to train the network [[cite:&mnih2015humanlevel;&schaul2015prioritized]]. The replay buffer simply is a buffer of stored transitions \((\State, \Action, \State\prime, \reward)\) which is sampled according to some distribution (typically uniform).

*Target Networks:* Target networks are a slow moving copy of the network representing the q-function. It is updated according to pre-determined frequency of agent steps. The target-network is used to calculate the bootstrapped target (\(U_t\) above). This is said to provide a more stable target for the network to approximate [[cite:&mnih2015humanlevel]].

*Auxiliary Tasks:* Auxiliary tasks are a set of learning objectives unrelated to the underlying control problem used to supplement an often sparse reward structure. These tasks are often prediction tasks of the observations on a separate head of the network [[cite:&jaderberg2017reinforcement]]. They have also been defined as a set of general value functions (see section ref:sec:bg:temporal-abstractions) [[cite:&sutton2011horde;&jaderberg2017reinforcement]] and discovered through a meta-learning process [[cite:&veeriah2019discovery]] or generate-and-test [[cite:&rafiee2022auxiliary]].


*** Architectural Choices for Recurrent Networks in RL
:PROPERTIES:
:CUSTOM_ID: sec:arnn:arch-choice
:END:
Previously, the components of a deep feed-forward reinforcement learning agent were introduced. Many of these components are the same when the architecture is recurrent, but there are minor differences and challenges which are described below.
  
*The woes of the experience replay buffer:* Current deep learning, including recurrent architectures, in reinforcement learning include the need for an experience replay buffer. While a learning algorithm which overcomes this limitation would likely be preferable, in the short term cohesive strategies for combining an experience replay with recurrent architectures should be empirically explored. There are two major approaches currently: 1) using the stale traces, or 2) warming up the agent from the beginning (or some number of time steps prior) of an episode cite:&hausknecht2015deep. Instead, a third strategy is used (using gradient information to refresh the hidden state to minimize the objective), but found little difference between this and the stale approach. For much more insight and discussion on this choice see cite:&kapturowski2019recurrent.
  
*Target networks and state:* Using a recurrent target network introduces a new challenge. Specifically, in the approach chosen to initialize the hidden state of the target network. Several choices could be made such as rolling forward from the start of an episode, or using the state stored in the replay buffer generated when the example was originally seen. Another possible approach is to use the state in the buffer to regularize the learning of the network [[cite:&nath2020training]]. Unless otherwise specified, the following results simply use the state stored in the buffer to initialize the recurrent network at the beginning of a sequence.

*Objectives matter even more:* It is known that some objective functions are more learnable in both the fully and partially observable settings cite:&mozer1991induction;&vanhasselt2015learning. In this thesis, a specific loss structure is adopted to make comparisons between several styles of architectures (see section ref:sec:bg:bptt  for details), but others could have been used. Another addition are auxiliary tasks which augment the objective function cite:&jaderberg2017reinforcement, and has been argued to improve learning state representations.

** Temporal Abstractions in Reinforcement Learning
:PROPERTIES:
:CUSTOM_ID: sec:bg:temporal-abstractions
:END:

*************** DONE Fill in initial section                      :noexport:
CLOSED: [2023-02-28 Tue 19:21]
*************** END


Reinforcement learning is built on predicting the effect of behavior on future observations and rewards. Many of our algorithms learn predictions of a cumulative sum of (discounted) future rewards, which is used as a bedrock for learning desirable policies. While reward has been the primary predictive target of focus, TD models [[cite:&sutton1995td]] lay out the use of temporal-difference learning to learn a world model through value function predictions. Temporal-difference networks [[cite:&tanner2005thesis;&sutton2005temporaldifference]] take advantage of this abstraction and build state and representations through predictions. [[citeauthor:&sutton2011horde]] ([[citeyear:&sutton2011horde]]) and [[citeauthor:&white2015developing]] (citeyear:&white2015developing) further the predictive perspective by developing a predictive approach to building world knowledge through general value functions (GVFs).

*************** DONE [#A] Fix citations below                     :noexport:
CLOSED: [2023-03-01 Wed 12:28]
*************** END

Two objects in RL which enable agents to reason beyond the moment-to-moment stream of experience are known as *options* [[cite:&precup1998theoretical]] and *general value functions* (GVFs) [[cite:&sutton2011horde]]. In this thesis, I focus on applying GVFs to learning a representation of history, leaving the incorporation of options for future work. GVFs have been pursued broadly in reinforcement learning: citeauthor:&gunther2016intelligent (citeyear:&gunther2016intelligent) used GVFs to build an open loop laser welder controller, [[citeauthor:&linke2020adapting]] ([[citeyear:&linke2020adapting]]) and [[citeauthor:&mcleod2021continual]] ([[citeyear:&mcleod2021continual]]) used predictions and their learning progress to develop an intrinsic reward, citeauthor:&edwards2016application (citeyear:&edwards2016application) used GVFs to build controllers for myoelectric prosthetics, using gvfs for auxiliary training tasks to improve representation learning [[cite:&jaderberg2017reinforcement;&veeriah2019discovery]], to extend a value function's approximation to generalize over goals as well as states [[cite:&schaul2015universal]], and to create a scheduled controller from a set of sub-tasks for sparse reward problems [[cite:&riedmiller2018learning]]. Successor representations and features are predictions of the state, learned or given, which have been shown to improve learning performance [[cite:&dayan1993improving;&russek2017predictive;&barreto2018successor;&sherstan2018acceleratinga]]. 

*************** DONE Make defn of GVFs not history dependent, and move history dependent intro to GVFNs chapter. :noexport:
CLOSED: [2023-03-28 Tue 11:53]
*************** END

General value functions (GVFs) [[cite:&sutton2011horde;&white2015developing]] are a generalization of value functions enabling agents to learn value function predictions of their sensorimotor stream beyond a reward signal. A GVF question is a tuple \((\tpolicy, \cumulant, \gamma)\) composed of a policy \(\tpolicy: \States \times \Actions \rightarrow [0, \infty)\), cumulant \(\cumulant: \States \times \Actions \times \States \rightarrow \RR\) and continuation function[fn:: The original GVF definition assumed the continuation was only a function of \(\State_{t+1}\). This was later extended to transition-based continuation citep:&white2017unifying, to better encompass episodic problems. Namely, it allows for different continuations based on the transition, such as if there is a sudden change from \(\State_t\) to \(\State_{t+1}\). I use this more general definition for this reason, and because the cumulant itself is already defined on the three tuple \((\State_t, a_t, \State_{t+1})\).] \(\gamma: \States \times \Actions \times \States \rightarrow [0,1]\) [[cite:&white2017unifying]]. The answer to a GVF question is a mapping \(\Value: \States \rightarrow \Reals\) or \(\QValue: \States \times \Actions \rightarrow \Reals\) to the expected discounted return of a cumulant function [fn:: Note how \(\State\) is used instead of \(\EnvState\) in these definitions. While the definition can be for either the agent state or the environment state, this dissertation focuses on the case when it is a function of agent state or history. See Chapter ref:chap:gvfn for more details on GVFs defined on history.]:
\[
\Value_{c, \gamma, \pi}(s) = \Expected\left[\sum_{k=t}^\infty \left(\prod_{i=t+1}^k \gamma(S_{i-1}, \Action_{i-1}, S_i)\right) c(\State_{i-1}, \Action_{i-1}, \State_{i}) \middle| S_t = s, A_{t:\infty} \sim \pi \right].
\]

*************** DONE Difference between pred and control demons   :noexport:
CLOSED: [2023-03-01 Wed 13:15]
*************** END

Above GVFs were introduced in the context of prediction (i.e. as a prediction demon). This object can be used to in both prediction (as described above) and for control (similarly to options). A control demon is encoded into a state-action value function and learned through Q-learning [[cite:&watkins1992qlearning]] or Sarsa [[cite:&rummery1994line]] to maximize the return. This is similar to the control problem (Section ref:sec:bg:control)--where the objective is to use value iteration to learn a policy--but often the behavior policy is arbitrarily different from the current policy of the demon. While these objects present unique sets of predictive information, our focus in the thesis will be on prediction demons throughout the thesis and consider the incorporation of control demons in future work. GVF questions, or the definitions used here, can be used as a unifying specification for reinforcement learning tasks (i.e. for options, predictions, control, etc...) [[cite:&white2017unifying]].

* Incorporating action into a recurrent network
:PROPERTIES:
:CUSTOM_ID: chap:arnn
:END:

The major contribution of this chapter is the comparison and analysis of several architectures for incorporating action into the state-update function of an RNN in partially observable RL settings. Many of these architectures have been proposed previously for recurrent architectures (i.e. cite:&zhu2018improving;&schlegel2021general), and others are either related to or obvious extensions of those architectures. The results include an in-depth empirical evaluation on several illustrative domains, and outline the relationship between the domain and architectures using the deep recurrent q-network (DRQN) framework [[cite:&hausknecht2015deep]]. Finally, future directions in developing recurrent architectures designed for the RL problem and discuss challenges specific to the RL setting are discussed.


Recurrent neural networks (RNNs) have been established as an important tool for modeling data with temporal dependencies. They have been primarily used in language and video prediction [[citep:&mikolov2010recurrent;&wang2016largercontext;&saon2017english;&wang2018eidetic;&oh2015actionconditional]], but have also been used in traditional time-series forecasting [[citep:&bianchi2017recurrent]] and RL citep:&onat1998recurrent;&bakker2002reinforcement;&wierstra2007solving;&hausknecht2015deep;&heess2015memorybased. Many specialized architectures have been developed to improve learning with recurrence. These architectures are designed to better model long temporal dependence and avoid saturation including, Long-short Term Memory units (LSTMs) citep:&hochreiter1997long, Gated Recurrent Units (GRUs) citep:&cho2014properties;&chung2014empirical, Non-saturating Recurrent Units (NRUs) citep:&chandar2019nonsaturating, and others. Most modern RNN architectures integrate information through additive operations. However, some work has also examined multiplicative updating citep:&sutskever2011generating;&wu2016multiplicative which follows from what were known as Second-order RNNs citep:&goudreau1994firstorder.

One important design decision is the strategy used to incorporate action in the state update function which can have a large impact on the agent's ability to predict and control (see Figure ref:fig:arnn:ring-world-example). This has been noted before, cite:&zhu2018improving provides a discussion on the importance of these choices developing an architecture which encodes the action through several layers before concatenating with the observation encoding. Other types of action encodings have been used for the state update in RNNs for RL citep:&schaefer2007recurrent;&zhu2018improving;&schlegel2021general, but without an in-depth discussion or focus on the ramifications of the particular choice of architecture.  In other cases, action has seemingly been omitted citep:&oh2015actionconditional;&hausknecht2015deep;&espeholt2018impala. Other state construction approaches also see action as a primary component, predictive representations of state encode predictions as the likelihood of seeing action-observation pairs given a history citep:&littman2002predictive.

#+CAPTION: Learning Curves for various RNN cells in Ring World using experience replay and three strategies to incorporate action into an RNN. The agent learns 20 GVF predictions for 300k steps. The results reported is the root mean squared value error averaged over 50 runs with \(95\%\) confidence intervals with window averaging over 1000 steps. See Section ref:sec:arnn:learnability for full details.
#+NAME: fig:arnn:ring-world-example
[[./plots/arnns/figures/ringworld_example_lc.pdf]]


** Problem Setting                                                :noexport:
CLOSED: [2023-01-18 Wed 13:57]

The agent-environment interaction is formalized as a partially observable markov decision processes (POMDP). The underlying dynamics are defined by a tuple \((\States, \Actions, \Pmat, f_\obs, \Rewards)\). Given a state \(\envstate \in \States\) and \(\action \in \Actions\) the environment transitions to a new state \(\envstate\prime \in \States\) according to the state transition probability matrix \(\Pmat \defeq \States \times \Actions \times \States \rightarrow [0,\infty)\) with a reward given by \(\Rewards \defeq \States \times \Actions \rightarrow \Reals\). The agent observes the sequence \(\obs_t, \action_t, \reward_{t+1}, \obs_{t+1}, \action_{t+1}, \ldots\) where the observations are a lossy function over the state \(\obs_t \defeq f_\obs(\envstate_t) \in \Reals^\obssize\), the actions are selected by the agent's current policy \(\action_t \sim \pi(\cdot|\obs_0, \action_0, \ldots, \action_{t-1}, \obs_t) \rightarrow [0, \infty)\), and the reward is \(\reward_t \defeq f_\reward(\envstate_0, \envstate_1, \ldots, \envstate_t) \in \Reals\).

Experiments are performed in two settings: prediction and control. For prediction, general value functions (GVFs) define the targets citep:&sutton2011horde;&white2015developing. A GVF is a tuple containing a cumulant \(c_{t+1} = f_c(o_t, a_t, o_{t+1}, r_{t+1}) \in \Reals\), a continuation function \(\gamma_{t+1} = f_\gamma(o_t, a_t, o_{t+1}) \in [0, 1]\), and a history \(\hist_t = [\action_0, \obs_1, \action_1, \obs_2, \action_2, \ldots, \obs_t]\) conditioned policy \(\pi(\action_t|\hist_t) \in [0,\infty)\). The goal of the agent is to learn a value function which estimates the expected cumulative return under \(\pi\), 
\begin{equation*}
\Expected_\pi\left[ G_t^c | H_t = \hist_t \right] \quad\quad\text{ where } G_t^c \defeq c_{t+1} + \gamma_{t+1} G_{t+1}^c
.
\end{equation*}
{{{c}}}
The value function is estimated using off-policy semi-gradient TD(0) citep:&sutton1988learning;&tesauro1994tdgammon. For the control setting, the agent learns a policy which maximizes the discounted sum of rewards or return \(G_t \defeq \sum_{i=0}^\infty \gamma^{i} \reward_{i+t+1}\) using Q-learning citep:&watkins1992qlearning.

** Constructing State with Recurrent Networks
CLOSED: [2023-01-18 Wed 13:57]

For convenience, this section reiterates the methods used to learning a state-update function using recurrent neural networks. Much of the content is the same as found in Section ref:sec:bg:perception, with details specific to this contribution. Specifically, the details needed for incorporating RNNs into the deep Q-network framework as originally discussed by [[citeauthor:&hausknecht2015deep]] ([[citeyear:&hausknecht2015deep]]).

For effective prediction and control, the agent requires a state
representation \(\state_t \in \Reals^\statesize\) that is a sufficient statistic of the past: \( \Expected\left[ G^c_t | \state_t \right] = \Expected\left[G^c_t | \state_t, \hist_t\right]\). When the agent learns such a state, it can build policies and value functions without the need to store any history. For example, for prediction, it can learn \(V(\state_t) \approx \Expected\left[ G^c_t | \state_t \right]\).

An RNN provides one such solution to learning \(\state_t\) and associated state update function. The simplest RNN is one which learns the parameters \(\weights \in \Reals^\numparams\) recursively
\[
  \state_t = \sigma(\weights \xvec_t + \bvec)
\]
where \(\xvec_t = [\obs_t, \state_{t-1}]\) and \(\sigma\) is any non-linear transfer function (typically tanh). While concatenating information (or doing additive operations) has become standard in RNNs, another idea explored earlier in the literature and in more modern cells is using multiplicative operations
\[
  (\state_t)_i = \sigma\left(\sum_{j=1}^M \sum_{k=1}^N\weights_{ijk} (\obs_t)_j (\state_{t-1})_k + \bvec_i\right) \quad\quad \triangleright \text{ where } \weights \in \Reals^{|\state| \times |\obs| \times |\state| }.
\]
Using this type of operation was initially called second-order RNNs cite:&goudreau1994firstorder, and was also explored in one of the first landmark successes of RNNs citep:&sutskever2011generating in a character-level language modeling task.

RNNs are typically trained through the use of back-propagation through time (BPTT) citep:&mozer1995focused. This algorithm effectively unrolls the network through the sequence and calculates the gradient as if it was one large network with shared weights.
This unrolling is often truncated at some number of steps \(\tau\). While this alleviates computational-cost concerns, the learning performance can be sensitive to the truncation parameter citep:&pascanu2013difficulty. When calculating the gradients through time for a specific sample our loss is defined as
\[
  \mathcal{L}_{t}(\weights) = \sum_{i}^{N} (v_i(\state_t(\weights)) - y_{t, i})^2
\]
where \(N\) is the size of the batch, and \(y\) is the target defined by the specific algorithm. This effectively means
the loss is calculated for a single step at the end of the seen sequence and gradients are rolled back from this step.

There are several known problems with simple recurrent units (and to a lesser extent other recurrent cells). The first is known as the vanishing and exploding gradient problem citep:&pascanu2013difficulty. In this, as gradients are multiplied together (via the chain rule in BPTT) the gradient can either become very large or vanish into nothing. In either case, the learned networks often cannot perform well and a number of practical tricks are applied to stabilize learning citep:&bengio2013advances. The second problem is called saturation. This occurs when the weights \(\weights\) become large and the activations of the hidden units are at the extremes of the transfer function. While not problematic for learning stability, this can limit the capacity of the network and make tracking changes in the environment dynamics more difficult citep:&chandar2019nonsaturating. Because of these issues, several variations on the simple recurrent cell have been developed including the LSTMs, GRUs, and NSRUs. This contribution uses simple recurrent cells (RNNs) and GRUs following the recommendations from [[citeauthor:&greff2017lstm]] ([[citeyear:&greff2017lstm]]). 

Finally, to improve sample efficiency experience replay (ER) is incorporated. ER is a critical part of a deep (recurrent) system in RL citep:&mnih2015humanlevel;&hausknecht2015deep. There are two key choices here: how states are stored and updated in the buffer and how sequences are sampled citep:&kapturowski2019recurrent. In the following sections, the hidden state of the cell is stored in the experience replay buffer as apart of the experience tuple. This is then used to initialize the state when sampled from the buffer for both the target and non-target networks. Gradients are passed back to the stored state to update them along with our model parameters, see a full discussion in Section ref:sec:conc:arnn:open-problems. A separate initial state is also stored for the beginning of episodes, which is updated with gradients. This is slightly differ from the approach taken by cite:&kapturowski2019recurrent, but this architectural choice should have little impact on the results presented here. If the beginning of an episode is sampled from the replay, the most up to date version of this vector was used to initialize the hidden state. The agent samples states across the episode. For samples at the end of the episode, a shorter sequence length \(\tau\) is used.

** Tensors and Low-Rank Decompositions
CLOSED: [2023-02-24 Fri 10:08]
:PROPERTIES:
:CUSTOM_ID: sec:bg:tensor
:END:

#  I introduce notation used in the next section as well as some details on low-rank decompositions used in some of the architectures.

Before getting to the details of how to encode actions in the state-update function, I will provide the required background on Tensors. The simplest, albeit slightly inaccurate, way to describe and use a tensor is as a multi-dimensional array of numbers (either real or complex) which transform under coordinate changes in predictable ways. In this chapter, tensors are multi-dimensional arrays using Einstein summation notation. The ith, jth, kth component of an order-3 tensor will be denoted with lower indices \(\weightmat_{ijk} \in \Reals\) with associated dimension size denoted with corresponding uppercase letters as \(\weightmat \in \Reals^{I\times J\times K}\). 

Like matrices, tensors have a number of decompositions which can prove useful. For example, every tensor can be factorized using canonical polyadic decomposition (CP decomposition), which decomposes an order-N tensor \(\weightmat \in \Reals^{I_1 \times I_2 \times \ldots \times I_N}\) into N matrices as follows
{{{c}}}
\begin{align*}
  \weightmat_{i_1, i_2, \ldots} &= \sum_{r=1}^R \lambda_r \weightmat^{(1)}_{i_1, r}  \weightmat^{(2)}_{i_2, r}  \ldots \weightmat^{(N)}_{i_N, r} \\
  &= \lambda_r \weightmat^{(1)}_{i_1, r}  \weightmat^{(2)}_{i_2, r} \ldots \weightmat^{(N)}_{i_N, r} \quad \triangleright \text{Explicit summation over $r\in\{1,\ldots,R\}$.}
\end{align*}
{{{c}}}
{{{c}}}
where \(\weightmat^{(j)} \in \Reals^{I_j \times R}\), and \(R\) is the rank of the tensor. This is a generalization of matrix rank decomposition, and exists for all tensors with finite dimensions.

Working with tensors takes a bit more care in deciding which fibers (generalization of row and column) the product should be over. One type of product is known as the n-mode product which is defined as follows 
{{{c}}}
\[
  (\weightmat \times_n \vvec)_{i_1, i_2, \ldots, i_{n-1}, j, i_{n+1}, \ldots i_{N}}
      = \weightmat_{i_1, i_2, \ldots, i_{n-1}, i_n, i_{n+1}, \ldots i_{N}} \vvec_{j, i_n}
\]
{{{c}}}
where \(\vvec \in \Reals^{J, I_n}\).

An important property, which will be used later in this chapter, are the simplifications when using n-mode products with a tensor's rank decomposition. For example, order 3 tensors \(\weightmat \in \Reals^{IJK}\), with CP-decomposition \(\weightmat_{ijk} = \lambda_{r}a_{ir}b_{jr}c_{kr}\) and vector over a strand \(\vvec^{M} = \vvec^{(1, M)} \in \Reals^{1 \times M}\)).
{{{c}}}
\begin{align*}
  (\weightmat \times_2 \vvec^{J} \times_3 \vvec^{K})_{i,1,1}
  &= \sum_{k=1}^K \left(\sum_{j=1}^J\weightmat_{ijk} \vvec^{J}_{1j}\right) \vvec^{K}_{1k} \\
  &= \sum_{k=1}^K\sum_{j=1}^J \left(\sum_{r=1}^R\lambda_{r}a_{ir}b_{jr}c_{kr}\right) \vvec^{J}_{1j} \vvec^{K}_{1k}\\
  &= \sum_{r=1}^R \lambda_{r} a_{ir}
    \left(\sum_{j=1}^J b_{jr}\vvec^{J}_{1j}\right)
    \left(\sum_{k=1}^K c_{kr}\vvec^{K}_{1k}\right)\\
  &=  \sum_{r=1}^R \lambda_{r} a_{ir}\left(\vvec^{J} \Bmat \odot \vvec^{K} \Cmat\right)_{1r} \\
  \weightmat \times_2 \vvec^{J} \times_3 \vvec^{K}
  &= \boldsymbol{\lambda} \Amat \left(\vvec^{J}\Bmat \odot \vvec^{K}\Cmat\right)^\trans
     \quad \triangleright \boldsymbol{\lambda}_{i,i} = \lambda_i
\end{align*}

Similarly to CP decomposition, Tucker rank decomposition can be used to create a similar operation. Tucker rank decomposition decomposes an order-N tensor \(\weightmat \in \Reals^{I_1 \times I_2 \times \ldots \times I_N}\) into N matrices another order-N tensor \(G \in \Reals^{R_1 \times R_2 \times \ldots \times R_N}\) as follows

\begin{align*}
  \weightmat_{i_1, i_2, \ldots i_N} &= \sum_{r_1=1}^{R_1} \sum_{r_1=1}^{R_1} \ldots
  \sum_{r_1=1}^{R_1} g_{r_1 r_2 \ldots r_N} \weightmat^{(1)}_{i_1, r_1}
  \weightmat^{(2)}_{i_2, r_2}  \ldots \weightmat^{(N)}_{i_N, r_N}.
\end{align*}

With similar simplifications to CP decomposition,

\begin{align*}
  (\weightmat \times_2 \vvec^{J} \times_3 \vvec^{K})_{i,1,1}
  &= \sum_{k=1}^K \left(\sum_{j=1}^J\weightmat_{ijk} \vvec^{J}_{1j}\right) \vvec^{K}_{1k} \\
  &= \sum_{k=1}^K\sum_{j=1}^J \left(\sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_{ip} b_{jq} c_{kr}\right) \vvec^{J}_{1j} \vvec^{K}_{1k}\\
  &= \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_{ip}
    \left(\sum_{j=1}^J b_{jq}\vvec^{J}_{1j}\right)
    \left(\sum_{k=1}^K c_{kr}\vvec^{K}_{1k}\right)\\
  &= \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_{ip} \left(\vvec^{J}  \Bmat\right)_{1q} \left(\vvec^{K} \Cmat\right)_{1r} \\
  \weightmat \times_2 \vvec^{J} \times_3 \vvec^{K}
  &= G \times_1 \Amat^\trans \times_2 \left(\vvec^{J}\Bmat\right)^\trans \times_3 \left(\vvec^{K}\Cmat\right)^\trans \\
  &= \Amat \left[\left(G ^\trans \times_2 \left(\vvec^{J}\Bmat\right)^\trans\right) \left(\vvec^{K}\Cmat\right)^\trans \right].
\end{align*}

One interesting property of this operation is now each of the dimensions can have a separately tuned rank, giving the system designer more discretion on where to focus representational resources.

Using a lower rank approximation of a multiplicative operation has been derived before several times. A multiplicative update was used to make action-conditional video predictions in Atari [[cite:&oh2015actionconditional]]. This operation also appears in a lower-rank approximation defined by Predictive State RNN hidden state update [[cite:&downey2017predictive]], albeit never performed as well as the full rank version.

** Architectural Designs for Incorporating Action
CLOSED: [2023-01-18 Wed 13:57]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:design
:END:

#+CAPTION: Visualizations of the multiplicative and additive RNNs. The dimensions of the weight matrices use the size of the RNN's state $|s_{t-1}| = n$ and the size of the observation $|o_t| = m$.
#+NAME: fig:arnn:viz-rnn
#+attr_latex: :width 0.8\linewidth
[[./plots/arnns/figures/RNN.pdf]]


There are two broad categories for incorporating action into the state update function of an RNN. This section defines these categories and discusses various variations in these categories (see Figure ref:fig:arnn:viz-rnn for a visualization of two main architectures).

*** Additive

The first category is to use an additive operation. The core concept of additive action recurrent networks is concatenating an action embedding as an input into the recurrent cell citep:&schaefer2007recurrent;&zhu2018improving. For example, the update becomes
{{{c}}}
\begin{align*}
  \state_t = \sigma\left( \Wmat^\xvec \xvec_t + \Wmat^\avec \avec_{t-1} + \bvec \right) \tag*{\bf (Additive)}
\end{align*}
{{{c}}} 
{{{c}}} 
where \(\Wmat^\xvec\) and \(\Wmat^\avec\) are appropriately sized weight matrices. This requires no changes to the recurrent cell if the action embedding \(\avec_{t-1} \in \Reals^\actionsize\) if concatenated to the observation vector. In the empirical experiments, the additive update cells use a hand-designed one-hot encoding function as all our domains have discrete actions.


A variant of the additive approach was explored in cite:&zhu2018improving, where they modified the architecture slightly to learn a function of the action input \(\avec_t = f_a(a_t)\). The label *Deep Additive* for this architecture, where the action encoding function \(f_a\) is a feed-forward neural network. As in their architecture, the action embedding is concatenated with the observation encodings right before the recurrent network. This focuses the empirical evaluation on the changes in the basic operation rather than enumerating all possible places the action can be concatenated before the recurrent operation.

*** Multiplicative

The second category is inspired by second-order RNNs citep:&goudreau1994firstorder and first appeared as a part of a state update function in cite:&rafols2006temporal, where the observation, hidden state, and action embedding are integrated using a multiplicative operation: 
{{{c}}}
\begin{align*}
  \state_t = \sigma\left(\Wmat \times_2 \xvec_{t} \times_3 \avec_{t-1}\right),  \tag*{\bf (Multiplicative)}
\end{align*}
{{{c}}} 
where \(\Wmat \in \Reals^{|\state_t| \times |\xvec_t| \times |\avec_{t-1}|}\) and \(\times_n\) is the \(n\)-mode product, which is detailed in Section ref:sec:bg:tensor. This type of operation is known to expand the types of functions learnable by a single layer RNN citep:&goudreau1994firstorder;&sutskever2011generating, and decreases the networks sensitivity to truncation citep:&schlegel2021general. 

While this type of update has very clear advantages, there is also a tradeoff in terms of number of parameters and potential re-learning depending on the granularity of the action representation. For example, in the Ring World experiment above the RNN cell with additive used 285 parameters with hidden state size of \(15\). The multiplicative version would have used 510 parameters with the same hidden state size. While this doesn't seem like a lot, in a domain like Atari (with 18 actions, 1024 inputs, and \(|s_t| = 1024\)) the number of parameters would be ~2 million vs ~38 million respectively. As shown below in the empirical study, the size of the state can be significantly reduced when using a multiplicative update. In any case, it would be worthwhile to develop strategies to reduce the number of parameters, which is discussed next.

*** Reducing parameters of the Multiplicative

The first way to reduce the number of parameters is by using a low-rank approximation of the tensor operations. Like matrices, tensors have a number of decompositions which can prove useful. For example, every tensor can be factorized using canonical polyadic decomposition, which decomposes an order-N tensor \(\Wmat \in \Reals^{I_1 \times I_2 \times \ldots \times I_N}\) into n matrices as follows
{{{c}}}
\begin{align*}
  \Wmat_{i_1, i_2, \ldots} &= \sum_{r=1}^\factors \lambda_r \Wmat^{(1)}_{i_1, r}  \Wmat^{(2)}_{i_2, r}  \ldots \Wmat^{(N)}_{i_N, r}
\end{align*}
{{{c}}}
where \(\Wmat^{(j)} \in \Reals^{I_j \times \factors}\), \(\lambda_r \in \Reals\) is the weighting for factor \(r\), and \(\factors\) is the rank of the tensor. This is a generalization of matrix rank decomposition and exists for all tensors with finite dimensions, see Appendix ref:sec:bg:tensor for more details. Several simplifications using the properties of n-mode products can be made. Using the  definition of the multiplicative RNN update,
{{{c}}}
\begin{align*}
  \Wmat \times_2 \xvec_t \times_3 \avec_{t-1}
  &\approx \boldsymbol{\lambda} \Wmat^{out} \left(\xvec_t\Wmat^{in} \odot \avec_{t-1}\Wmat^{a}\right)^\trans
     \quad \triangleright \boldsymbol{\lambda}_{i,i} = \lambda_i.  \tag*{\bf(Factored)}
\end{align*}

Previous work explored using a low-rank approximation of a multiplicative operation. A multiplicative update was used to make action-conditional video predictions in Atari citep:&oh2015actionconditional.  This operation also appears in a Predictive State RNN hidden state update citep:&downey2017predictive, albeit it never performed as well as the full rank version. Our low rank approximation is also similar to the network used in cite:&sutskever2011generating, where they mention optimization issues (which were overcome through the use of quasi-second order methods).

*************** DONE [#B] Deal with deep action appendix section :noexport:
CLOSED: [2023-02-24 Fri 09:29]
*************** END

Another approach to reducing the number of parameters required---and to reduce redundant learning---by using an action embedding rather than a one-hot encoding. For example, in Pong it is known that only ~5 actions matter. By taking advantage of the structure of the action space further reductions to the number of parameters could be made. This architecture is explored briefly in Section ref:app:arnns:deep-action. While this is an important piece of the puzzle, no effort is afforded to learning good action embeddings in the following results and leave it to future work.

** Empirical Questions
CLOSED: [2023-01-18 Wed 13:57]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:experiments
:END:


#+caption: The illustrative environments used in Section ref:sec:arnn:learnability and Section ref:sec:arnn:control respectively. (*left*) The Ring World environment with 6 states is depicted, where the observation the agent receives is denoted in each of the circles, available actions denoted by the red arrows, and the agent's current location denoted by a double line. (*right*) The base TMaze environments are depicted with the available actions denoted below and labeled according to the Bakker's TMaze and Directional TMaze used in Section ref:sec:arnn:control.
#+name: fig:arnn:envs
[[./plots/arnns/figures/environments.pdf]]


In the following sections, I set out to empirically evaluate the three operations for incorporating action into the state update function: \textbf{N}o \textbf{A}ction input (``\textbf{NA}''), \textbf{A}dditive \textbf{A}ction (``\textbf{AA}''), \textbf{M}ultiplicative \textbf{A}ction (``\textbf{MA}''), \textbf{Fac}tored (``\textbf{Fac}''), \textbf{D}eep \textbf{A}dditive \textbf{A}ction (``\textbf{DAA}''). I explore all the variants using both standard RNNs and a GRU cell. Our experiments are primarily driven by the main hypothesis that the multiplicative will strictly outperform the other variants, as suggested by cite:&schlegel2021general. To explore this hypothesis I focus on two main empirical questions:
1. How do the different cells affect the properties of the learned value function and internal state of the agent?
2. Are there examples where the other variants outperform the multiplicative variant?


*Question 1:*

There are several properties I am interested in when analyzing the learning capabilities of our agent. First, and most obvious, is prediction error (calculated using root mean squared value error). While error is a reasonable method to compare different architectures, cite:&kearney2019making argue only inspecting error can be misleading in the quality of the prediction. To account for this in our analysis I visually inspect the raw predictions as well to confirm they are reasonably modeling the target returns. With respect to the internal state, I am primarily interested in understanding if there are qualitative differences which lead to differences in prediction quality.

*Question 2:*

The second question is more straightforward than the first, and requires a complete empirical investigation of all the variants on a set of problems with a diverse set of underlying dynamics and characteristics. You can see this question as an extension of the hypothesis implied by Figure ref:fig:arnn:ring-world-example and cite:&schlegel2021general:
{{{c}}}
\begin{quote}
  The multiplicative update outperforms the other variants in the reinforcement learning setting for both control and prediction.
\end{quote}
{{{c}}}
While the above hypothesis can not be confirmed empirically, if question 2 is affirmed the hypothesis is false. Counter examples for the hypothesis will also lead to more intuitive knowledge about when to apply one of the above variants.


*Other details:*

*************** DONE [#B] Deal with ARNN appendix empirical section :noexport:
CLOSED: [2023-02-24 Fri 09:28]
*************** END

In all control experiments, an \(\epsilon\)-greedy policy with \(\epsilon=0.1\) is used. All networks are initialized using a uniform Xavier strategy citep:&glorot2010understanding, with the multiplicative operation independently normalizing across the action dimension (i.e. each matrix associated with an action in the tensor is independently sampled using the Xavier distribution). Unless otherwise stated, a hyperparameter search was performed for all models using a grid search over various parameters (listed appropriately in the Appendix ref:app:arnns:emp). The number of hyperparameter settings were fixed to be equivalent across all models, except the factored variants which use several combinations of hidden state size and number of factors. The best settings were selected and reported using independent runs with seeds different from those used in the hyperparameter search, unless otherwise specified. All the network sizes were controlled such that they had an approximately equal number of free parameters. All final network sizes can be found in Appendix ref:app:arnns:emp.

** Investigating Properties of the Predictions and State
CLOSED: [2023-01-18 Wed 13:57]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:learnability
:END:

#+caption: Ring World sensitivity curves of RMSVE over the final 50k steps for CELL (hidden size) *(left)* RNN (15), AARNN (15), MARNN (12), FacRNN (12 [solid] and 15 [dashed]), DARNN (12, \(|\avec|=2\)), and *(right)* GRU (12), AAGRU (12), MAGRU (9), FacGRU (9 [solid] and 12 [dashed]), DAGRU (9, \(|\avec|=10\)). Reported results are averaged over 50 runs with a \(95\%\) confidence interval. FacRNN used factors \(\factors=\{12, 8\}\) respectively, and FacGRU used \(\factors=\{14, 12\}\). All agents were trained over 300k steps.
#+name: fig:arnn:rw-sens
[[./plots/arnns/figures/ringworld_trunc.pdf]]

I explore the first empirical question by revisiting the Ring World environment, specifically to test model performance with various truncations, and to compare the architecture's learned state. The Ring World, depicted in Figure ref:fig:arnn:envs, consists of a cycle of states with a single state containing an active observation bit, and other states having an inactive observation bit. The agent can take actions moving either clockwise or counter clockwise in the cycle of states. The agent must keep track of how far it has moved from the active bit. For all experiments, Ring World had 10 underlying states.

The agent's objective is to learn a total of 20 GVFs with state-termination continuation functions of  \(\gamma \in \{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\}\). When the agent observes the active bit in Ring World (i.e. enters the first state) the predictions are terminated (i.e. \(\gamma = 0.0\)). The GVFs use the observed bit as a cumulant. Half follow a persistent policy of going clockwise and the other follow the opposite direction persistently. The agent follows an equiprobable random behavior policy. The agent updates its weights on every step following a off-policy semi-gradient TD update with a truncation values denoted. The agent was trained for \(300000\) steps and averaged over 50 independent runs. Root mean squared value error (RMSVE) is used as the main error metric. RMSVE can be calculated as \[\text{RMSVE}_t = \frac{1}{|V(h_t)|} ||V(h_t) - V_{\text{oracle}}(\envstate_t)||_2,\]
where \(V_{\text{oracle}}\) is a known oracle for the true value function.

*Results:*

Figure ref:fig:arnn:rw-sens contains a survey over truncation values for all the architectures. For both the RNN and GRU cells the MA variant performs the best, while the additive performs the worst of the cells which include action information. Interestingly, the factored variants for the GRU perform almost identically, while the FacRNN with a smaller hidden state perform marginally better. All factored variants straddled the performance of the additive and multiplicative updates. The DAAGRU performs similarly to the AAGRU, while the DAARNN fails to learn in this setting. Finally, the MARNN performs the best overall, only needing a truncation value of \(\tau=6\) to learn, which is shorter than the Ring World. With the same number of parameters, the operation used to update the state can have a significant effect on the required training sequence length \(\tau\) and final performance.

#+caption: Ring World predictions of $\text{seed}=62$ for the multiplicative and additive RNNs. Discounts listed with the target policy persistently going counter-clockwise.
#+ATTR_LATEX: :float wrap :width 0.38\textwidth :placement [15]{r}{0.4\textwidth}
#+name: fig:arnn:rw-pred
[[./plots/arnns/figures/ringworld_pred_truth_vert.pdf]]

To ground the prediction error reported, two representative examples of the learned predictions for the additive and multiplicative RNNs are reported in Figure ref:fig:arnn:rw-pred. These plots show a single seed (selected as the best for the additive) over a small snippet of time, but are representative of our observations of the general performance for both cells. The multiplicative follows the actual prediction within a small delta being as close to zero error as can be expected, while the additive has many artifacts and other miss-predictions for both the myopic (\(\gamma = 0.0\)) and long-horizon (\(\gamma=0.9\)) predictions. In Figure ref:fig:arnn:rw-ind-lcs, all the individual learning curves were reported for the additive and multiplicative.

#+caption: Individual learning curves for the additive (hidden size of 15) and multiplicative (hidden size 12) RNNs in Ring World with truncation $\tau=6$. The plots are smoothed with a moving average with 1000 step window sizes. The gray box denotes the seed used in Figures ref:fig:arnn:rw-pred and ref:fig:arnn:rw-tsne. Overall, the multiplicative is quite resilient to initialization, but the distance from zero error in Figure ref:fig:arnn:ring-world-example can be explained by a few bad initializations.
#+name: fig:arnn:rw-ind-lcs
[[./plots/arnns/figures/ringworld_ind_lcs.pdf]]


#+ATTR_LATEX: :width 0.88\linewidth
#+caption: TSNE plots for the additive and multiplicative RNNs for truncation \(\in \{1, 6\}\). Given the learning objective (described in Section ref:sec:arnn:learnability), the state should have 10 distinct clusters for each state of the underlying environment. One would expect the truncation $\tau=1$ to not be able to produce this kind of state for either cell variant. The learning curves correspond to a single seed (seed=62 which is best for the Additive update). The top scatter plots are colored on the underlying state the agent is currently in, the bottom scatter plots are colored based on the previous action the agent took. TSNE is initilized with the same random seed, with max iterations set to 1000, and perplexity set to 30. {\bf (top)} additive and {\bf (bottom)} multiplicative update functions.
#+name: fig:arnn:rw-tsne
[[./plots/arnns/figures/tsne_combined_seed_62.pdf]]

#+ATTR_LATEX: :width 0.88\linewidth
#+caption: TSNE plots for the additive and multiplicative RNNs for truncation $\in \{1, 6\}$. Given the learning objective (described in Section ref:sec:arnn:learnability), the state should to have 10 distinct clusters for each state of the underlying environment. One would expect the truncation $\tau=1$ to not be able to produce this kind of state for either cell variant. The learning curves correspond to a single seed. The top scatter plots are colored on the underlying state the agent is currently in, the bottom scatter plots are colored based on the previous action the agent took. TSNE is initialized with the same random seed, with max iterations set to 1000, and perplexity set to 30. The results are presened for the median seeds of both cells {\bf (top)} additive uses seed=55 and {\bf (bottom)} multiplicative uses seed=67.
#+name: fig:arnn:rw-tsne-median
[[./plots/arnns/figures/tsne_combined_median.pdf]]

*Looking beyond performance:*

A natural question is why might the multiplicative cell perform significantly better than the other cells in this simple setting? One hypothesis is that the multiplicative cell does a better job at separating the histories on action sequence as compared to the additive operation. While this question is difficult to test, one can peer into the learned state of each cell and see if there are qualitative features that appear to help explain the better performance. After learning (using the same parameters as in Figure ref:fig:arnn:rw-sens) another 1000 steps of hidden states are collected. With these hidden states TSNE citep:&maaten2008visualizing is applied to reduce the space of hidden states to two dimensions. The resulting scatter plots for the additive and multiplicative simple RNNs can be seen in Figures ref:fig:arnn:rw-tsne and ref:fig:arnn:rw-tsne-median.

Overall, the additive and multiplicative separate on the previous action equally well, matching our initial hypothesis. While action is important, the additive seems to be hyper-focused on action even as the cell is able to partition on environment state. The multiplicative, on the other hand, is able to cluster the hidden states for various environment states together with only minor separation on action as seen in states 1 and 7. It is possible this is a natural part of th learning process for both the cells, but the multiplicative is able to cluster the states in less samples. Looking at the median performer (seed=55 and seed=67 for the additive and multiplicative respectively), the additive fails to separate on environment state, while the multiplicative looks similarly to the previous seed.

Above, I hypothesized the separation of action faced by the additive agent could have been an artifact of the learning dynamics. To test this hypothesis TSNEs were generated for several agents at different points in the training process. The results can be seen in Figure ref:fig:arnn:tsnes-over-time. For the multiplicative [50000, 75000, 100000, 300000] are reported. These temporal points represent the major learning milestones of the network. For the additive [50000, 150000, 200000, 500000] was chosen. These go beyond the original experiment's sample limits and shows the major milestones when the network separates the histories according to state. For 100000 steps of training for the multiplicative similar properties where the actions taken to get to specific states are quite separated. As the number of samples grow, to 300000, the states converging to be mostly clustered together regardless of the action taken. The additive version never sees the states converging, where even after 500000 timesteps the actions are still regarded highly by the network.

\begin{figure}
  \centering
  \begin{subfigure}{\textwidth}
    \includegraphics[width=0.95\linewidth]{plots/arnns/figures/ringworld_tsne_marnn_1_6_time_67.pdf}
    \caption{Multiplicative for $\tau=6$ and seed=67.}
  \end{subfigure}
  
  \begin{subfigure}{\textwidth}
    \includegraphics[width=0.95\linewidth]{plots/arnns/figures/ringworld_tsne_aarnn_1_6_time_62.pdf}
    \caption{Additive for $\tau=6$ and seed=62.}
  \end{subfigure}
  \caption{TSNE plots for multiplicative and additive RNNs for various number of training samples.} \label{fig:arnn:tsnes-over-time}
\end{figure}

** Understanding when Action Encoding Does and Does Not Matter
CLOSED: [2023-01-20 Fri 15:59]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:control
:END:

This section investigates learning behavior in two environments with slightly differing properties. The first domains is called TMaze citep:&bakker2002reinforcement, depicted in Figure ref:fig:arnn:envs, with a size of 10, which was initially proposed to test the capabilities of LSTMs in RL using Q-Learning. The environment is a long hallway with a T-junction at the end. The agent receives an observation indicating whether the goal state is in the north position or south position at the T-junction (which is randomly chosen at the start of the episode). The agent can take actions in the compass directions. On each step the agent receives a reward of -0.1 and in the final transition receives a reward of 4 or -1 depending if the agent was able to remember which direction the goal was in. The agent deterministically starts at the beginning of the hallway. The observation in the first state is \([1, 1, 0]\) if the goal state is located above the agent and \([0, 1, 1]\) if the goal state is below the agent. In the final state of the hallway the agent receives \([0, 1, 0]\) as an observation, and everywhere else the observation is \([1, 0, 1]\).

#+caption: *(left)* Bakker's TMaze box plots and violin plots over the performance averaged over the final $10\%$ with 50 independent runs. Trained over 300k steps with $\tau=10$. All GRUs use a state size 6, while RNNs use a state size 20. The deep additive used an action encoding of $|\avec|=4$. *(right)* Directional TMaze comparison over the performance averaged over the final $10\%$ of episodes with 100 independent runs trained over 300k steps with $\tau=12$ for CELL (hidden size): RNN (30), AARNN (30), MARNN (18), DARNN (25, $|\avec|=15$), GRU (17), AAGRU (17), MAGRU (10), DAGRU (15, $|\avec|=8$).
#+name: fig:arnn:tmazes
[[./plots/arnns/figures/dirtmaze_and_tmaze.pdf]]

# \begin{figure}
#   \centering
#   \includegraphics[width=\linewidth]{./plots/arnns/figures/dirtmaze_and_tmaze.pdf}
#   \caption{{\bf (left)} Bakker's TMaze box plots and violin plots over the performance averaged over the final $10\%$ with 50 independent runs. Trained over 300k steps with $\tau=10$. All GRUs use a state size 6, while RNNs use a state size 20. The deep additive used an action encoding of $|\avec|=4$. {\bf (right)} Directional TMaze comparison over the performance averaged over the final $10\%$ of episodes with 100 independent runs trained over 300k steps with $\tau=12$ for CELL (hidden size): RNN (30), AARNN (30), MARNN (18), DARNN (25, $|\avec|=15$), GRU (17), AAGRU (17), MAGRU (10), DAGRU (15, $|\avec|=8$).} 
# \end{figure}

Our control agents are constructed similarly to those used in the Ring World environment. The agent's network is a single recurrent layer followed by a linear layer. A sweep is performed over the size of the hidden state and learning rates, and selected all variants of a cell type to have the same value. The network is trained over 300000 steps with further details reported in appendix ref:app:arnns. The agent's performance is reported over the final \(10\%\) of episodes by averaging the agent success in reaching the correct goal. All results are reported using a box and whisker plot with the distribution. The upper and lower edges of the box represent the upper and lower quartiles respectively, with the median denoted by a line. The whiskers denote the maximum and minimum values, excluding outliers which are marked.

Shown in Figure ref:fig:arnn:tmazes (left), all the cells have similar median performance with the GRU (with no action input) performing the best with the least amount of spread. This conclusion is the same across the size of the hidden state, where the multiplicative and factored variants performed poorly (see Appendix ref:app:arnns for factored results). While this initially suggests the action embedding is not important beyond our simple Ring World experiment, notice the difference in how the environment's dynamics interact with the agent's action. In the TMaze, the underlying position of the agent is affected by only two of the actions (the East and West action), while the North and South actions only transition to a different state at the very end of the maze. Also, the agent's actions do not affect needs to be remembered, no matter what trajectory the agent sees the meaning of the first observation is always the same. Thus, these results are much less surprising. For example, the multiplicative variants will have to learn the update dynamics multiple times for the North and South actions.

#+caption: Sensitivity curves over number of factors \(\factors\) with standard error for the *(top)* FacRNN (30) and *(bottom)* FacGRU (17). All agents were trained over 300k steps. See Appendix ref:app:emp_dtm for sweeps over different state sizes. The data was generated by a sweep over the learning rate with 40 runs and compare to the data in figure ref:fig:arnn:tmazes. The red labels on the x-axis indicate when the network has the same number of parameters as the multiplicative.
#+name: fig:arnn:dirtmaze-fac
#+attr_latex: :width 0.6\textwidth
[[./plots/arnns/figures/dirtmaze_fac.pdf]]


To better replicate these dynamics in TMaze, a direction component is added to the underlying state. For example, many robotics systems must be able to orient and turn to progress in a maze, which I hypothesize actions will be critical for modeling the state.  The agent can take an action moving forward, turning clockwise, or turning counter-clockwise. Instead of the observations only being a function of the position, the agents direction plays a critical role. In the first state, the agent receives the goal observation \([1, 1, 0]\) when facing the wall corresponding to the goal's direction. All other walls have the observation \([0, 1, 0]\), and when not facing a wall the agent receives the observation \([0, 0, 1]\). In DirectionalTMaze the agent is forced to contextualize its observation by the action it takes before or after seeing the observation. The state updates are evaluated using the same methodology as in the TMaze with results reported in Figure ref:fig:arnn:tmazes (right).


Now that the agent must be mindful of its orientation, the action again becomes a critical component in learning. The multiplicative variants outperforming all other variants in this domain. Without action, the GRU and RNN are unable to learn, and even the additive and deep additive versions are unable to learn in 300000 steps. Figure ref:fig:arnn:dirtmaze-fac reports the performance of a sweep over the number of factors and report the as compared to the multiplicative and additive variants. As the factors increase, generally the performance increases as well. This matches our expectations, as with increased factors the factored variants should better approximate the multiplicative variances. But there is a tradeoff when adding too many factors, causing performance to decrease substantially. While the factored variant has some interesting properties, the remaining experiments focus on the base architectures (NA, MA, AA, DA) and report full results with the factored variant in Appendix ref:app:arnns.

** Combining Cell Architectures
CLOSED: [2023-01-20 Fri 15:45]
:PROPERTIES:
:CUSTOM_ID: sec:arnn:combining
:END:

# \begin{SCfigure}
#   \includegraphics[width=0.6\linewidth]{./plots/arnns/figures/combo_cell.pdf}
#   \caption{Two variants of combining cells. State size chosen based on procedures of previous environments. ({\bf top}) Performance of success rates ({\bf left}) TMaze with same basic parameters as above for CELL (hidden size): Softmax GRU (6), Cat GRU (6), Softmax RNN (20), Cat RNN (20). ({\bf right}) Directional TMaze with same parameters as above for CELL (hidden size): Softmax GRU (8), Cat GRU (12), Softmax RNN (15), Cat RNN (22). ({\bf Bottom}) Average softmax weights of cells over training with standard error over runs.} \label{fig:arnn:combination}
# \end{SCfigure}

#+caption: Two variants of combining cells. State size chosen based on procedures of previous environments. *(top)* Performance of success rates *(left)* TMaze with same basic parameters as above for CELL (hidden size): Softmax GRU (6), Cat GRU (6), Softmax RNN (20), Cat RNN (20). *(right)* Directional TMaze with same parameters as above for CELL (hidden size): Softmax GRU (8), Cat GRU (12), Softmax RNN (15), Cat RNN (22). *(bottom)* Average softmax weights of cells over training with standard error over runs.
#+name: fig:arnn:combination
#+attr_latex: :width 0.6\textwidth
[[./plots/arnns/figures/combo_cell.pdf]]

This section empirically evaluates combinations of the additive and multiplicative cells. These architectures are a minor step toward building an architecture which learns the structural bias currently hand designed.

The hidden state between an additive and multiplicative operation are combined through two techniques. The first is through an element-wise softmax. Both the additive and multiplicative have the same size hidden state (\(\state^a\) and \(\state^m\) respectively), and each element of the hidden states are weighted by
{{{c}}}
\[
  \state_i = \frac{e^{\theta^a_i} \state^a_i + e^{\theta^m_i} \state^m_i}{e^{\theta^a_i} + e^{\theta^m_i}}
\]
{{{c}}}
where \(\boldsymbol{\theta}^a, \boldsymbol{\theta}^m \in \Reals^\statesize\). This should learn which cell to use depending on the structure of the problem. The second combination is through concatenating the two hidden state together \(\state = cat(\state^a, \state^m)\). This gives more room for experts to add more state to the different architectures, but in this work the two architectures are fixed to have the same state size.

These combinations are compared to the original architectures in TMaze and Directional TMaze following the same procedure as above. These cells should perform as well as either the additive or the multiplicative (which ever is doing the best in the specific domain). The results can be seen in Figure ref:fig:arnn:combination. Overall, the softmax combination performs similarly or slightly better than the multiplicative version except in the Directional TMaze for the GRUs. In TMaze, concatenating the two states together performed better than the additive and multiplicative cells, but this operation worked slightly worse than the multiplicative in the Directional TMaze. To test the hypothesis that the softmax weighting should emphasize the better cell in a given domain the softmax weighting over the training period is reported. For the TMaze the weightings end being approximately equivalent while the Directional TMaze shows a very distinct separation where the multiplicative is weighted significantly more and the additive is continually down-weighted.

** Learning State Representations from Pixels
CLOSED: [2023-01-20 Fri 15:49]

Finally, an empirical study in two environments with non-binary observations is performed. I am particularly interested in whether the recurrent architectures perform comparably when the observation needs to be transformed by fully connected layers, or when the observation is an image. These experiments only use the GRU cells. Full details can be found in Appendix ref:app:arnns:emp.

The first domain considered is a version of DirectionalTMaze which uses images instead of bit observations. The agent receives a gray scale image observation on every step of size \(28\times28\). The agent sees a fully black screen when looking down the hallway, and a half white half black screen when looking at a wall. The agent observes an even (or odd) number sampled from the MNIST citep:&lecun2010mnist dataset when facing the direction of (or opposite of) the goal. The  rewards are -1 on every step and 4 or -4 for entering the correct and incorrect goal position respectively. The same error metrics as in the prior TMaze environments are reported, with the environment's hallway being of length 6. Notice the hallway size is smaller and the negative reward is larger, this was to speed up learning for all architectures.

Results for the Image DirectionalTMaze can be seen in Figure ref:fig:arnn:scaling-up. In this domains, the multiplicative performs quite well, although not as well as in the simple version. The AAGRU is unable to learn in this setting, and the deep additive variant performs slightly better than the additive.

#+caption: *(left)* Image Directional TMaze percent success over the final \(10\%\) of episodes for 20 runs for CELL (hidden size): AAGRU (70), MAGRU (32), DAGRU (45, \(|\avec| = 128\)). Using ADAM trained over 400k steps, \((\tau = 20)\). GRU omitted due to prior performance. *(center) Lunar Lander average reward over all episodes for CELL (hidden size): GRU (154), AAGRU (152), MAGRU (64), DAGRU (152, \(|\avec|=64\)) and \((\tau = 16)$. {\bf (right)} Lunar Lander learning curves over total reward. Ribbons show standard error and a window averaging over 100k steps was used. Lunar Lander agents were trained for 20 independent runs for 4M steps.
#+name: fig:arnn:scaling-up
#+attr_latex: :width \linewidth
[[./plots/arnns/figures/scale.pdf]]

# \begin{figure}
#   \centering
#   \includegraphics[width=\linewidth]{./plots/arnns/figures/scale.pdf}
#   \caption{{\bf (left)} Image Directional TMaze percent success over the final $10\%$ of episodes for 20 runs for CELL (hidden size): AAGRU (70), MAGRU (32), DAGRU (45, $|\avec| = 128$). Using ADAM trained over 400k steps, $(\tau) = 20$. GRU omitted due to prior performance. {\bf (center)} Lunar Lander average reward over all episodes for CELL (hidden size): GRU (154), AAGRU (152), MAGRU (64), DAGRU (152, $|\avec|=64$) and $(\tau) = 16$. {\bf (right)} Lunar Lander learning curves over total reward. Ribbons show standard error and a window averaging over 100k steps was used. Lunar Lander agents were trained for 20 independent runs for 4M steps.}
# \label{fig:scaling_up}
# \end{figure}

** Learning State Representations from Agent-Centric Sensors
CLOSED: [2023-01-23 Mon 12:56]

The second domain is a partially observable version of the LunarLander-v2 environment from OpenAI Gym cite:&brockman2016openai. The goal is to land a lander on the moon within a landing area. Further details and results can be found in Appendix ref:app:arnns:emp. The observation is modified by removing the anglular speed, and filtering the angle \(\theta\) such that it is 1 if \(-7.5 \le \theta \le 7.5\) and 0 otherwise. The average reward obtained over all episodes and learning curves are reported.

As seen in Figure ref:fig:arnn:scaling-up, our findings generalize to this domain as well. The multiplicative variant improves over the factored (see Appendix ref:app:arnns), additive, and deep additive variants significantly. In the LunarLander environment the multiplicative learns faster, reaching a policy which receives on average 100 total reward per episode. Both the additive and factored eventually learn similar policies, while the standard GRU seems to perform less well (although not statistically significant from the additive variant). The average return is ~100 less than some of the best agents on this domains. The agent does this well \(50\%\) of the time, as seen in the individual median curves in Figure ref:fig:arnn:lunar-median. This difference can be explained by the failure start states being more frequent than in the fully observed case.

\begin{figure}
  \centering
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_ind_gru.pdf}
      \caption{GRU}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_ind_magru.pdf}
      \caption{MAGRU}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_ind_aagru.pdf}
      \caption{AAGRU}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_ind_daagru.pdf}
      \caption{DAAGRU}
    \end{subfigure}
  \caption{Individual learning curves. Line is the median over 1000 episodes, with the shaded region as the 1st and 3rd quantile over the same window.} \label{fig:arnn:lunar-median}
\end{figure}

** Summary and Conclusions
CLOSED: [2023-01-23 Mon 12:57]

This chapter empirically evaluated several strategies for incorporating the previous action into the state update of a recurrent neural network.
The impact of this choice was shown to have a large impact on an RL agent's performance in several environments from several observation types. These empirical results suggest that the multiplicative operation performs the best even when using a smaller state vector, and the factored and the deep additive versions perform marginally better than the additive versions in most domains.

While the multiplicative seems to be the clear winner on the tested domains, it is important to note not all domains require this architecture. One interesting strategy could be to use the softmax combined cells to decide which cell to use in your final architecture (by looking at the softmax weighting). One could also imagine an architecture which is able to learn which cell to use conditioned on the history of the agent (see Section ref:sec:conc:arnnbias). Until better architectures for RL are defined this choice is left to system designers. While the additive and deep additive versions under-performed compared to the other encodings, it still out-performed naively using RNNs without action input.

What is apparent in these experiments and other empirical evidence recently gathered on the performance of recurrent architectures in the online setting citep:&rafiee2022eyeblinks;&schlegel2021general is that the methods and architectures developed and utilized by supervised learning might not be suitable for the reinforcement learning problem. This paper uncovered a simple choice can have a large impact, and provides some evidence that the assumptions made in supervised learning might be holding back recurrent architectures in the reinforcement learning setting (see Section  [[ref:sec:conc:arnn:open-problems]] for details). Small, focused studies using recurrent agents in controlled experiments will continue to produce insights on the limitations of the base algorithms and continue to inspire future algorithm developments.

* General Value Function Networks
:PROPERTIES:
:CUSTOM_ID: chap:gvfn
:END:


In this chapter, a predictive representations of state approach known as general value function networks is introduced. In short, these networks constrain the state of a recurrent network to answer predictive questions in the form of GVFs. This chapter defines the core framework of the general value function network (GVFN). The key contributions include the restriction of predictive questions to be composed in acyclic graphs, and extensions to include the set of predictions made by PSRs, TDNets, and other forecasting networks. In later chapters a set of learning algorithms for these architectures are derived (Chapter ref:chap:gvfn:algs), and then empirically evaluated (Chapter ref:chap:gvfn:empirical).

** Representations of State

Most domains of interest are partially observable, where an agent only observes a limited part of the state. In such a setting, if the agent uses only the immediate observations, then it has insufficient information to make accurate predictions or decisions. A natural approach to overcome partial observability is for the agent to maintain a history of its interaction with the world. For example, consider an agent in a large and empty room with low-powered sensors that reach only a few meters. In the middle of the room, with just the immediate sensor readings, the agent cannot know how far it is from a wall. Once the agent reaches a wall, though, it can determine its distance from the wall in the future by remembering this interaction. This simple strategy, however, can be problematic if a long history length is needed [[citep:&mccallum1996learning]].

State construction enables the agent to overcome partial observability, with a more compact representation than an explicit history. Because most environments and datasets are partially observable---in time series prediction, in modeling dynamical systems and in reinforcement learning---there is a large literature on state construction. These strategies can be separated into Objective-state and Subjective-state approaches.

Objective-state approaches specify a true latent space, and use observations to identify this latent state. An objective representation is one that is defined in human-terms, external to the agent's data-stream of interaction. They typically require an expert to provide feature generators or models of the agent's motion and sensor apparatus. Many approaches are designed for a discrete set of latent states, including HMMs citep:&baum1966statistical and POMDPs [[citep:&kaelbling1998planning]].
A classical example is Simultaneous Localization and Mapping, where the agent attempts to extract its position and orientation as a part of the state [[cite:&durrant-whyte2006simultaneous]].
These methods are particularly useful in applications where the dynamics are well-understood or provided, and so accurate transitions can be used in the explicit models. When models need to be estimated or the latent space is unknown, however, these methods either cannot be applied or are prone to misspecification.

The goal of subjective-state approaches, on the other hand, is to construct an internal state only from a stream of experience. This contrasts objective-state approaches in two key ways. First, the agent is not provided with a true latent space to identify. Second, the agent need not identify a true latent state, even if there is one. Rather, it only needs to identify an internal state that is sufficient for making predictions about target variables of interest. Such a state will likely not correspond to objective quantities like meters and angles, but could be much simpler than the true latent state and can be readily learned from the data stream. Examples of subjective-state approaches to state construction include Recurrent Neural Networks (RNNs) [[cite:&hopfield1982neural;&lin1993reinforcement]], Predictive State Representations (PSRs) [[cite:&littman2002predictive]] and TD Networks [[cite:&sutton2005temporaldifference]].

RNNs have emerged as one of the most popular approaches for online state construction, due to their generality and the ability to leverage advances in optimizing neural networks. An RNN provides a recurrent state-update function, where the state is updated as a function of the (learned) state on the previous step and the current observations. These recurrent connections can be unrolled back in time, making it possible for the current RNN state to be dependent on observations far back in time. There have been several specialized activation units crafted to improve learning long-term dependencies, including long short-term memory units (LSTMs) [[cite:&hochreiter1997long]] and gated recurrent units (GRUs) cite:&cho2014properties. PSRs and TD Networks are not as widely used, because they make use of complex training algorithms that do not work well in practice (see [[cite:&mccracken2006online;&boots2011closing]] and [[cite:&vigorito2009temporaldifference;&silver2013gradient]] respectively). In fact, recent work has investigated facilitating use of these models by combining them with RNNs

[[cite:&downey2017predictive;&choromanski2018initialization;&venkatraman2017predictivestate]]. Other subjective state approaches based on filtering can be complicated to extend to nonlinear dynamics, such as system identification approaches [[cite:&ljung2010perspectives]] or Predictive Linear Gaussian models [[cite:&rudary2005predictive;&wingate2006mixtures]].

One issue with RNNs, however, is that training can be unstable and expensive. There are two well-known approaches to training RNNs. The first, Real Time Recurrent Learning (RTRL) [[cite:&williams1989learning]] relies on a recursive form to estimate gradients. This gradient computation is exact in the offline setting---when RNN parameters are fixed---but only an approximation when computing gradients online. RTRL is prohibitively expensive, requiring computation that is quartic in the hidden dimension size \(\statesize\). Low-rank approximations have been developed [[cite:&tallec2018unbiased;&mujika2018approximating;&benzing2019optimal]] to improve computational efficiency, but these approaches to training RNNs remain less popular than the simpler strategy of Back propagation through time (BPTT).

BPTT explicitly computes gradients of the parameters, by using the chain rule back in time, essentially unrolling the recursive RNN computation. This approach requires maintaining the entire trajectory, which is infeasible for many online learning systems. A truncated form of BPTT (p-BPTT) is often used to reduce the complexity of training, where complexity grows linearly with p: \(O(p \statesize^2)\).
Unfortunately, training can be highly sensitive to the truncation parameters [[cite:&pascanu2013difficulty]], particularly if the dependencies back-in-time are longer than the chosen \(p\)---as I reaffirm in the following experiments.

One potential cause of this instability is precisely the generality of RNNs. These systems require expertise in selecting architectures and tuning hyperparameters [[cite:&pascanu2013difficulty;&sutskever2013training]]. This design space can already be difficult to navigate with standard feed-forward neural networks, and is exacerbated by the recurrence that makes the learning dynamics more unstable. Further, it can be hard to leverage domain expertise to constrain the space of RNNs, and so improve trainability. Specialized, complex architectures have been designed for speech recognition [[cite:&saon2017english]] and NLP [[cite:&peters2018deep]]; redesigning such systems for new problems is an onerous task. Many general purpose architectural restrictions have been proposed, such as GRUs and skip connections (see [[cite:&greff2017lstm]] and [[cite:&trinh2018learning]] for thorough overviews). These methods all provide tools to design, and tune, better architectures, but still do not provide a simple mechanism for a non-expert in deep learning to inject prior knowledge.

An alternative direction, that requires more domain expertise than RNN expertise, is to use predictions as auxiliary losses. Auxiliary unsupervised losses have been used in NLP to improve trainability [[cite:&trinh2018learning]]. Less directly, auxiliary losses were used in reinforcement learning [[cite:&jaderberg2017reinforcement]] and for modeling dynamical systems citep:&venkatraman2017predictivestate, to improve the quality of the representation; this is a slightly different but nonetheless related goal to trainability. The use of predictions for auxiliary losses is an elegant way to constrain the RNN, because the system designers are likely to have some understanding of the relevant system components to predict. For the larger goals of AI, augmenting the RNN with additional predictions is promising because one could imagine the agent discovering these predictions autonomously---predictions by design are grounded in the data stream and learnable without human supervision. Nonetheless, the use of predictions as auxiliary tasks provides a more indirect (second-order) mechanism to influence the state variables. In this work, I ask: is there utility in directly constraining states to be predictions?

To answer this question, a practical approach for learning RNNs where the internal state corresponds to predictions is needed. I propose a new RNN architecture, where the hidden state is constrained to be multi-step predictions, using an explicit loss function on the hidden state.
Specifically, the architecture uses general policy-contingent, multi-step predictions---called General Value Functions (GVFs) [[cite:&sutton2011horde]]---generalizing the types of predictions considered in related predictive representation architectures [[cite:&rafols2005using;&silver2013gradient;&sun2016learning;&downey2017predictive]]. These GVFs have been shown to represent a wide array of multi-step predictions
[[cite:&modayil2014multitimescale]]. 


** Constraining State to be Predictions
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:constraining
:END:

Let us start in a simpler setting and explain how the hidden units could be trained to be n-horizon predictions about the future. Imagine you have a multi-dimensional time series of a power-plant, consisting of \(d\) sensory observations with the first sensor corresponding to water temperature. Your goal is to make a hidden node in your RNN predict the water temperature in 10 steps, because you think this feature is useful to make other predictions about the future.

This can be done simply by adding the following loss: \((\state_{t,1} - \xvec_{t+10, 1})^2\). The combined loss \(L_t(\weights)\) on time step \(t\) is
{{{c}}}
{{{c}}}
\begin{equation}
L_t(\weights) \defeq
\ell(\yhat_t, y_t) +  (\state_{t,1} - \xvec_{t+10, 1})^2
\end{equation}
{{{c}}}
where both \(\yhat_t\) and \(\state_t\) are implicitly functions of \(\weights\). This loss still encourages the RNN to find a hidden state \(\state_t\) that predicts \(y_t\) well. There is likely a whole space of solutions that have similar accuracy for this prediction. The second loss constrains this search to pick a solution where the first state node is a prediction about an observation 10 steps into the future. This second term can be seen as a regularizer on the network, specifying a preference on the learned solution. In general, more than one state node---even all of \(\state_t\)---could be learned to be predictions about the future.

The difficulty in training such a state depends on the chosen targets. For example, long horizon targets---such as 100 steps rather than 10 steps into the future---can be high variance. Even if such a predictive feature could be useful, it may be difficult to learn accurately and could make the state-update less stable. Using n-horizon predictions also requires a delay in the update: the agent must wait 100 steps to see the target to update the state at time \(t\).

Therefore, I propose to restrict ourselves to a class of prediction that have been shown to be more robust to these issues [[citep:&vanhasselt2015learning;&sutton2011horde;&modayil2014multitimescale]]. This class of predictions correspond to predictions of discounted cumulative sums of signals into the future, called General Value Functions (GVFs). Algorithms exist to estimate these predictions online, without having to wait to see outcomes in the future. This property of GVFs is called /independence of span/ citep:&vanhasselt2015learning, meaning learning can be achieved with computation and memory independent of the horizon. Such a property is doubly critical for predictions within an RNN, as it is more likely these predictions can be estimated sufficiently quickly to be usable as state. Further, there is some evidence that this class of predictions is sufficient for a broad range of predictions about the future [[citep:&sutton2011horde;&modayil2014multitimescale;&momennejad2018predicting;&banino2018vectorbased;&white2015developing;&pezzulo2008coordinating]], and so the restriction to GVFs does not significantly limit representability.


*** DONE The components of a predictive state approach
CLOSED: [2023-02-28 Tue 14:26]
   :PROPERTIES:
   :CUSTOM_ID: sec:bg:predreps
   :END:
   
Every approach to constructing state with predictions has three core components. The first is how a predictive question is asked or phrased. This can have dramatic changes to the hypothesis/function class of the predictive state, and induce large differences in the underlying algorithmic assumptions used for training. The second is in how the questions will be answered. An approach must consider the base function classes used to represent answers, the abstractions (either temporally or otherwise), and the learning algorithms applied to the architecture. The third, and probably less studied, is that of discovery. Discovery is the automatic specification of predictive questions to use. GVFNs use general value functions (GVFs) to define predictive questions, and a simple recurrent neural network to answer these questions. And algorithmic approach to discovery is still largely unexplored, tied to the discovery of GVFs more broadly, with some efforts applied to a generate-and-test approach cite:&schlegel2021general.
   
Choosing the semantics of how predictive questions are asked will have major effects in the question's discoverability and answerability. PSRs use histories of action observation pairs to construct predictive question, where the answer is a representation of the probability of the sequence of observations being seen given a history and the agent follows the action sequence cite:&littman2002predictive;&singh2004predictive. TDNs use an /answer network/ which is a graph of target dependencies with the core nodes representing specific parts of the observational space. This graph can be many layers, and is acyclic with a single exception. Both TDNs and PSRs were originally defined only using primitive actions to ask questions, but were later extended to included temporally abstract options through option-conditional TDNs cite:&sutton2005temporaldifference;&rafols2006temporal and hierarchical PSRs (HPSRs) cite:&wolfe2006predictive.

The second topic is that of learning and representing the answers of the predictive questions. While respectively different questions, they are deeply connected in the design of any system. The original work in PSRs restricted the sets of observations and actions to be finite. The reason this was needed was how the answers were represented, given a history and sequence of actions for the sequence of observations to not be trivially zero the observations must be sampled according to a mass function. This was addressed in later work using kernel density estimation and information-theoretic tools to realize PSRs in the case of continuous observations and actions cite:&wingate2007discovery. The answers were then represented as a matrix of predicted values for the core tests, which could be updated incrementally with new observations. TDNs use artificial neural networks to underly their representation of answers. While the organization of nodes is not restricted cite:&sutton2005temporaldifference, most of the empirical results shown can be described as using a recurrent neural network. cite:&schlegel2021general make this restriction more apparent, where they explicitly learn the predictive representation as the state of a recurrent network. This simplifies the comparison to non-predictive subjective state approaches (i.e. RNNs), while also enabling the application of backpropagation through time and real-time recurrent learning. 

The third and final topic is that of discovery. Discovery is the automatic specification of predictive questions to use in learning the predictive state. PSRs approached discovery by exploring the set of tests to construct a core set that enables all other tests to be answered [[cite:&wolfe2005learning;&james2005combining;&mccracken2006online;&wingate2007discovery]]. This objective is trying to find a sufficient statistic of the history for all predictions, and has been discussed in various forms cite:&subramanian2022approximate. I conjecture that finding such a state is not feasible in large complex problems, and searching for such a state would be a poor use of a finite set of computational resources. Instead, the agent should focus on finding a set of questions which is useful for the agents overarching goals---for example, maximizing the return in the control problem. Along this new objective several other approaches have been proposed. Generate and test is a general algorithm for searching through a large space with opaque dynamics cite:&mahmood2013representation;&javed2020learning. Another approach is to define the predictive questions as a parametric optimization problem and use meta-gradient descent cite:&bacon2017theoption;&veeriah2019discovery. This approach splits the problem into two optimization problems: an inner problem and an outer problem. The inner optimization consists of the usual control or prediction procedure, where the agent seeks to maximize the discounted return or lower prediction error. The outer optimization calculates gradients through this procedure, with respect to the meta-parameters.

Given a predictive approach to state building requires consideration of these difficult algorithmic choices, a natural question arises ``Why shouldn't we use non-predictive subjective based approaches for learning state, such as the usual recurrent networks?''. While this thesis won't provide (or seek) a conclusive answer to this question, predictive approaches to state construction may have a positive effect on a system's ability to generalize and learn a state representation. This is stated in the /Predictive Representation Hypothesis/ cite:&schaul2013better:

#+begin_quote
  a(n) /(explicit) predictive representation of state/ will be able to continually construct useful generalizations of the regularities in an environment.
#+end_quote

An /(explicit) predictive representation of state/ is an algorithm, or architecture, which constrains the state to be predictions which minimize an objective separate (or jointly) from the agent's general goal in an environment. This class of algorithms includes PSRs, TDNs, GVFNs, and several others. Because the state will be made of small-specific predictive questions of the agent's sensory-motor stream, as the distributions of the underlying dynamics shift the answers to the questions should appropriately shift as well.

** GVF Networks
:PROPERTIES:
:CUSTOM_ID: GVFNs
:END:

In this section, GVF Networks are introduced. The GVF Network (GVFN) is an RNN architecture where hidden states are constrained to predict policy-contingent, multi-step outcomes about the future. I first describe GVFs and the GVF Network (GVFN) architecture. Several related predictive approaches, such as TD Networks, are discussed in Section [[ref:sec:gvfn:connections]], after introducing GVFNs.

First, the definition of GVFs citep:&sutton2011horde needs to be expanded to the partially observable setting, to use them within RNNs. The first step is to replace state with histories.
\(\Hist\) is defined to be the minimal set of histories, that enables the Markov property for the distribution over next observation
{{{c}}}
{{{c}}}
\begin{equation}
\!\Hist = \left\{ \hist_t \!=\! (\obs_0, \action_0, \ldots, \obs_{t-1}, \action_{t-1}, \obs_t) \ | \ \substack{\text{(Markov property)} \Pr(\obs_{t+1} | \hist_t, a_t ) = \Pr(\obs_{t+1} | \obs_{-1}, \action_{-1}, \hist_t a_t), \\ \text{ (Minimal history) }   \Pr(\obs_{t+1} | \hist_t ) \neq \Pr(\obs_{t+1} | \obs_1, a_1, \ldots, \action_{t-1}, \obs_t )} \right\}
\end{equation}
{{{c}}}
{{{c}}}
A GVF question is a tuple \((\tpolicy, \cumulant, \gamma)\) composed of a policy \(\tpolicy: \Hist \times \Actions \rightarrow [0, \infty)\), cumulant
\(\cumulant: \Hist \times \Actions \times \Hist \rightarrow \RR\) and continuation function[fn:: The original GVF definition assumed the continuation was only a function of \(H_{t+1}\). This was later extended to transition-based continuation citep:&white2017unifying, to better encompass episodic problems. Namely, it allows for different continuations based on the transition, such as if there is a sudden change from \(\hist_t\) to \(\hist_{t+1}\). This more general definition is used for this reason, and because the cumulant itself is already defined on the three tuple \((\hist_t, a_t, \hist_{t+1})\).] \(\gamma: \Hist \times \Actions \times \Hist \rightarrow [0,1]\), also called the discount. On time step t, the agent is in \(H_t\), takes actions \(A_t\), transitions to \(H_{t+1}\) and observes[fn:: Throughout this document, unbolded uppercase variables are random variables; lowercase variables are instances of that random variable; and bolded variables are vectors. When indexing into a vector on time step \(t\), such as \(\hist_t\), a double subscript is used as \(\hist_{t,j}\) for the \(j\)th component of \(\hist_t\).] cumulant \(\Cumulant_{t+1}\) and continuation \(\gamma_{t+1}\). The answer to a GVF question is defined as the value function, \(V: \Hist \rightarrow \RR\), which gives the expected, cumulative discounted cumulant from any history \(\hist_t \in \Hist\). The value function which can be defined recursively with a Bellman equation as
{{{c}}}
{{{c}}}
\begin{align}
  \Value(\hist_t) &\defeq \expect*{ \Cumulant_{t+1} + \gamma_{t+1} \Value(H_{t+1}) | H_t = \hist_t, \Action_{t} \sim \pi(\cdot | \hist_t)} \label{eq-bewh}\\
  &= \sum_{\action_t \in \Actions} \pi(\action_t | \hist_t) \sum_{\hist_{t+1} \in \Hists} \Pr(\hist_{t+1} | \hist_t, \action_t) \left[\cumulant(\hist_t, a_t, \hist_{t+1}) + \gamma(\hist_t,a_t,\hist_{t+1}) \Value(\hist_{t+1}) \right] \nonumber
 .
\end{align}
{{{c}}}
The sums can be replaced with integrals if \(\Actions\) or \(\Observations\) are continuous sets. Assuming that \(\Hist\) is a finite set, for simplicity; the definitions and theory, however, can be extended to infinite and uncountable sets.


A GVFN is an RNN, and so is a state-update function \(f\), but with the additional criteria that each element in \(\state_t\) corresponds to a prediction---to a GVF.
A GVFN is composed of \(\numgvfs\) GVFs, with each hidden state component \(\state_{t,j}\) trained such that at time step \(t\), \(\state_{t,j} \approx \vifunc{j}(\state_t)\) for the \(j\)th GVF and history \(\state_t\). Each hidden state component, therefore, is a prediction about a multi-step policy-contingent question. The hidden state is updated recurrently as \(\state_t \defeq f_\weights(\state_{t-1}, \xvec_t)\) for a parametrized function \(f_\weights\), where \(\xvec_t = [\action_{t-1}, \obs_t]\) and \(f_\weights\) is trained so that \(\state_j \approx \vifunc{j}(\hist_t)\). This is summarized in Figure [[ref:fig:gvfn:gvfnsrnns]].

#+name: fig:gvfn:gvfnsrnns
#+caption: GVF Networks (GVFNs), where each state component $\svec_{t,i}$ is updated towards the return $G_{t,i} \defeq C_{t+1}^{(i)} + \gamma_{t+1}^{(i)} \svec_{t+1,i}$ for the $i$th GVF. The solid forward arrows indicate how state is updated; in fact, the update is the same as a standard RNN. The difference is with the dotted lines, that indicate training. The dotted black arrows indicate the targets for the state components. The dotted red arrows indicate that the target $G_{t,i}$ are created using the observation and state on the next step.
#+attr_latex: :width 0.5\textwidth
[[./plots/gvfn/figures/GVFN_v2.pdf]]

General value functions provide a rich language for encoding predictive knowledge. In their simplest form, GVFs with constant \(\gamma\) correspond to multi-timescale predictions referred to as Nexting predictions citep:&modayil2014multitimescale. Allowing \(\gamma\) to change as a function of state or history, GVF predictions can combine finite-horizon prediction with predictions that terminate when specific outcomes are observed citep:&modayil2014multitimescale.

#+NAME: fig:gvfn:compass-world-env
#+CAPTION: The Compass World: A partially observable grid world with observations of the color directly in front of the agent. \textbf{Actions:} The agent can take the actions Move Forward (one cell), Turn Left, and Turn Right. \textbf{Observations:} The agent observes the color of the grid cell it is facing. This means the agent can only observe a color if it is at the wall and facing outwards. The agent depicted as an arrow would see Blue. In the middle of the world, the agent sees White.  \textbf{Goal:} The agent's goal is to make accurate predictions about which direction it is facing.
#+ATTR_LATEX: :width 0.5\textwidth
[[./plots/gvfn/figures/compworld_with_agent.pdf]]

# \begin{figure}
# +ATTR_LATEX: :float wrap :width 0.38\textwidth :placement {l}{0.4\textwidth}
#   \centering
#   \begin{subfigure}{0.43\textwidth}
#     \includegraphics[width=0.8\textwidth]{plots/gvfn/figures/compworld_with_agent.pdf}
#   \end{subfigure}
#     \caption{ The Compass World: A partially observable grid world with observations of the color directly in front of the agent. \textbf{Actions:} The agent can take the actions Move Forward (one cell), Turn Left, and Turn Right. \textbf{Observations:} The agent observes the color of the grid cell it is facing. This means the agent can only observe a color if it is at the wall and facing outwards. The agent depicted as an arrow would see Blue. In the middle of the world, the agent sees White.  \textbf{Goal:} The agent's goal is to make accurate predictions about which direction it is facing. } \label{fig:compass_world_env}
# \end{figure}

To build some intuition, below are some examples in Compass World. This environment is used in our experiments and depicted in Figure [[ref:fig:gvfn:compass-world-env]]. Compass World is a grid world where the agent is only provided information about the color directly in front it. This world is partially observable, with all the tiles in the middle having a white observation, with the only distinguishing color information available to the agent at the walls. The actions taken by the agent are to move forward, turn left, or turn right.

In this environment, the agent might want to know if it is facing the red wall. This can be specified as a GVF question: ``If I go forward until I hit a wall, what is the probability I will see red?". The policy is to always go forward. If the current observation is `Red', then the cumulant is 1; otherwise it is zero. The continuation \(\gamma\) is 1 everywhere, except when the agent hits a wall and see a color; then it becomes zero. The sampled return from a state is 1.0 if the agent is facing the Red wall, because going forward will result in summing many zero plus a 1 right before termination. If the agent is not facing the Red wall, the return is 0, because the agent terminates when hitting the wall but only sees cumulants that are zero for the entire trajectory. Because the outcome is deterministic, the probabilities are 1 or 0.

The agent could also ask about how frequently it will see Red, within a horizon of about 10 steps. An approximation to this question can be obtained by using a constant continuation of \(\gamma = 0.9\). The intuition for this comes from thinking of \(1-\gamma\) as a success probability for a geometric distribution: the probability of successfully terminating. The mean of this geometric distribution is \(\tfrac{1}{1-\gamma}\)---which in this case is \(\tfrac{1}{1-0.9}= 10\)---provides the expected number of steps until the first success. Recall that termination indicates that a return is cut-off, and so a cumulant is not included in the sum after termination. This probabilistic termination means that even if Red is seen after 10 steps, it will still be included in the return. However, it does indicate its contribution has been significantly decayed. This exponential prediction loses precision, and so the GVF only provides an approximation to this question.

The agent could also also ask if it will see Red, within a horizon of about 10 steps. In this case, the continuation would be \(0.9\) until the agent observed Red, at which point it would become zero (indicating termination). The GVF answer corresponds to a discounted probability of observing Red, with a smaller number if Red is observed further in the future. If the agent always see Red in 1 step from \(\hist_t\), then it observes \(\Cumulant_{t+1} = \) 1 and \(\gamma_{t+1} = 0\) and the value is precisely 1. If the agent sees Red in 2 steps from \(\hist_t\), then \(\Cumulant_{t+1} = 0, \gamma_{t+1} = 0.9, \Cumulant_{t+2} = 1\) and \(\gamma_{t+2} = 0\) resulting in a value of \(0.9\). If the agent sees Red in 10 steps from \(\hist_t\), then the value is \(0.9^9 \approx 0.4\). If just a few more steps into the future, say 15 steps, then the value would be \(0.2\). The magnitudes start to get quite low, indicating that it is less likely to observe Red in this window.

Notice that though the cumulants and continuation functions are defined on the underlying (unknown) state \(\hist_t\), this is a generalization of defining it on the observations. The observations are a function of state; the cumulants and continuations \(\gamma\) that are defined on observations are therefore defined on \(\hist_t\). In the examples above, these functions were defined using just the observations. More generally, these functions are a part of the problem definition. This means they could be defined using short histories, or other separate summaries of the history. As discussed in Section [[ref:chap:gvfn:algs]], one can also consider cumulants that are a function of our own predictions or constructed state.

A natural question is how these GVFs are chosen. This problem corresponds to the discovery problem for predictive representations. This chapter focuses on the utility of this architecture with simple heuristics or expert chosen GVFs. I briefly discuss simple ideas for discovery in Chapter ref:chap:gvfn:discovery, but leave a more systematic investigation of the discovery problem to future work.

** A Case Study using GVFNs for Time Series Prediction
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:case-study
:END:


#+caption: *(left)* Example learning curve for MSO with GVFNs and simple RNNs *(right)* The returns for different \(\gamma\), corresponding to GVFs in the GVFN, for a small section of the MSO time series dataset. The dotted red line for \(\gamma = 0\) looks overlayed with the time series plotted in black, but is actually the observation one step in the future.
#+name: fig:gvfn:mg-example
#+attr_latex: :width 0.95\textwidth
[[./plots/gvfn/timeseries/mso_example_comb.pdf]]


GVFNs can be used for time series prediction by simply assuming that a fixed (unknown) policy generates the data. The GVFs within the network are assumed to have this same fixed unknown policy in common, but differ in the pair of continuation and cumulant functions. For a multi-variate time series, one GVF could have a cumulant corresponding to the first entry of the observation on the next time step, and another GVF could use the second entry. Even for a single-variate time series, meaningfully different cumulants be defined. For example, one GVF could correspond to the probability that the observation becomes larger than 1. The cumulant would be zero until this event occurs, at which point it would be 1. In Figure ref:fig:gvfn:mg-example (left) preliminary results using a GVFN to forecast 12-steps into the future on the single-variate Multiple Super-imposed Oscillator (MSO) time series dataset. The full empirical set-up is discussed in Section ref:sec:gvfn:exp-forecasting, and here some insights relevant to building intuition for how to use GVFNs are provided.

The GVFN consists of a recurrent, constrained layer of 128 GVFS with \(\gamma\)s spaced linearly in \([0.1,0.97]\) to learn the state. To make predictions, feedforward layers from this recurrent layer can be added; here a ReLu layer is added for additional nonlinearity in the prediction. For comparison, a simple RNN is also included, which similarily uses an additional ReLu layer after its recurrent layer. The prediction target is the observation 12 steps into the future. Both the RNN and GVFN have to wait 12 steps to see the accuracy of their prediction, delaying updates based on the target by 12 steps. The GVFN, however, can use the loss on the state at each step, and so more directly influence the value of states with the most recent observations. Both methods use p-BPTT, with truncation \(p\). With a sufficiently high \(p\), both perform well (see Section ref:sec:gvfn:exp-forecasting for results with many \(p\)). The result reported here is for \(p = 1\), where the GVFN already obtains near-optimal performance.

It might be surprising that this simple GVFN, with GVFs only differing in continuation \(\gamma\), can perform well. For time series data, however, such constant \(\gamma\) predictions provide anticipatory information about observations in the future. To see why, the time series as well as returns for \(\gamma\in\{0, 0.75,0.9, 0.96\}\) as dotted lines are shown in Figure ref:fig:gvfn:mg-example (right). These returns reflect the type of information that would be provided by a GVF prediction. At each time point \(t\) on the x-axis, the smaller \(\gamma\), like \(\gamma = 0.75\) as dotted green, anticipate the observations in a nearby window. If the time series is starting to rise in the near future, then the dotted green starts to rise right now. Returns can thus provide useful predictive information about increases and decreases that are expected to soon appear in the time series. Notice that the magnitude of the returns are approximately equal. For practical use, the magnitude of each GVF prediction should be similar, to avoid large differences in magnitude between state variables. With large \(\gamma\), however, the return becomes large and so too does the value function. The standard fix to this is straightforward: each GVF uses a scaled cumulant of \((1-\gamma) o_{t+1}\).

Notice, though, that there is a trade-off between anticipating a cumulant farther into the future and the precision of predictions about the future. Returns with lower continuations predict trends closer to when they occur in the dataset and have higher resolution. Returns with higher continuations anticipate changes further in the future, at the cost of smoothing over the detailed changes in the dataset. By using both lower and higher continuations, the benefits of both can be obtained. This simple heuristic---GVFs with the same cumulant and varying \(\gamma\)---as a general purpose heuristic is further discussed in Chapter ref:chap:gvfn:discovery.
** Connections to Other Predictive State Approaches
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:connections
:END:

The idea that an agent's knowledge might be represented as predictions has a long history in machine learning. The first references to such a predictive approach can be found in the work of [[citeauthor:&cunningham2013intelligence]] ([[citeyear:&cunningham2013intelligence]]), [[citeauthor:&becker1973model]] ([[citeyear:&becker1973model]]), and [[citeauthor:&drescher1991madeup]] ([[citeyear:&drescher1991madeup]]), who hypothesized that agents would construct their understanding of the world from interaction, rather than human engineering. These ideas inspired work on predictive state representations (PSRs) [[cite:&littman2002predictive]], as an approach to modeling dynamical systems. Simply put, a PSR can predict all possible interactions between an agent and it's environment by reweighting a minimal collection of core test (sequence of actions and observations) and their predictions, without the need for a finite history or dynamics model. Extensions to high-dimensional continuous tasks have demonstrated that the predictive approach to dynamical system modeling is competitive with state-of-the-art system identification methods [[cite:&hsu2012spectral]]. PSRs can be combined with options [[cite:&wolfe2006predictive]], and some work suggests discovery of the core tests is possible [[cite:&mccracken2006online]]. One important limitation of the PSR formalism is that the agent's internal representation of state must be composed exclusively of probabilities of action-observation sequences.

TD networks [[cite:&sutton2005temporaldifference]] were introduced after PSRs, and inspired by the PSR approach to state construction that is grounded in observations. GVFNs build on and are a strict generalization of TD networks. A TD network [[cite:&sutton2005temporaldifference]] is similarly composed of \(\numgvfs\) predictions, and updates using the current observation and previous step predictions like an RNN. TD networks with options [[cite:&rafols2005using]] condition the predictions on temporally extended actions similar to GVF Networks, but do not incorporate several of the recent modernizations around GVFs, including state-dependent discounting and convergent off-policy training methods. The key differences, then, between GVF Networks and TD networks is in how the question networks are expressed and subsequently how they can be answered. GVF Networks are less cumbersome to specify, because they use the language of GVFs. Further, once in this language, it is more straightforward to apply algorithms designed for learning GVFs.

More recently, there has been an effort to combine the benefits of PSRs and RNNs. This began with work on Predictive State Inference Machines (PSIMs) cite:&sun2016learning, for inference in linear dynamical systems. The state is learned in a supervised way, by using statistics of the future \(k\) observations as targets for the predictive state. This earlier work focused on inference in linear dynamical systems, and did not state a clear connection to RNNs. Later work more explicitly combines PSRs and RNNs cite:&downey2017predictive,choromanski2018initialization, but restricts the RNN architecture to a bilinear update to encode the PSR update for predictive state. In parallel, another strategy to incorporate ideas from PSRs into RNNs, without restricting the RNN architecture, called Predictive State Decoders (PSDs) cite:&venkatraman2017predictivestate. Instead of constraining internal state to be predictions about future observations, statistics about future observations are used as auxiliary tasks in the RNN.

Of all these approaches, the most directly related to GVFNs is PSIMs. This connection is most clear from the PSIM objective \cite[Equation 8]{sun2016learning}, where the goal is to make predictive state match a vector of statistics about future outcomes. There are some key differences, mainly due to a focus on offline estimation in PSIMs. The predictive questions in PSIMs are typically about observations 1-step, 2-step up to \(k\)-steps into the future. To use such targets, batches of data need to be gathered and statistics computed offline to create the targets. Further, the state-update (filtering) function is trained using an alternating minimization strategy, with an algorithm called DAgger, rather than with algorithms for RNNs. Nonetheless, the motivation is similar: using an explicit objective to encourage internal state to be a predictive state.

A natural question, then, is whether the types of questions used by GVFNs provides advantages over PSIMs. Unlike \(k\)-step predictions in the future, GVFs allow questions about outcomes infinitely far into the far, through the use of cumulative discounted sums. Such predictions, though, do not provide high precision about such future events. As motivated in Section ref:sec:gvfn:constraining, GVFs should be easier to learn online. In our experiments, a baseline, called a Forecast Network, that uses \(k\)-step predictions as predictive features, is included to provide some evidence that GVFs are more suitable as predictive features for online agents.

Researchers in reinforcement learning, decision making, and artificial intelligence aren't alone in asking if decision making systems use predictions to effectively navigate their world cite:&bubic2010prediction;&hawkins2004intelligence;&clark2013whatever.  Anticipation cite:&butz2003anticipatory;&pezzulo2008challenge --which has similar properties to the GVF approach to prediction--has been used to mean elevated processing prior to an event (also prediction) as well as the overall effect of prediction on an agent behaviour. An agent can anticipate an event in the future, and act accordingly. This requires the agent's policy to be defined in terms of predictions, or for the representation to have predictive/anticipatory properties. Hierarchical predictive coding cite:&rao1999predictive;&huang2011predictive was used to explain non-classical interference observed in the visual cortex. In this approach, feedback connections transport predictions (or priors) from higher layers to lower layers to give context to the current observations. Prospective codes cite:&schutz2007prospective take the theory of prospection and encode future events as representations used for planning and simulation. While there is evidence to suggest organic decision making systems are directed forward in their representation of the world, memory and ``postdiction'' both play an important, separate role in building a systems underlying representations cite:&soga2009;&synofzik2013. While I focus on two distinct classes in this thesis (i.e. predictive and postdictive), future architectures should be built to take advantage of both approaches.

While GVFs provide, in our opinion, a better language to ask complex predictive questions, one should understand what is given up when moving away from PSRs and PSIMs in predictive power. In the next two sections it is shown that the questions used in PSIMs and PSRs can be posed by GVFs not only through standard cumulants, but also through composite GVFs. Below I use composite GVFs to show the potential predictive power of complex networks of GVFs. See Chapter ref:chap:composite for a more detailed exploration into some of the properties of composite predictions.

*** \(k\)-step forecasts using GVFs

To warm up, I first show how to construct k-step forecasts using GVFs and then move to the more complex case of the core-tests in PSRs. The simplest way to make \(k\)-step forecasts with a GVF is to construct a cumulant function that requires the agent to receive \(k\) new observations and then update the forecast through any usual means. While this is reasonable in the offline setting, when training online our agent should be able to update predictions as soon as possible without waiting for \(k\) timesteps. This can be done trough composite predictions. With myopic discounts (i.e. \(\gamma=0\))  a chain of GVFs can be constructed such that each GVF in the chain has a cumulant of the prior GVF's prediction on the next time step. The \(k-1th\) GVF in this sequence will be predicting the observation of the first GVF \(k\) steps into the future.

*** Core-tests in Predictive State Representations can be Defined by Composite GVFs

A predictive state representation is made up of a finite set of core-tests \(\{q_1, q_2, \ldots, q_n\}\) and a set of likelihoods indicating the probability of seeing the core-test given the current history \(\Prob(q_1 | H_t)\). The history is constructed as a sequence of observation-actions from the beginning of the agent's lifetime \(\Hist_t = \{o_0 a_0 o_1 a_1 \ldots o_t\}\). Each core-test is a sequence \(q = \{a_0, o_1, a_1, o_2, \ldots, a_{l-1}, o_{l}\}\). Note how the sequence starts with the pair \((a_0, o_1)\), which is different from the usual notation used in the PSR literature.

To construct a single core-test using composite GVFs a chain of myopic predictions much like the \(k\)-step forecasts can be used. The first GVF in the chain of GVFs will have a cumulant \(c(o_t, a_t, o_{t+1}) = \indicator(a_t = a_{l-1}, o_{t+1} = o_l)\), where \(\mathbb{1}\) is the indicator function. Given a history \(\Hist_t\), the expected target of this GVF will be \(v^l = \Expected[\indicator(a_t = a_{l-1}, o_{t+1} = o_{l}) \lvert \Hist_t] = \Prob(a_{l-1}, o_l | \Hist_t)\). Now if GVFs are composed together, where subsequent gvfs are chained according to the cumulant function \(c(o_t, a_t, o_{t+1}, \hat{v}_{l-i}) = \indicator(a_t = a_{l-i-1}, o_{t+1} = o_{l-i}) \text{ and }  \hat{v}^{l-i}_{t+1}\) (where the "and" represents a product). As the chain continues the final GVF should represent the likelihood \(\Prob(q | \Hist_t)\) which is the likelihood for a PSR test. I prove the two chain in the following lemma, but the more general case very easily extends from this case.

#+begin_corollary
Given a length 2 sequence of actions and observations \(\mathcal{S} = a_0, o_1, a_1, o_2\) and a history \(\Hist_t\), the probability of seeing the sequence \(\mathcal{S}\) given the history can be represented as two composed value function targets.
#+end_corollary
#+begin_proof

The above corollary can be easily proven through the rules of conditional expectation and probabilities. The probability of seeing a sequence \(\mathcal{S}\) given a history \(\Hist_t\) can be written as
\begin{align*}
\Prob(A_t = a_0, O_{t+1} = o_1, A_{t+1} = a_1, O_{t+2} = o_2 | \Hist_t) = \\ \hspace{2cm} \Expected[\indicator\{A_t = a_0, O_{t+1} = o_1, A_{t+1} = a_1, O_{t+2} = o_2 |\} \Hist_t]
\end{align*}

\noindent where \(\indicator\) is the indicator function. Now following the procedure laid out above to build composite GVFs. The first GVF in the chain will have a cumulant \(c_2(o_t, a_t, o_{t+1}) = \indicator\{A_t = a_{1}, O_{t+1} = o_{2}\}\) with a myopic discount (i.e. \(\gamma = 0\)). The resulting value function prediction is
\[
v_2(\Hist_t) = \Expected[\indicator(a_t = a_{1}, o_{t+1} = o_{2}) \lvert \Hist_t] = \Prob(A_t = a_{1}, O_{t+1} = o_{2} \lvert \Hist_t).
\]

The next value function in the chain uses the previous predictions value \(c_1(o_t, a_t, o_{t+1}, H_t) = \indicator\{A_t = a_{0}, O_{t+1} = o_{1} \hat{v}_2(\{\Hist_t A_t, O_{t+1}\}) \} \). The resulting value function will predict

\begin{align*}
v_1(\Hist_t) &= \Expected[\indicator(A_t = a_{0}, O_{t+1} = o_{1}) \Expected[\indicator(A_{t+1} = a_{1}, O_{t+2} = o_{2}) \lvert \Hist_t, A_t, O_{t+1}]  \lvert \Hist_t] \\
&= \sum \indicator(A_t = a_{0}, O_{t+1} = o_{1}) \Prob(A_t, O_{t+1} \lvert \Hist_t) \sum \indicator(A_{t+1} = a_{1}, O_{t+2} = o_{2}) \Prob(A_{t+1}, O_{t+2} \lvert \Hist_t, A_{t}, O_{t+1}) \\
&= \sum \sum \indicator(A_t = a_{0}, O_{t+1} = o_{1}) \indicator(A_{t+1} = a_{1}, O_{t+2} = o_{2}) \Prob(A_t, O_{t+1} \lvert \Hist_t) \Prob(A_{t+1}, O_{t+2} \lvert \Hist_t, A_{t}, O_{t+1}) \\
&= \sum \sum \indicator(A_t = a_{0}, O_{t+1} = o_{1}, A_{t+1} = a_{1}, O_{t+2} = o_{2}) \Prob(A_t, O_{t+1} \lvert \Hist_t) \frac{\Prob(A_{t}, O_{t+1}, A_{t+1}, O_{t+2} \lvert \Hist_t)}{\Prob(A_t, O_{t+1} \lvert \Hist_t)} \\
&= \sum \sum \indicator(A_t = a_{0}, O_{t+1} = o_{1}, A_{t+1} = a_{1}, O_{t+2} = o_{2}) \Prob(A_{t}, O_{t+1}, A_{t+1}, O_{t+2} \lvert \Hist_t) \\
&= \Prob(A_{t} = a_0, O_{t+1}=o_1, A_{t+1}=a_1, O_{t+2}=o_2 \lvert \Hist_t)
\end{align*}
#+end_proof

** Summary

This chapter presented the GVFN predictive representations of state and compares this approach to various types of representations of state including recurrent neural networks, predictive state representations, and TD Networks. This chapter concluded by providing some intuitive constructions of GVFNs corresponding to state in compass world, \(k\)-forecasts, and finally the core-tests of a PSR.

The aim of this chapter was to lay the ground work needed for the next sections, and to discuss the intuitive motivation behind the GVFN approach to state construction. As discussed, GVFNs lay squarely as a subjective state approach, only considering possible state variables which are constructed from general value functions. The motivation behind the GVFN work is extremely similar to that of TD networks and PSRs, except sufficiency is explicitly considered to be tied only to an agent's goals rather than all possible predictions provided by the agent. While this makes intuitive sense from the perspective of searching and learning the agent state, proving viability becomes intractable from these constraints [[cite:&subramanian2022approximate]]. The subsequent chapters will focus on the question of usefulness of such an approach through empirical means.

* Learning Algorithms for GVFNs
:PROPERTIES:
:CUSTOM_ID: chap:gvfn:algs
:END:


In this section, I introduce the objective function for GVFNs, that constrains the learned state to be GVF predictions.
Each state component of a GVFN is a value function prediction, and so is approximating the fixed point to a Bellman equation with history in Equation eqref:eq-bewh. The extension is not as simple as using a standard Bellman operator, however, because the GVFs are in a network. In fact, the Bellman equations are coupled in two ways: through composition---where one GVF can be the cumulant for another GVF as seen in section [[ref:sec:gvfn:emp:poorlyspecified]]---and through the parametric recurrent state representation. I first discuss the Bellman network operator in Section [[ref:sec:gvfn:operator]], which extends the typical Bellman operator to allow for composition. I then explain how the coupling that arises from the recurrent state representation can be handled using a projected operator, and provide the objective for GVFNs, called the /Mean-Squared Projected Bellman Network Error/ (MSPBNE), in Section [[ref:sec:gvfn:objective]]. Then I discuss several algorithms to optimize this objective in Section [[ref:sec:gvfn:algs]]. Finally, I also describe a simple procedure for using eligibility traces when using semi-gradient temporal-difference for GVFNs.

The GVFN objective can be mixed with the standard RNN objective, to provide an RNN where the learned states are both useful for prediction of the target and encouraged---or regularized---to be GVF predictions. In this work, GVFNs are only trained with the GVFN objective, without including the loss to a target, to focus the investigation on the utility of the proposed objective and on predictive features.

** The Bellman Network Operator
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:operator
:END:

To understand the Bellman network operator, it is useful to first revisit the Bellman operator for learning a single GVF.
The set of histories \(\Hist\) is assumed finite.
# \footnote{It is common to assume finite state spaces when analyzing value functions and defining Bellman operators. Extensions to infinite spaces is possible, but complicates presentation.}
Assume a tabular encoding for the values, \(\vi{j} \in \RR^{|\Hists|}\), for a GVF question \((\pij{j}, \cj{j}, \gammaj{j})\). The Bellman equation in [[eqref:eq-bewh]] can be written as a fixed point equation, with Bellman operator
{{{c}}}
\begin{equation}
\Bn^{(j)} \vi{j} \defeq \Cpij{j} + \Ppigammaj{j} \vi{j}
\end{equation}
{{{c}}}
where \(\Cpij{j} \in \RR^{|\Hists|}\) is the vector of expected cumulant values under \(\pij{j}\), with entries
{{{c}}}
\begin{equation}
\Cpij{j}(\hist_t) \defeq \sum_{\action_t \in \Actions} \pij{j}(\action_t | \hist_t) \sum_{\hist_{t+1} \in \Hists} \Pr(\hist_{t+1} | \hist_t, \action_t) \cj{j}(\hist_t, \action_t, \hist_{t+1})
.
\end{equation}
{{{c}}}
and
 \(\Ppigammaj{j} \in \RR^{|\Hists| \times |\Hists| }\) is the matrix of values satisfying
{{{c}}}
\begin{equation}
\Ppigammaj{j}(\hist_t, \hist_{t+1}) = \sum_{\action_t \in \Actions} \pij{j}(\action_t | \hist_t) \Pr(\hist_{t+1} | \hist_t, a_t) \gammaj{j}(\hist_t, \action_t, \hist_{t+1})
.
\end{equation}
{{{c}}}
If the operator \(\Bn^{(j)}\) is a contraction, then iteratively applying this operator converges to a fixed point. More precisely, if for any \(\vi{j}_1, \vi{j}_{2} \in \RR$, $\| \Bn^{(j)} \vi{j}_1 -  \Bn^{(j)}\vi{j}_2 \| <  \| \vi{j}_1 - \vi{j}_2 \|\), then iteratively applying \(\Bn^{(j)}\), as \(\vi{j}_2 = \Bn^{(j)} \vi{j}_1, \ldots, \vi{j}_{t+1} = \Bn^{(j)} \vi{j}_t, \ldots\), converges to a fixed point.  Because temporal difference learning algorithms are based on this fixed-point update, the Bellman operator is central to the analysis of many algorithms for learning value functions, and is used in the definition of objectives for value estimation.

Similarly, Bellman operator can be defined that accounts for the relationships between GVFs in the network. Assume there are \(\numgvfs\) GVFs, with \(\vinone \in \RR^{\numgvfs | \Hists |}\) the stacked values for all the GVFs,
{{{c}}}
\begin{equation}
\vinone \defeq \left[\begin{array}{c}
\vi{1}\\
\vdots \\
\vi{\numgvfs}
\end{array}
\right]
.
\end{equation}
{{{c}}}
The cumulants may now be functions of the values of other GVFs; therefore cumulants are explicitly written as \(\Cpij{j}_{\vinone}\).
The Bellman network operator \(\Bn\) is
{{{c}}}
\begin{equation}
\Bn \vinone
\defeq
\left[\begin{array}{c}
\Cpij{1}_{\vinone} + \Ppigammaj{1} \vi{1}\\
\vdots \\
\Cpij{\numgvfs}_{\vinone} + \Ppigammaj{\numgvfs} \vi{\numgvfs}
\end{array}
\right]
.
\end{equation}
{{{c}}}
The Bellman network operator needs to be treated as a joint operator on all the GVFs because of compositional predictions, where the prediction on the next step of GVF \(j\) is the cumulant for GVF \(i\). When iterating the Bellman operator \(\vi{j}\) is not only involved in its own Bellman equation, but also in the Bellman equation for \(\vi{i}\). Notice that if there were no compositions, the Bellman network operator would separate into individual Bellman operators, that operate on each \(\vi{j}\) independently.

To use such a Bellman network operator, it needs to be ensured that iterating under this operator converges to a fixed point. For no composition, this result is straightforward, as it simply follows from previous results showing when the Bellman operator is a contraction. This is stated explicitly below in Corollary ref:cor:gvfn:main. Under composition, the effect of the current value function on the cumulant needs to be considered. Consequently, the operator may no longer be a simple linear projection of the values, followed by a sum of expected cumulants.

First, identify a necessary condition: the connections between GVFs must be acyclic. For example, GVF \(i\) cannot be a cumulant for GVF \(j\), if \(j\) is already a cumulant for \(i\). More generally, the connections between GVFs cannot create a cycle, such as \(1 \rightarrow 2 \rightarrow 3 \rightarrow 1\). A counterexample is provided, where the Bellman network operator is not a contraction when there is a cycle, to illustrate that this condition is necessary.

Further restrictions are placed on the cumulant, if it is a function of other GVFs. In particular, it is required that the cumulant is a Lipschitz function of the other value functions.
Note that this restriction encompasses the setting for a non-compositional GVF, because the cumulant can be a constant w.r.t. these values. It also encompasses the setting used in the below experiments: that each cumulant is a linear function of the GVF values on the next step.

#+ATTR_LATEX: :options [Acyclic Connections]
#+ATTR_HTML: :title Acyclic Connections
#+begin_assumption
The directed graph \(G\) is acyclic. \(G\) consists of \(\numgvfs\) vertices, each corresponding to a GVF, and each directed edge \((i,j)\) indicates that \(j\) is used in the cumulant for \(i\).
#+end_assumption

#+ATTR_LATEX: :options [Lipschitz Compositional Cumulants]
#+ATTR_HTML: :title Lipschitz Compositional Cumulants
#+begin_assumption
If GVF \(i\) has directed edges to \(\{j_1, \ldots, j_k\}\), then the cumulant \(c^{(i)}_{\vinone}(\hist_{t+1})\) is Lipschitz in \(\vi{j_1}, \ldots, \vi{j_k}\) with Lipschitz constant \(K_i\). That is, for \(\vinone_1,\vinone_2 \in \RR^{\numgvfs | \Hists |}\), \(\| \Cpij{i}_{\vinone_1} - \Cpij{i}_{\vinone_2} \| \le K_i \sum_{l=1}^k \|  \vi{j_l}_1 - \vi{j_l}_2 \|\).
#+end_assumption

Note that this assumption is satisfied if when assuming that for some bounded weights \(w_1, \ldots, w_k \in \RR\), the cumulant must satisfy \(c^{(i)}_{\vinone}(\hist_{t+1}) = \sum_{l=1}^k w_l \vi{j_l}(\hist_{t+1})\) or equivalently, \(\Cpij{i}_{\vinone} = \sum_{l=1}^k w_l \Ppigammaj{j_l} \vi{j_l}\).  This is because \(\Ppigammaj{j_l}\) is a non-expansion, and so
\begin{align*}
\| \Cpij{i}_{\vinone_1} - \Cpij{i}_{\vinone_2} \|
= \left\| \sum_{l=1}^k w_l \Ppigammaj{j_l} (\vi{j_l}_1 - \vi{j_l}_2) \right\|
 &\le \sum_{l=1}^k | w_l | \| \Ppigammaj{j_l} (\vi{j_l}_1 - \vi{j_l}_2) \|\\
& \le (\max_{l} | w_l | ) \sum_{l=1}^k \| \vi{j_l}_1 - \vi{j_l}_2 \|
.
\end{align*}

The third assumption is standard for showing Bellman operators are contractions, and is easily satisfied if the policy is proper: is guaranteed to visit at least one state where the continuation is less than 1.

#+ATTR_LATEX: :options [Discounted Transitions are Contractions]
#+ATTR_HTML: :title Discounted Transitions are Contractions
#+begin_assumption
For all \(j \in \{1, \ldots, \numgvfs\}\), \(\beta_j \defeq \| \Ppigammaj{j} \| < 1\), where \(\| \cdot \|\) is the spectral norm.
#+end_assumption
{{{c}}}
With these three assumptions, the main result can be proven.
{{{c}}}

#+name thm_main
#+begin_theorem
Under Assumptions 1-3, iterating \(\vt{t+1} = \Bn \vt{t}\) converges to a unique fixed point.
#+end_theorem

#+begin_proof
First, the proof that the sequence of value estimates converges (Part 1) is detailed, then follows a proof that it converges to a unique fixed point (Part 2 and 3).

\noindent
\textbf{Part 1:} \textit{The sequence $\vinone_{1}, \vinone_{2}, \ldots$ defined by $\vinone_{t+1} = \Bn \vinone_t$ converges to a limit $\vinone^* \in \RR^{\numgvfs|\Hists|}$.}

Because \(G\) is acyclic, there is a linear topological ordering of the vertices, \(i_1, \ldots, i_\numgvfs\): for each directed edge \((i,j)\), \(i\) comes before \(j\) in the ordering. Therefore, starting from the last GVF \(j = i_\numgvfs\), remember that the Bellman operator \(\Bn^{(j)}\) is a contraction with rate \(\beta_{j} < 1\),
{{{c}}}
\begin{equation*}
\| \Bn^{(j)} \vit{j}{1} - \Bn^{(j)} \vit{j}{0} \| = \| \Ppigammaj{j} \vit{j}{1} - \Ppigammaj{j}  \vit{j}{0} \| \le \beta_j\| \vit{j}{1} - \vit{j}{0} \|
.
\end{equation*}
{{{c}}}
Therefore, iterating \(\Bn\) for \(t\) steps results in the error
{{{c}}}
\begin{equation*}
\| \vit{j}{t+1} - \vit{j}{t} \| \le \beta_j^t \| \vit{j}{1} - \vit{j}{0} \|
\end{equation*}
{{{c}}}
and as \(t \rightarrow \infty\), \(\vit{j}{t}\) converges to its fixed point.

Induction is used for the argument, with the above as the base case.
Assume for all \(j \in \{i_k, \ldots, i_{\numgvfs}\}\) there exists a ball of radius \(\epsilon(t)\) where \(\| \vit{j}{t+1} - \vit{j}{t} \| \le \epsilon(t)\) and \(\epsilon(t) \rightarrow 0\) as \(t \rightarrow \infty\).
Consider the next GVF in the ordering, \(i = i_{k-1}\).

\textbf{Case 1: } There are no outgoing edges from \(i\). If \(i\) does not use another GVF \(j\) in its cumulant, then iterating with \(\Bn\) independently iterates \(\vit{i}{t}\) with \(\Bn^{(i)}\). Therefore, as above, \(\vit{i}{t}\) converges because the Bellman operator is a contraction. In this setting, clearly such an \(\epsilon_i(t)\) exists because \(\| \vit{j}{t+1} - \vit{j}{t} \| \rightarrow 0\) as \(t \rightarrow \infty\).

\textbf{Case 2: } The cumulant for GVF \(i\) is composed of the values for the set of GVFs \(\mathcal{J} \subseteq  \{i_k, \ldots, i_{\numgvfs}\}\). The basic idea, formalized below, is that GVF \(i\) will be guaranteed to converge once the GVFs used to construct the become sufficiently accurate. The update is \(\vit{i}{t+1} =  \Cpij{i}_{\vinone_t}  + \Ppigammaj{i} \vit{i}{t}\). The change in \(\vit{i}{t}\) is
{{{c}}}
\begin{align*}
\| \vit{i}{t+1} - \vit{i}{t} \|
&=  \| (\Cpij{i}_{\vinone_t} - \Cpij{i}_{\vinone_{t-1}})+ \Ppigammaj{i} ( \vit{i}{t} - \vit{i}{t-1}) \|\\
&\le K_i \sum_{j \in \mathcal{J}} \| \vit{j}{t} - \vit{j}{t-1}\| + \beta_i \| \vit{i}{t} - \vit{i}{t-1} \|\\
&\le  \numgvfs K_i  \epsilon(t-1) + \beta_i \| \vit{i}{t} - \vit{i}{t-1} \|
.
\end{align*}
{{{c}}}
In the first inequality, the first term is due to Lipschitz continuity of the cumulant and the second term is due to the fact that \(\| \Ppigammaj{i}  \| = \beta_i\). In the second inequality, \(\| \vit{j}{t} - \vit{j}{t-1}\| \le \epsilon_j(t)\), under the inductive hypothesis. The second inequality is loose, as the sum only involves \(|\mathcal{J}| < \numgvfs\) terms, but \(\numgvfs\) is used for simplicity since the results goes through with this constant as well. For sufficiently large \(t\), \(\epsilon(t-1)\) can be made arbitrarily small. If \(\numgvfs K_i \epsilon(t-1) < (1-\beta_i) \| \vit{i}{t} - \vit{i}{t-1} \|\), i.e., \(\epsilon(t-1) < \tfrac{(1-\beta_i)}{\numgvfs K_i} \| \vit{i}{t} - \vit{i}{t-1} \|\) then
{{{c}}}
\begin{align*}
\| \vit{i}{t+1} - \vit{i}{t} \|
&\le \tilde{\beta}_i \| \vit{i}{t} - \vit{i}{t-1} \| \hspace{1.0cm}\text{for some $\tilde{\beta}_i < 1$}
\end{align*}
{{{c}}}
and so the iteration is a contraction on step \(t\).
Else, if \(\epsilon(t-1) \ge \tfrac{(1-\beta_i)}{\numgvfs K_i} \| \vit{i}{t} - \vit{i}{t-1} \|\), then this implies the difference \(\| \vit{i}{t+1} - \vit{i}{t} \|\) is already within a small ball, with radius \(\numgvfs K_i \epsilon(t-1)/(1-\beta_i)\).  As \(t \rightarrow \infty\), the difference can oscillate between being within this ball, which shrinks to zero because \(\epsilon(t)\) shrinks to zero, or being iterated with a contraction that also shrinks the difference. In either case, there exists an \(\epsilon_i(t)\) such that  \(\| \vit{i}{t+1} - \vit{i}{t} \| \le \epsilon_i(t)\), where \(\epsilon_i(t) \rightarrow 0\) as \(t \rightarrow \infty\).

By induction, there exists such a \(\epsilon_i\) for all GVFs in the network. Therefore, the sequence \(\vit{i}{t}\) converges.

\noindent
\textbf{Part 2:} \textit{$\vinone^*$ is a fixed point of $\Bn$.}

Because the Bellman network operator is continuous, the limit can be taken inside the operator
{{{c}}}
\begin{equation*}
\vinone^* = \lim_{t \rightarrow \infty} \vt{t}
= \lim_{t \rightarrow \infty} \Bn\vt{t-1}
= \Bn \left(\lim_{t \rightarrow \infty} \vt{t-1}\right) = \Bn \vinone^*
\end{equation*}

\noindent
\textbf{Part 3: } \textit{\(\vinone^*\) is the only fixed point of $\Bn$.}

Consider an alternative solution \(\vinone\). Because of the uniqueness of fixed points under Bellman operators, all those GVFs that have non-compositional cumulants have unique fixed points and so those components in \(\vinone\) must be the same as \(\vinone^*\). All the GVFs next in the ordering that use those GVFs as cumulants must then also converge to a unique value, because their Bellman operators with fixed GVFs as cumulants have a unique fixed point. This argument continues for the remaining GVFs in the ordering.
#+end_proof

#+name: cor:gvfn:main
#+begin_corollary
Under Assumption 3 with non-compositional cumulants (no edges in \(G\)), iterating \(\vt{t+1} = \Bn \vt{t}\) converges to a unique fixed point.
#+end_corollary

#+ATTR_LATEX: :options [Necessity of Acyclic Composition]
#+ATTR_HTML: :title Necessity of Acyclic Composition
#+begin_proposition
There exists transition function \(\Pfcn: \EnvStates \times \Actions \times \EnvStates \rightarrow [0,1]\) and policy \(\pi: \EnvStates \times \Actions \rightarrow [0,1]\) such that, for two GVFs in a cycle, iteration with the Bellman network operator diverges.
#+end_proposition

#+begin_proof
Assume there are two states, with the policy defined such that the Markov chain has the following dynamics
\begin{equation}
\Ppi =
\left[\begin{array}{cc}
0.9 & 0.1\\
0.1 & 0.9
\end{array}
\right]
.
\end{equation}
{{{c}}}
Assume further that \(\gamma = 0.95\). The resulting Bellman iteration is
{{{c}}}
\begin{align*}
\twovec{\vi{1}}{\vi{2}}
&= \Ppi \twovec{\vi{2}}{\vi{1}} + \gamma  \Ppi \twovec{\vi{1}}{\vi{2}} \\
&= \Ppi \left[\begin{array}{cc}
0 & 1\\
1 & 0
\end{array}
\right] \twovec{\vi{1}}{\vi{2}} + \Ppi \left[\begin{array}{cc}
\gamma & 0\\
0 & \gamma
\end{array}
\right] \twovec{\vi{1}}{\vi{2}} \\
&= \Ppi \left[\begin{array}{cc}
\gamma & 1\\
1 & \gamma
\end{array}
\right] \twovec{\vi{1}}{\vi{2}}
\end{align*}
{{{c}}}
Since the matrix \(\Ppi \left[\begin{array}{cc}
\gamma & 1\\
1 & \gamma
\end{array}
\right] \)
is an expansion, for many initial \(\twovec{\vi{1}}{\vi{2}}\) this iteration goes to infinity, such as initial \(\vi{1} = \vi{2} = \twovec{1}{1}\).
#+end_proof

** The Objective Function for GVFNs
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:objective
:END:


With a valid Bellman network operator, the objective function for GVFNs can be defined. The above fixed point equation assumes a tabular setting, where the values can be estimated directly for each history. GVFNs, however, have a restricted functional form, where the value estimates must be a parametrized function of the current observation and value predictions from the last time step. Under such a functional form, it is unlikely that the fixed point can be exactly solved for. Rather, a projected fixed point will be used, which projects into the space of representable value functions.

Define the space of functions as
{{{c}}}
\begin{align}
\mathcal{F} = \Big\{ &\vinone_\weights = [\vi{1}_\weights, \ldots, \vi{\numgvfs}_\weights] \in \RR^{\numgvfs|\Hists|}  \ \ | \ \ \text{ where } \weights \in \weightspace \ \ \text{ and } \\
&V_\weights(\hist_{t+1}) = f_\weights([\viweights{1}(\hist_t), \ldots, \viweights{\numgvfs}(\hist_t)], \xvec_{t+1})
\ \ \text{ when } \text{Pr}(\hist_{t+1} | \hist_t, \xvec_{t+1}) > 0 \Big\} \nonumber
\end{align}
{{{c}}}
Recall that \(\xvec_{t+1} = [\action_t, \obs_{t+1}]\). \(\text{Pr}(\hist_{t+1} | \hist_t, \xvec_{t+1}) > 0\) only when \(\hist_{t+1} \equiv \hist_t a_t \obs_{t+1}\), and so expect this to only be true for one outcome \(\hist_{t+1}\). And write that \(\hist_{t+1}\) is equivalent, rather than equal, to the current history appended with action \(\action_t\) and observation \(\obs_{t+1}\), because \(\hist_{t+1}\) might be shorter (more minimal): earlier actions and observations might not be needed.
Define the projection operator
{{{c}}}
\begin{align}
\Pi_{\mathcal{F}}(\vinone) &\defeq \min_{\hat{\vinone} \in \mathcal{F}} \| \vinone - \hat{\vinone} \|_{\dw}^2
\hspace{0.5cm}\text{ where } \| \vinone - \hat{\vinone} \|_{\dw}^2 \defeq \sum_{\hist \in \Hist} \dw(\hist) (V(\hist) - \hat{V}(\hist))^2
\end{align}
{{{c}}}
{{{c}}}
where \(\dw: \Hists \rightarrow [0,1]\) is the sampling distribution over histories. Typically, data is assumed to be generated by following a behavior policy \(\mu: \Hists \rightarrow [0,1]\), and that \(\dw\) is the stationary distribution for this policy. The value functions for policies \(\pi_i\) are typically learned off-policy, since in general \(\pi_i\) will not equal \(\mu\). The behavior policy \(\mu\) used to gather the data is different, or off of, the policy---or policies---that are being evaluated.

To obtain the projected fixed point solution, a natural goal is to minimize the following projected objective,
{{{c}}}
\begin{equation}
\min_{\weights \in \weightspace} \| \Pi_{\mathcal{F}} \Bn \vinone_\weights - \vinone_\weights \|_{\dw}^2
\end{equation}
{{{c}}}
Unfortunately, this objective can be hard to compute, because the projection operator \(\Pi_{\mathcal{F}}\) onto the nonlinear manifold can be intractable. Instead, the same approach as [[citeauthor:&maei2010offpolicy]] [[citeyear:&maei2010offpolicy]] is taken, when defining the nonlinear MSPBE for learning value functions with neural networks and other nonlinear function approximators. The idea is to approximate the projection onto the nonlinear manifold by assuming it is locally linear. Then, a linear projection operator can be used, defined locally at the current set of parameters \(\weights \in \weightspace\), spanned by the basis \(\phivec_{j,\weights}(\hist) \defeq \nabla_\theta \vi{j}_\weights(\hist)\) for all \(\hist \in \Hists\) and GVFs \(j\). Let \(\phimat_{j,\weights}\) correspond to the matrix of stacked \(\phivec_{j,\weights}(\hist)^\trans\) for all \(\hist \in \Hists\), having \(|\Hists|\) rows. Define
{{{c}}}
\begin{align*}
  \phimat_{\weights}
  \defeq
  \left[\begin{array}{c}
          \phimat_{1, \weights}\\
          \vdots \\
          \phimat_{\numgvfs, \weights}
        \end{array}\right]
  \quad
  \quad
  \quad
  \dwdiag \defeq \diag\left[
  \begin{array}{c}
    \dw \\
    \vdots \\
    \dw
  \end{array}
  \right]
  \quad
  \quad
  \quad
  \Pi_{\weights}
  \defeq
    \phimat_{\weights}
    (\phimat_{\weights}^\trans \dwdiag \phimat_{\weights})^\inv
    \phimat_{\weights}^\trans \dwdiag
    .
\end{align*}
{{{c}}}
{{{c}}}
Using this locally linear approximation to the objective potentially expands the set of stationary points. The fixed points under the original projection are still fixed points under this locally linear approximation. But, there could be points that are fixed points under this locally linear approximation, that would not be under the original.

The final objective using this projection is called the MSPBNE[fn:: A variant of the MSPBNE has been introduced for TD networks cite:&silver2013gradient; the above generalizes that MSPBNE to GVF Networks. Because it is a strict generalization, thus the same name is used.], defined as
{{{c}}}
\begin{align}
    \text{MSPBNE}(\weights) &\defeq \| \Pi_{\weights} \Bn \vinone_\weights - \vinone_\weights \|_{\dw}^2 \label{eq_projform}
   \end{align}
{{{c}}}


In the following lemma it is shown, with proof, that in can be rewritten in a way that makes it more amenable to compute and sample gradients.[fn:: Since developing the MSPBNE, an alternative approach to defining a nonlinear MSPBE has been developed using a conjugate form for the Bellman error (see citeauthor:&dai2017learning (citeyear:&dai2017learning) and in-preparation work that makes the connection the MSPBE more explicit cite:&patterson2022generalized). The extension here should be relatively straightforward, as the objective is formulated using histories.] This reformulation is used to develop algorithms to minimize this objective in the next section.

#+name: lemma:gvfn:mspbne-exp
#+begin_lemma
{{{c}}}
The MSPBNE defined in Equation \eqref{eq_projform} can be rewritten as
{{{c}}}
\begin{align}
\text{MSPBNE}(\weights) &= \boldsymbol{\delta}(\weights)^\top \Lambda(\weights)^\inv  \boldsymbol{\delta}(\weights) \label{eq_mspbne}
\end{align}
{{{c}}}
where
{{{c}}}
\begin{align}
 \Lambda(\weights) &\defeq
       \Expected_d\bigg[\sum_{j=1}^\numgvfs \phivec_{j,\weights}(H) \phivec_{j,\weights}(H)^\trans \bigg]
       = \sum_{\hist \in \Hists} d(\hist) \sum_{j=1}^\numgvfs \phivec_{j,\weights}(\hist) \phivec_{j,\weights}(\hist)^\trans \label{eqn:gvfn:w}\\
     \boldsymbol{\delta}(\weights) &\defeq
     \sum_{j=1}^\numgvfs \Expected_{d,\pi_j}\bigg[\tderror_j(H, A, H') \phivec_{j,\weights}(H) \bigg] \nonumber\\
     \tderror_j(H,A,H') &\defeq c^{(j)}(H, A, H') + \gamma^{(j)}(H, A, H')\viweights{j}(H') - \viweights{j}(H) \nonumber
     .
\end{align}
#+end_lemma


#+begin_proof

Starting with equation \eqref{eq_projform} and for $\Delta_{\weights} \defeq \Bn \vinone_\weights - \vinone_\weights$,
\begin{align*}
     \text{MSPBNE}(\weights)
     & = \| \Pi_{\weights} \Bn \vinone_\weights - \vinone_\weights \|_{\dw}^2\\
     & = \| \Pi_{\weights} \left[\Bn\vinone_\weights - \vinone_\weights \right] \|_{\dw}^2\\
     & = \| \Pi_{\weights}\Delta_{\weights} \|_{\dw}^2
\end{align*}
The projection operator can be wrapped around the full TD error $\Delta_{\weights}$, because it has no affect on $\vinone_\weights$ which is already in the space. Then plugging in the definition of $\Pi_\weights$
\begin{align}
     \Pi_\weights^\top \dwdiag \Pi_\weights
         &= \dwdiag^\top \phimat_\weights (\phimat_\weights^\top \dwdiag \phimat_\weights)^\inv \phimat^\top \dwdiag \nonumber \\
     \| \Pi_{\weights}\Delta_{\weights} \|_{\dw}^2
         &= \Delta_\weights^\top \Pi_\weights^\top \dwdiag \Pi_\weights \Delta_\weights \nonumber \\
    &= \Delta_\weights^\top \dwdiag^\top \phimat_\weights (\phimat_\weights^\top \dwdiag \phimat_\weights)^\inv \phimat^\top \dwdiag \Delta_\weights \label{mspbne_mat}
\end{align}
As in prior gradient TD work the matrix operations are converted to expectation forms.
\begin{align*}
     \phimat_\weights^\top \dwdiag \phimat_\weights &= \sum_{j=1}^n \sum_{\hist\in\Hists} \dw(\hist) \phivec_{j,\weights}(\hist) \phi_{j, \weights}(\hist)^\top = \Expected_d\left[\sum_{j=1}^n \phivec_{j,\weights}(H)\phivec_{j,\weights}(H)^\top\right]\\
     &= W(\weights)\\
     \phimat_\weights^\top \dwdiag \Delta_\weights &= \sum_{j=1}^n \sum_{\hist\in\Hists} \dw(\hist) \phivec_{j,\theta}(\hist) \sum_{a\in\Actions} \pi_j(a|\hist) \Expected[\delta_j(\hist,a,H')] = \sum_{j=1}^n\Expected_{d,\pi_j}\left[\delta_j(H,A,H')\phivec_{j,\weights}(H)\right]\\
     &=  \boldsymbol{\delta}(\weights)
\end{align*}
Then substituting into equation \eqref{mspbne_mat}, getting the result $\text{MSPBNE}(\weights) = \boldsymbol{\delta}(\weights)^\top \Lambda(\weights)^\inv \boldsymbol{\delta}(\weights)$.
#+end_proof

Samples are not received according to $\pi_j$; instead, they are sampled according to the behavior $\mu$. Throughout this work, it has been assumed $\mu$ has a coverage property. This means that the behaviour policy $\mu$ satisfies $\mu(a | \hist) > 0$ if any $\pi_j(a | \hist) > 0$ for policies $\pi_1, \ldots, \pi_\numgvfs$. 

#+name: col:gvfn:mspbne-is
#+begin_corollary
For importance sampling ratios $\rho_j(a | \hist) \defeq \frac{\pi_j(a | \hist)}{\mu(a | \hist)}$ and
  \begin{align*}
    \boldsymbol{\delta}_\mu(\weights) &\defeq \Expected_{d, \mu}\bigg[\sum_{j=1}^\numgvfs \rho_j(H,A) \tderror_j(H,A,H') \phivec_{j,\weights}(H) \bigg]\\
         &= \sum_{\hist \in \Hists} d(\hist) \sum_{a \in \Actions} \mu(a|\hist) \sum_{j=1}^\numgvfs  \rho_j (a | \hist) \Expected\bigg[\tderror_j(H,A,H') \phivec_{j,\weights}(\hist) | H = \hist, A = a \bigg] 
  \end{align*} 
  then it can be shown that $\boldsymbol{\delta}_\mu(\weights) = \boldsymbol{\delta}(\weights)$ and so
   \begin{align*}
    \text{MSPBNE}(\weights) &= \boldsymbol{\delta}_\mu(\weights)^\top \Lambda(\weights)^\inv  \boldsymbol{\delta}_\mu(\weights)
   \end{align*}
#+end_corollary

#+begin_proof
The key is simply to show that $\boldsymbol{\delta}_\mu(\weights) = \boldsymbol{\delta}(\weights)$, because $W(\weights)$ depends only on $d$, not on the policies $\pi$ or $\mu$. This is straightforward with the typical cancellation in importance sampling ratios
  \begin{align*}
   \boldsymbol{\delta}_\mu(\weights)
   &= \sum_{\hist \in \Hists} d(\hist) \sum_{a \in \Actions} \mu(a|\hist) \sum_{j=1}^\numgvfs  \rho_j (a | \hist) \Expected\bigg[\tderror_j(\hist, a, H') \phivec_{j,\weights}(h) | H = h, A = a \bigg] \\
     &= \sum_{\hist \in \Hists} d(\hist)  \sum_{j=1}^\numgvfs  \sum_{a \in \Actions} \mu(a|\hist) \rho_j (a | \hist) \Expected\bigg[\tderror_j(\hist, a, H') \phivec_{j,\weights}(h) | H = h, A = a \bigg] \\ 
     &= \sum_{\hist \in \Hists} d(\hist)  \sum_{j=1}^\numgvfs  \sum_{a \in \Actions} \pi_j (a | \hist) \Expected\bigg[\tderror_j(\hist, a, H') \phivec_{j,\weights}(h) | H = h, A = a \bigg] \\ 
&= \boldsymbol{\delta}(\weights)         .
  \end{align*}
#+end_proof
From here on, therefore, it is assumed that $\boldsymbol{\delta}(\weights)$ is defined more generally as the above $\boldsymbol{\delta}_\mu(\weights)$, since they result in the same objective but this more general expression more obviously highlights off-policy sampling. 

From this reformulation, one can see that the MSPBNE objective is a weighted quadratic objective, with weighting matrix \(W(\weights)\) on vector \(\boldsymbol{\delta}(\weights)\). The objective is zero---and so minimal---when \(\boldsymbol{\delta}(\weights) = \zerovec\). This is similar to the temporal difference (TD) learning fixed point criteria. In fact, TD implicitly optimizes the linear MSPBE, which corresponds to the above objective with \(\numgvfs = 1\) and fixed features that do not depend on the parameters. Once we have a projected Bellman error objective, we can take advantage of the many advances in formulating TD algorithms to optimized MSPBE objectives. Therefore, though this objective looks quite complex, there is substantial literature to facilitate minimizing the MSPBNE.

** Algorithms for the MSPBNE
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:algs
:END:

The algorithms to optimize the MSPBNE are a relatively straightforward combination of standard algorithms for RNNs and the TD algorithms designed to optimize the MSPBE. To provide some intuition on these algorithms, and how to obtain this combination of TD and RNN algorithms, I begin with a simpler setting: extending TD to a recurrent setting,  with one GVF. From there, two algorithms for the MSPBNE are introduced: Recurrent TD and Recurrent GTD.

Consider first the on-policy TD update, without recurrence, assuming the true state \(\svec_t\) at time t is given:
{{{c}}}
\begin{align*}
\weights_{t+1} \gets \weights_t + \alpha_t \delta_t \nabla_\weights V_\weights(\svec_t) \hspace{0.5cm} \text{ where } \delta_t \defeq C_{t+1} + \gamma_{t+1} V_\weights(\svec_{t+1}) - V_\weights(\svec_{t})
.
\end{align*}
{{{c}}}
With recurrence, where the state is estimated and so is a function of \(\weights\), the only difference to this update is in the computation of \(\nabla_\weights V_\weights(\svec_t)\), where \(\svec_t\) should instead be thought of \(\svec_t(\weights)\). This gradient now requires the chain rule, to account for the impact of \(\weights\) on the last state, and the state before then, and so on:
{{{c}}}
\begin{equation*}
\frac{\partial V_\weights(\svec_t)}{\partial \weights_i} = \frac{\partial V_\weights(\svec_t)}{\partial \svec_t}^\top \frac{\partial \svec_t}{\partial \weights_i}
\end{equation*}
{{{c}}}
where \(\svec_t = f_\weights(\svec_{t-1}, \xvec_t)\).
{{{c}}}
Computing this gradient back-in-time, \(\nabla_\weights \svec_t\)---which is also called the \emph{sensitivity}---is precisely the aim of most RNN algorithms, including truncated BPTT and RTRL. Any algorithm that computes sensitivities can be used to obtain a TD update with recurrent connections to estimate the state.

For GVFNs, there are two differences: one needs to account for off-policy sampling and the fact that state is itself composed of these value estimates, rather than being learned to estimate values. Value estimation within GVFNs requires off-policy updates, because the target policies \(\pi_j\) are not typically equal to the behavior policy \(\mu\). Therefore, importance sampling ratios must be included in the update
\begin{align*}
\rho_{t,j} \defeq \frac{\pi_j(A_t | \hist_t)}{\mu(A_t | \hist_t)} \ \ \ \ \text{ for all $j \in \{1, 2, \ldots, \numgvfs\}$}
.
\end{align*}
{{{c}}}
This ratio multiplies the TD update, to adjust the expectation of the update to be as if action \(A_t\) had been taken under \(\pi_j\) rather than the behavior \(\mu\). For the second difference, the Recurrent TD update is actually even simpler because the value function itself is the state. For the \(j\)-th value function---which is the \(j\)-th state variable---\(\nabla_\weights \vifunc{j}_\weights\) at time \(t\) is \(\nabla_\weights \svec_{t,j}\). Notice that this gradient actually corresponds to using the above chain rule update, by using \(\vifunc{j}(\svec_t) = \svec_{t,j}\) as a selector function into the state variable.

The *Recurrent TD* update for GVFNs is
{{{c}}}
\begin{align}
\svec_t &\gets f_{\weights_t}(\svec_{t-1}, \xvec_t) &&\triangleright \text{ where } \xvec_t \defeq [a_{t-1}, \obs_t] \nonumber\\
\svec_{t+1} &\gets f_{\weights_t}(\svec_{t}, \xvec_{t+1}) &&\triangleright \text{ where } \xvec_{t+1} \defeq [a_{t}, \obs_{t+1}]\nonumber\\
\phivec_{t,j} &\gets \nabla_\weights \svec_{t,j} &&\triangleright \text{ Compute sensitivities using truncated BPTT} \nonumber\\
\delta_{t,j} &\gets C_{t+1}^{(j)} + \gamma_{t+1}^{(j)} \svec_{t+1,j} - \svec_{t,j}   \nonumber\\
\rho_{t,j} &\gets \frac{\pi^{(j)}(a_t | \obs_t)}{\mu(a_t | \obs_t)} &&\triangleright \text{ Policies can be functions of histories, not just of $\obs_t$}  \nonumber\\
\weights_{t+1} &\gets \weights_{t} + \alpha_t \bigg[ \sum_{j=1}^{\numgvfs} \rho_{t,j}\tderror_{t,j} \phivec_{t,j}  \bigg] \label{eq_rtd}
\end{align}
{{{c}}}

The TD update, however, is only an approximate semi-gradient update, even in the fully observable setting. To obtain exact gradient formulas, I turn to Gradient TD (GTD) algorithms. In particular, the nonlinear GTD strategy developed by [[cite:&maei2009convergent]] to the MSPBNE. As above, any algorithm to compute the sensitivities can be used in the Recurrent GTD algorithm. But, the algorithm becomes more complex, simply because nonlinear GTD is more complex than TD even without recurrence.

The following theorem is used to facilitate estimating the gradient. The main idea is to introduce an auxiliary weight vector, \(\secweights\), to provide a quasi-stationary estimate of part of the objective. This proof and explicit derivation for the resulting Recurrent TD algorithm is given in the Section ref:sec:gvfn:fullrgtd. As a warm up, the result for non-compositional GVFs is derived: no GVFs predict the outcomes of other GVFs. This makes the algorithm easier to follow. The more general proof and derivation is contained in Section ref:sec:gvfn:fullrgtd.
{{{c}}}
#+name: thm:gradients
#+begin_theorem
Assume that \(V_{\weights}(\hist)\) is twice continuously differentiable as a function of \(\weights\) for all histories \(\hist\in\Hist\) where \(\dw(\hist)>0\) and that \(W(\cdot)\), defined in Equation eqref:eqn:gvfn:w, is non-singular in a small neighbourhood of \(\weights\). Assume further that there are no compositional GVFs in the GVFN: no GVFs has a cumulant that corresponds to another GVFs prediction. Then for \(W(\weights)\) and \(\boldsymbol{\delta}(\weights) \) defined in Lemma ref:lemma:gvfn:mspbne-exp,
\begin{align}
\secweights(\weights) &\defeq
    \Lambda(\weights)^\inv \boldsymbol{\delta}(\weights) \label{eqn:gvfn:secondw} \\
\hat{\delta}_{j,\theta}(H) &\defeq \phivec_{j,\weights}(H)^\trans \secweights(\weights) \nonumber\\
    \psivec(\weights) &\defeq \Expected_{d, \mu}\left[\sum\limits_{j=1}^{\numgvfs} \rho_j(H,A)\Big(\delta_j(H,A,H') - \hat{\delta}_{j,\theta}(H)\Big)  \nabla^2 \viweights{j}(H)  \secweights(\weights) \right] \label{eqn:gvfn:hv}
\end{align}
getting the gradient
\begin{align}
   -\tfrac{1}{2} \nabla  \text{MSPBNE}(\weights) &=
       \boldsymbol{\delta}(\weights) -
       \Expected_{d,\mu}\bigg[\rho_j(H,A)\gamma^{(j)}(H,A,H') \hat{\delta}_{j,\theta}(H) \phivec_{j,\weights}(H') \bigg] - \psivec(\weights) \label{eqn:gvfn:tdc}
\end{align}
#+end_theorem
{{{c}}}
Now two additional terms are needed to estimate beyond the standard sensitivities in a typical RNN gradient. First, the additional weight vector \(\secweights\) is estimated via Equation eqref:eqn:gvfn:secondw. This can be done using standard techniques in reinforcement learning. Second, a Hessian-vector product must be estimated. The Hessian-vector product is given in Equation eqref:eqn:gvfn:hv. Fortunately, this can be computed using R-operators, without explicitly computing the Hessian-vector product, using only computation linear in the length of the vector.


The *Recurrent GTD* update, for this simpler setting without composition, is[fn:: As mentioned above, one could have considered an alternative MSPBNE, using an recently published nonlinear MSPBE objective cite:&patterson2022generalized. The resulting Recurrent GTD algorithm would look very similar, except the Hessian-vector product could be omitted: \(\psivec_t\) is simply dropped in the update to \(\theta\).]
{{{c}}}
\begin{align}
\state_t &\gets f_{\weights_t}(\svec_{t-1}, \xvec_t) \nonumber\\
\state_{t+1} &\gets f_{\weights_t}(\svec_{t}, \xvec_{t+1}) \nonumber\\
\phivec_{t,j} &\gets \nabla_\weights \svec_{t,j} \hspace{2.0cm} \triangleright \text{ Compute sensitivities using truncated BPTT}  \nonumber\\
\phivec'_{t,j} &\gets \nabla_\weights \svec_{t+1,j}  \nonumber\\
\rho_{t,j} &\gets \frac{\pi^{(j)}(a_t | \obs_t)}{\mu(a_t | \obs_t)}  \nonumber\\
\vvec_t &\gets \nabla^2 \svec_t \secweights_t \hspace{2.0cm} \triangleright \text{ Computed using R-operators, see Appendix \ref{sec:gvfn:gradbtt}} \nonumber\\
\hat{\delta}_{t,j} &\gets \phivec_{t,j}^\trans  \secweights_t \nonumber\\
  \psivec_t &\gets \sum_{j=1}^{\numgvfs} ( \rho_{t,j}\delta_{t,j} - \hat{\delta}_{t,j})  \vvec_t \nonumber\\
  \weights_{t+1} &\gets \weights_{t} + \alpha_t \bigg[ \sum_{j=1}^{\numgvfs}  \rho_{t,j} \tderror_{t,j} \phivec_{t,j} - \rho_{t,j}  \gamma_{j,t+1} \hat{\delta}_{t,j} \phivec'_{t,j} \bigg] - \alpha_t\psivec_t  \label{eq_rgtd}\\
   \secweights_{t+1} &\gets \secweights_t + \beta_t \bigg[ \sum_{j=1}^{\numgvfs}  \rho_{t,j} (\tderror_{t,j} - \hat{\delta}_{t,j} )\phivec_{t,j} \bigg] \nonumber
\end{align}
{{{c}}}
The derivation for this algorithm is similar to the derivation for Gradient TD Networks cite:&silver2013gradient, though for this more general setting with GVF Networks.

As alluded to, there are a variety of possible strategies to optimize the MSPBNE for GVFNs. This variety arises from different strategies to optimize RNNs, back-in-time, as well as from the variety of strategies to optimize the MSPBE for value estimation. For example, sensitivities can be computed using truncated BPTT or RTRL and its many approximations. Similarly, for the MSPBE, there are a variety of different strategies to approximate gradients, because the gradient is not straightforward to sample. These including a variety of gradient TD methods---such as GTD and GTD2---saddlepoint methods and semi-gradient TD (see citeauthor:&ghiassian2018online (citeyear:&ghiassian2018online) for a more exhaustive list).

** Deriving the Full Recurrent GTD Update
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:fullrgtd
:END:

Now that the objective is written in its expectation form, the gradients can be take with respect to the weight parameter. The main body stated the result for a simplified setting (Theorem ref:thm:gradients), to make it simpler to understand the result. The more general result, for compositional GVFs, is presented in this section.

# \begin{restatable}{theorem}{gradtheoremgen}\label{thm:gradientsgen}
#+name: thm:gvfn:gradientsgen
#+begin_theorem
Assume that $V_{\weights}(\hist)$ is twice continuously differentiable as a function of $\weights$ for all histories $\hist\in\Hist$ where $\dw(\hist)>0$ and that $W(\cdot)$, defined in Equation eqref:eqn:gvfn:w, is non-singular in a small neighbourhood of $\weights$. Then for
\begin{align*}
    \boldsymbol{\delta}(\weights)
      &\defeq 
        \Expected_{d,\mu}\bigg[ \sum_{j=1}^\numgvfs \rho_j(H,A) \tderror_j(H,A,H') \phivec_{j,\weights}(H) \bigg] \\
    \secweights(\weights)
      &= W(\weights)^\inv\boldsymbol{\delta}(\weights) \\
     \psivec(\weights) &= \Expected_{d, \mu}\left[\sum\limits_{j=1}^{\numgvfs} \Big(\rho_j(H,A)\delta_j(H,A,H') - \phivec_{j,\weights}(H)^\trans  \secweights(\weights)\Big)  \nabla^2 V^{(j)}_{\weights}(H)  \secweights(\weights) \right]  
\end{align*}
getting the gradient
\begin{align}
  & -\frac{1}{2} \nabla  \textrm{MSPBNE}(\weights)
    =
     -\Expected_{d, \mu}\bigg[ 
     \sum\limits_{j=1}^\numgvfs  \rho_j(H,A) \nabla_\weights \delta_j(H,A,H') \phivec_{j,\theta}(H)^\top  \bigg] \secweights(\weights)  - \psivec(\weights) \label{eq_gtd2}\\
    &=
    \boldsymbol{\delta}(\weights) - \psivec(\weights) \\
      & \ \ \ \ - \Expected_{d,\mu}\bigg[ \sum\limits_{j+1}^\numgvfs  \rho_j(H,A) \bigg[\sum_{i=1}^\numgvfs \cfunc(j,i) \phivec_{i,\weights}(H) + \gamma_j(H,A,H') \phivec_{j,\weights}(H')\bigg] \phivec_{j,\weights}(H)^\trans \secweights(\weights) \bigg]    \nonumber
\end{align}
#+end_theorem

#+begin_proof
For simplicity in notation below, the explicit dependence on the random
variable $H$ are dropped in the expectations. 
\begin{align*}
  \phivec_{j,\weights}(H) \rightarrow \phivec_{j,\weights}&,\hspace{1cm}
  \phivec_{j,\weights}(H') \rightarrow \phivec_{j,\weights}'\\
  \tderror_j(H,A,H') \rightarrow \tderror_j&,\hspace{1cm}\rho_j(H,A) \rightarrow \rho_j
\end{align*}
{{{c}}}
Further, $\partial_i$ indicates the partial derivative with respect to $\weights_i$. 
Also, assume all expectations are with respect to $d, \text{ and } \mu$. $J$ is used to denote the MSPBNE, which from Lemma ref:lemma:gvfn:mspbne-exp and Corollary ref:col:gvfn:mspbne-is, can be written $J(\weights) = \boldsymbol{\delta}(\weights)^\top W(\weights)^\inv \boldsymbol{\delta}(\weights)$.
When applying the product rule
{{{c}}}
{{{c}}}
Recall that $\secweights(\weights) = W(\weights)^\inv\boldsymbol{\delta}(\weights)$, and that $W(\weights)$ is symmetric, giving
\begin{align*}
  \partial_i J(\weights)
  &= 2 (\partial_i \boldsymbol{\delta}(\weights))^\top \secweights(\weights) + \boldsymbol{\delta}(\theta)^\top  \partial_i W(\weights)^\inv \boldsymbol{\delta}(\theta) \\
   \partial_i \boldsymbol{\delta}(\weights)
  &= \Expected\bigg[\sum_{j=1}^\numgvfs \rho_j   \partial_i\phivec_{j,\weights} \tderror_j + \phivec_{j,\weights}  \partial_i\tderror_j \bigg] \\ 
   \partial_i W(\weights)^\inv
  &= - W(\weights)^\inv  \partial_i W(\weights) W(\weights)^\inv 
  = -2 W(\weights)^\inv \Expected\bigg[\sum_{j=1}^\numgvfs (\partial_i\phivec_{j,\weights}) \phivec_{j,\weights}^\trans \bigg] W(\weights)^\inv 
\end{align*}
\begin{align*}
 \boldsymbol{\delta}(\theta)^\top \partial_i W(\weights)^\inv \boldsymbol{\delta}(\theta)
 &= -2\boldsymbol{\delta}(\theta)^\top W(\weights)^\inv \Expected\bigg[\sum_{j=1}^\numgvfs (\partial_i\phivec_{j,\weights})\phivec_{j,\weights}^\trans \bigg] W(\weights)^\inv  \boldsymbol{\delta}(\theta)\\
  &= -2\secweights(\weights)^\trans \Expected\bigg[\sum_{j=1}^\numgvfs (\partial_i\phivec_{j,\weights}) \phivec_{j,\weights}^\trans \bigg] \secweights(\weights)\\
  &= -2\secweights(\weights)^\trans \Expected\bigg[\sum_{j=1}^\numgvfs \phivec_{j,\weights} (\partial_i\phivec_{j,\weights})^\trans \bigg] \secweights(\weights)
\end{align*}
{{{c}}}
   The last line follows from the fact that the transpose of a scalar is equal to the scalar. Here the whole expression is transposed, leading to a transpose of the outer-product inside the sum.
  Additionally,
{{{c}}}
\begin{align*}
 \partial_i \boldsymbol{\delta}(\weights)^\top \secweights(\weights) 
  &= \Expected\bigg[\sum_{j=1}^\numgvfs \rho_j   \tderror_j (\partial_i\phivec_{j,\weights}) + \rho_j\phivec_{j,\weights}  \partial_i\tderror_j \bigg]^\trans  \secweights(\weights)\\ 
&=   \Expected\bigg[\sum_{j=1}^\numgvfs \rho_j \tderror_j (\partial_i\phivec_{j,\weights})^\trans \bigg] \secweights(\weights)
    + \Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \partial_i\tderror_j  \phivec_{j,\weights}^\trans  \bigg] \secweights(\weights)
\end{align*}
{{{c}}}
Grouping the terms with $(\partial_i\phivec_{j,\weights})$,
{{{c}}}
\begin{align*}
&\Expected\bigg[\sum_{j=1}^\numgvfs \rho_j \tderror_j (\partial_i\phivec_{j,\weights})^\trans \bigg] \secweights(\weights) - \secweights(\weights)^\trans \Expected\bigg[\sum_{j=1}^\numgvfs \phivec_{j,\weights}(\partial_i\phivec_{j,\weights})^\trans \bigg] \secweights(\weights)\\
&= \Expected\bigg[\sum_{j=1}^\numgvfs \Big( \rho_j \tderror_j - \secweights(\weights)^\trans\phivec_{j,\weights} \Big)(\partial_i\phivec_{j,\weights})^\trans\secweights(\weights)\bigg] \\
&= \boldsymbol{\psi}_i(\weights)
\end{align*}
{{{c}}}
where the last follows from the definition of $\nabla_\weights \boldsymbol{\psi}(\weights)$, which is the gradient vector composed of partial derivatives $\boldsymbol{\psi}_i(\weights)$. Therefore,
{{{c}}}
\begin{align*}
  \partial_i J(\weights)
  &= 2 \partial_i \boldsymbol{\delta}(\weights)^\top \secweights(\weights) + \boldsymbol{\delta}(\theta)^\top  \partial_i W(\weights)^\inv \boldsymbol{\delta}(\theta) \\
&= 2\boldsymbol{\psi}_i(\weights) + 2\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \partial_i\tderror_j \phivec_{j,\weights}^\trans  \secweights(\weights)  \bigg]
\end{align*}
 which proves Equation \eqref{eq_gtd2}. Now the second term can be further simplified, using the fact that $\phivec_{j,\weights} = \nabla_\weights V_{j,\weights}$, giving
\begin{align*}
\nabla_\weights \tderror_j = \nabla_\weights c_{j,\weights} + \gamma_j \phivec_{j,\weights}' - \phivec_{j,\weights}
.
\end{align*}
Now notice that 
\begin{align*}
\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \nabla_\weights\tderror_j \phivec_{j,\weights}^\trans  \secweights(\weights)  \bigg] 
&= \Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \big(\nabla_\weights c_{j,\weights} + \gamma_j \phivec_{j,\weights}' - \phivec_{j,\weights}\big) \phivec_{j,\weights}^\trans  \secweights(\weights)  \bigg] \\
&= -\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \phivec_{j,\weights}\phivec_{j,\weights}^\trans \bigg]\secweights(\weights) + \Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \big(\nabla_\weights c_{j,\weights} + \gamma_j \phivec_{j,\weights}'\big) \phivec_{j,\weights}^\trans  \secweights(\weights)  \bigg] 
\end{align*}
Because $\secweights(\weights) = W(\weights)^\inv\boldsymbol{\delta}(\weights)$, 
\begin{align*}
\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \phivec_{j,\weights}\phivec_{j,\weights}^\trans \bigg]\secweights(\weights) 
=
W(\weights)\secweights(\weights) = \boldsymbol{\delta}(\weights)
\end{align*}
Putting this all together,
\begin{align*}
  -\tfrac{1}{2} \nabla_\weights J(\weights)
  &= -\boldsymbol{\psi}_i(\weights) - \Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j  \nabla_\weights\tderror_j \phivec_{j,\weights}^\trans\secweights(\weights) \bigg]\\
  &= -\boldsymbol{\psi}(\weights) + \boldsymbol{\delta}(\weights) -\Expected\bigg[ \sum_{j=1}^\numgvfs \rho_j \big(\nabla_\weights c_{j,\weights} + \gamma_j \phivec_{j,\weights}'\big) \phivec_{j,\weights}^\trans  \secweights(\weights)  \bigg] 
\end{align*}
{{{c}}}
completing the proof.
#+end_proof

The resulting Recurrent GTD algorithm explicitly learns a second set of weights $\secweights$, to perform this update. In our implementation, a particular form of composition is used, namely that the cumulant for a GVF is a linear weighting of the predictions of some of the other GVFs on the next time step. If $c(i, j)$ indicates the weight on the \(i\)th GVF in the cumulant for the \(j\)th GVF, then $\nabla_\weights c_{j,t} =   \sum_{i=1}^\numgvfs c(j,i) \phivec_{i,t}'$. 

The full *Recurrent GTD* update is 

\begin{align}
\svec_t &\gets f_{\weights_t}(\svec_{t-1}, \xvec_t) \nonumber\\
\svec_{t+1} &\gets f_{\weights_t}(\svec_{t}, \xvec_{t+1}) \nonumber\\
\phivec_{t,j} &\gets \nabla_\weights \svec_{t,j} \hspace{2.0cm} \triangleright \text{ Compute sensitivities using truncated BPTT}  \nonumber\\
\phivec'_{t,j} &\gets \nabla_\weights \svec_{t+1,j}  \nonumber\\
\rho_{t,j} &\gets \frac{\pi_j(a_t | \obs_t)}{\mu(a_t | \obs_t)}  \nonumber\\
\vvec_t &= \nabla^2 \svec_t \secweights_t \hspace{2.0cm} \triangleright \text{ Computed using R-operators, see Appendix \ref{sec:gvfn:gradbtt}} \nonumber\\
  \psivec_t &= \sum_{j=1}^{\numgvfs} ( \rho_{j,t}\delta_{j,t} - \phivec_{j,t}^\trans  \secweights_t)  \vvec_t \label{eq_rgtd_gen}\\
  \weights_{t+1} &= \weights_{t} + \alpha_t \bigg[ \sum_{j=1}^{\numgvfs}  \rho_{j,t} \tderror_{j,t} \phivec_{j,t} - \rho_{j,t} \bigg[\nabla_\weights c_{j,t} + \gamma_{j,t+1} \phivec'_{j,t}  \bigg] \phivec_{j,t}^\trans \secweights_t - \psivec_t  \bigg] \nonumber\\
   \secweights_{t+1} &= \secweights_t + \beta_t \bigg[ \sum_{j=1}^{\numgvfs}  \rho_{j,t} \Big(\tderror_{j,t} - \phivec_{j,t}^\trans \secweights_t\Big) \phivec_{j,t} \bigg] \nonumber
\end{align}

** Computing gradients of the value function back through time
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:gradbtt
:END:

In this section, I show how to compute $\phivec_t$, which was needed in the
algorithms. Recall from Section ref:sec:gvfn:algs that $V^{(j)}(\svec_{t+1}) = \svec_{t+1,j}$, and using
$\Feats_{t+1}\defeq\twovec{\svec_{t}}{\xvec_{t+1}}$ let
  $\svec_{t+1,j} =
  \sigma\left( \Feats_{t+1}^\top\weights^{(j)}\right)$ for
  some activation function $\sigma:\RR\rightarrow\RR$. 
  For both Backpropagation Through Time or Real Time Recurrent Learning,
  it is useful to take advantage of the following formula for /recurrent sensitivities/
{{{c}}}
\begin{align*}
  \pd{V^{(i)}(S_{t+1})}{\weights_{(k,j)}} &= \actdot(\Feats_{t+1}^\trans \weights^{(i)}) \biggr(\biggr(\pd{\Feats_{t+1}}{\weights_{(k,j)}} \biggr)^\trans \weights^{(i)} + (\Feats_{t+1})_j \krondelta_{i,k}\biggr) \\
  &= \actdot(\Feats_{t+1}^\trans \weights^{(i)}) \left(\biggr[\pd{V^{(1)}(S_{t})}{\weights_{(k,j)}}, ... ,\pd{V^{(n)}(S_{t})}{\weights_{(k,j)}},\zerovec^\top\biggr] \weights^{(i)} + (\Feats_{t+1})_j \krondelta_{i,k}\right) 
\end{align*}
{{{c}}}
where $\krondelta$ is the Kronecker delta function and $\actdot(\cdot)$ is shorthand for the derivative of $\sigma$ w.r.t its scalar input. Given this formula, BPTT or RTRL can simply be applied.

For Recurrent GTD---though not for Recurrent TD---Hessian needs to be computed back through time, for the Hessian-vector product. The Hessian for each value function is a $\numgvfs(\featuresize)\times \numgvfs(\featuresize)$ matrix; computing the Hessian-vector product naively would cost at least $O((\featuresize + \numgvfs)^2 \numgvfs^2)$ for each GVF, which is prohibitively expensive. This can be avoided using R-operators also known as Pearlmutter's method cite:&pearlmutter1994fast. 

The R-operator $\Roperator\{\cdot\}$ is defined as 
\begin{equation*}
  \mathcal{R}_\secweights\biggr\{\gvec(\weights)\biggr\} \defeq \frac{\partial \gvec(\weights + r \secweights)}{ \partial r} \biggr\rvert_{r=0}
\end{equation*}
for a (vector-valued) function $\gvec$ and satisfies 
\begin{equation*}
  \mathcal{R}_\secweights\biggr\{\nabla_\weights f(\weights)\biggr\} = \nabla^2_\weights f(\weights) \secweights.
\end{equation*}
{{{c}}}
Therefore, instead of computing the Hessian and then producting with $\secweights_t$, this operation can be completed in linear time, in the length of $\secweights_t$. 

Specifically, for our setting,
\begin{align*}
  &\mathcal{R}_w\biggr\{\actdot(\Feats_t^\top\weights)[\nabla_\weights \Feats_t^\trans \weights + \Feats_t^\trans \nabla_\weights\weights]\biggr\}\\
  & \quad = \pd{}{r}\biggr(\actdot(\Feats_t^\top(\weights + r \secweights)[\nabla_\weights \Feats_t^\trans (\weights + r\secweights) + \Feats_t^\trans \nabla_\weights(\weights+r\secweights)]\biggr) \biggr\rvert_{r=0}
\end{align*}
{{{c}}}
To make the calculation more manageable each partial for every node k and associated weight j is separated.
{{{c}}}
\begin{align*}
  \pd{V^{(i)}(S_{t+1}, \weights)}{\weights_{(k,j)}} &= \actdot(\Feats_{t+1}^\trans \weights^{(i)}) (\eta_{t+1})_{i,k,j} \\
  (\eta_{t+1})_{i, k, j} &= ((\valuedtheta_t)_{k,j}^\trans \weights^{(i)} + (\Feats_{t+1})_j \delta_{i,k}) \\
  (\valuedtheta_t)_{k,j} &= \left[\pd{V^{(1)}(S_{t})}{\weights_{(k,j)}}, ... ,\pd{V^{(n)}(S_{t})}{\weights_{(k,j)}},\zerovec^\top\right]^\top\\
  \valuedr_t &= \left[\pd{V^{(1)}(S_{t})}{r}, ... ,\pd{V^{(n)}(S_{t})}{r},\zerovec^\top\right]^\top \\
\end{align*}
\begin{align*}
  \RopValueVect &= \left[\mathcal{R}_w\biggr\{\pd{V^{(1)}(S_{t-1})}{\weights_{(k,j)}}\biggr\}, ..., \mathcal{R}_w\biggr\{\pd{V^{(\numgvfs)}(S_{t-1})}{\weights_{(k,j)}}\biggr\},\zerovec^\top\right]^\top \\
  \mathcal{R}_w\left\{\pd{V^{(i)}(S_{t+1}, \weights)}{\weights_{(k,j)}}\right\} &= \frac{\partial^2 V^{(i)}(S_{t+1}, \weights + r\secweights)}{\partial r \partial \weights_{(k,j)}} \biggr\rvert_{r=0} \\
  &= \actdotdot\biggr(\Feats_{t+1}^\trans (\weights^{(i)} + r\secweights_i)\biggr) \biggr(\valuedr_t^\trans (\weights^{(i)} + r\secweights_i) + \Feats_{t+1}^\trans \secweights_i\biggr) (\eta_{t+1})_{i,k,j} \\
  &\phantom{{}=}
   + \actdot\biggr(\Feats_{t+1}^\trans (\weights^{(i)} + r\secweights_i)\biggr)\biggr(\RopValueVect^\trans (\weights^{(i)} + r\secweights)
   % &\phantom{{}= + \actdot(x_{t+1}^\trans (\weights^{(i)} + r\secweights_i))}
  + (\valuedtheta_t)_{k,j}^\trans w_i
  + (\valuedr_t)_j \krondelta_{k,i}\biggr) \biggr\rvert_{r=0} \\
  &= \actdotdot\biggr(\Feats_{t+1}^\trans \weights^{(i)}\biggr) \biggr(\valuedr_t^\trans (\weights^{(i)}) + \Feats_{t+1}^\trans \secweights_i\biggr) (\eta_{t+1})_{i,k,j} \\
  &\phantom{{}=}
   + \actdot\biggr(\Feats_{t+1}^\trans \weights^{(i)}\biggr) \biggr(\RopValueVect^\trans \weights^{(i)}
   % &\phantom{{}=}
  + (\valuedtheta_t)_{k,j}^\trans \secweights_i + (\valuedr_t)_j \krondelta_{k,i}\biggr)\\
  \pd{V^{(i)}(S_{t})}{r} &= \actdot(\Feats_t^\trans \weights^{(i)})(\valuedr_{t-1}^\trans \weights^{(i)} + \Feats_t^\trans w_i)
  % \krondelta_{k,i} &\defeq \text{Kronecker Delta} 
\end{align*}

** TD(\(\lambda\)) for GVFNs
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:tdlambda
:END:

For many of the experiments Recurrent TD is used with no back-propagation through time $p=1$. This algorithm only adjusts parameters to minimize immediate TD error. In many cases, this was sufficient, but at times it was slow and increasing $p$ improved learning. Another strategy is to use traces to obtain credit assignment back-in-time. The TD-error on this step can be attributed to state values back-in-time, with the \textbf{TD($\boldsymbol{\lambda}$) algorithm} 
{{{c}}}
\begin{align}
\svec_t &\gets f_{\weights_t}(\svec_{t-1}, \xvec_t) \nonumber\\
\svec_{t+1} &\gets f_{\weights_t}(\svec_{t}, \xvec_{t+1}) \nonumber\\
\gvec_{t,j} &\gets \nabla_{\weights_j} f_{\weights_t}(\svec_{t-1}, \xvec_{t}) && \triangleright \text{ gradient given $\svec_{t-1}$, no BPTT} \nonumber\\
\evec_{t,j} &\gets \gvec_{t,j} + \gamma_{t,j} \lambda \evec_{t-1,j} && \triangleright \text{ eligibility trace, $0 \le \lambda \le 1$} \nonumber\\
\delta_{t,j} &\gets C_{t+1}^{(j)} + \gamma_{t+1, j} \svec_{t+1,j} - \svec_{t,j}   \nonumber\\
\weights_{t+1,j} &\gets \weights_{t,j} + \alpha_t \tderror_{t,j} \evec_{t,j} \label{eq_td_lambda}
\end{align}
{{{c}}}
Notice the difference to Recurrent TD and Recurrent GTD, that the weights for each GVF are updated independently. This difference arises because the gradient computations for back-in-time, for the sensitivities, is what couples the updates. Without these sensitivities, the immediate gradient of the value $\gvec_{t,j}$ is independent for each GVF. 

** Summary

This chapter focused on the characterization of the mean squared projected bellman network error (MSPBNE) and subsequent derivations of algorithms based on this objective function. Specifically, I defined the Bellman network operator and the subsequent restrictions on the cumulants (i.e. acyclic composite connections). Equipped with the Bellman network operator and proving it is a contraction under some standard assumptions, the mean squared projected bellman network error can be defined. The final sections of the chapter derived the recurrent GTD and TD algorithms, and provided some insight into how to calculate various components with temporal sensitivities.
* Empirically Exploring Hand Designed GVFNs
:PROPERTIES:
:CUSTOM_ID: chap:gvfn:empirical
:END:


In this chapter, I empirically explore GVFNs with hand-designed GVFs. These empirical investigations provide the initial evidence that GVFNs can learn without gradients calculated through BPTT. Through this empirical experimentation I show the importance of choosing GVFs as a means to develop predictive targets, explore the application of GVFNs on time-series forecasting, and finally show recurrent TD to be enough to learn in Ring World a Cycle World.

** Experiments in Forecasting
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:exp-forecasting
:END:

In this section, I compare GVFNs and RNNs on two time series prediction datasets, particularly to ask 1) can GVFNs obtain comparable performance and 2) do GVFNs allow for faster learning, due to the regularizing effect of constraining the state to be predictions.[fn:: All code for these experiments can be found at [[https://github.com/mkschleg/GVFN]]] I investigate if they allow for faster learning both by examining learning speed as well as robustness to truncation length in BPTT.

\paragraph{Datasets}

I consider two time series datasets previously studied in a comparative
analysis of RNN architectures by [[cite:&bianchi2017recurrent]]: the Mackey-Glass
time series (previously introduced), and the Multiple Superimposed Oscillator.

The single-variate *Mackey-Glass (MG)* time series dataset is a synthetic data set generated from a time-delay differential equation:
{{{c}}}
\begin{equation}
  \partialderivative{y(t)}{t} = \alpha\frac{y(t-\tau)}{1+y(t-\tau)^{10}} - \beta y(t)\label{eq:mg}
  .
\end{equation}
{{{c}}}
I follow the learning setup in [[cite:&bianchi2017recurrent]]: setting $\tau=17$,
$\alpha=0.2$, $\beta=0.1$, and take integration steps of size $0.1$. The target variable $y$ is forcasted twelve steps into the
future, starting from an initial value $y(0)=1.2$. $\nsamples = 600,000$ samples are generated.

The *Multiple Superimposed Oscillator (MSO)* synthetic time series cite:&jaeger2004harnessing is defined by the sum of four sinusoids with unique frequencies

\begin{equation}
  y(t) = \sin(0.2t)+\sin(0.311t)+\sin(0.42t)+\sin(0.51t). \label{eq:mso}
\end{equation}
{{{c}}}
The resulting oscillator has a long period of $2000\pi \approx 6283.19$. Because data is generated using $t\in\Naturals$, the oscillator effectively never returns to a previously seen state. These attributes make prediction difficult with the MSO, as the model cannot rely on memory alone to make good predictions. $\nsamples = 600,000$ samples are generated and make predictions with a forecast horizon of $h=12$.

\paragraph{Experiment Settings}

The focus in this work is on online prediction, and so report online prediction error. At each step $t$, after observing $o_t = y(t)$, the RNN (or GVFN) makes a prediction $\hat{y}_t$ about the target $y_t$, which is the observation 12 steps into the future, $y_t = y(t+h)$. The magnitude of the squared error $(\hat{y}_t - y_t)^2$ depends on the scale of $y_t$. To provide a more scale invariant error, the error is normalized by the mean of the target---a mean predictor. Specifically, for each run, the average error over windows of size 10000 with the mean predictor is computed for each window is reported. This results in $\nsamples/10000$ normalized squared errors, where $\nsamples$ is the length of the time series. This process is repeated 30 times, and average these errors across the 30 runs, and take the square root, to get a Normalized Root Mean Squared Error (NRMSE).

The values for hyperparameters are fixed as much as possible, using the previously reported value for the RNN and reasonable defaults for the GVFN. The stepsize is typically difficult to pick ahead of time, and so sweep that hyperparameter for all the algorithms. The number of hyperparameters swept are made comparable for all methods, to avoid an unfair advantage. The truncation length is not tuned, as results are reported for each truncation length $p\in \{1, 2, 4, 8, 16, 32\}$ for all the algorithms.

\paragraph{Algorithm Details}

The GVFN consists of a single layer of size 32 and 128 (for MG and MSO respectively), corresponding to horizon GVFs. As described in Section ref:sec:gvfn:case-study, each GVF has a constant continuation $\gamma^{(j)} \in [0.2,0.95]$ and cumulant $C_{t}^{(j)}=\frac{1-\gamma^{(j)}}{y^{\text{max}}_{t}}y(t)$, where
$y^{\text{max}}_t$ is an incrementally-computed maximum of the observations
$y(t)$ up to time $t$. The GVFs are generated to linearly cover the range
$[0.2,0.95]$. This set is chosen as one of the simplest options that can be used
without much domain knowledge. It is likely not the optimal set of GVFs for the
GVFN, but represents a reasonable default choice.
The GVFN is followed by
a fully-connected layer with relu activations to produce a non-linear
representation, which is linearly weighted to predict the target.
The GVFN layer uses a linear activation, with clipping between [-10,
10], to help ensure state features remain bounded;
again, this represented a simple rather than optimized choice.

The GVFN was trained using Recurrent TD with a constant learning rate and a batch size of 32. The weights for the fully-connected relu layer and the weights for the linear output  are trained using ADAM, to minimize the mean squared error between the prediction at time $t$ and target $y(t+h)$. The stepsize hyperparameters were swept: the learning rate for the GVFN $\alpha_{\text{\tiny GVFN}} = N\cdot10^{-k}$ for $N\in\{1,5\}$, $k\in\{3,\ldots,6\}$, and the learning rate for the fully-connected and output layers $\alpha_{\text{pred}} =N\cdot10^{-k}$ for $N\in\{1,5\}$, $k\in\{2,\ldots,5\}$.

I compare to RNNs, LSTMs, and GRUs [fn:: Using standard implementations found in Flux cite:&innes:2018.]. The network architecture is similar to the GVFN for all recurrent models. The RNN size is set to 32 for MG and 128 for MSO, while the GRU and LSTM have 8 hidden units for MG and 128 for MSO. Notice how the GRU and LSTM have fewer hidden units than the RNN and GVFN for the MG experiment. This roughly accounts for the increased complexity of the LSTMs and GRUs as compared to the GVFN and RNN. While this was needed to make all the models competitive in MG, I found the GVFNs performed well in MSO even with the same number of hidden units as the GRU and LSTMs.
These models were trained using p-BPTT---specifically with the ADAM optimizer with a batch size of 32---to
minimize the mean squared error between the prediction at time $t$ and $y(t+h)$. The learning rate was swept in the range $\alpha = 2^{-k}$ with $k \in \{1,\ldots,20\}$.
\begin{figure}[t!]
  \center
  \includegraphics[width=0.95\textwidth]{plots/gvfn/timeseries/trunc_comb_3.pdf}
  \caption{
    Truncation sensitivity for the (\textbf{left}) Mackey-Glass and (\textbf{right}) Multiple Superimposed Oscillator datasets. Errors are calculated using the normalized root mean squared error (NRMSE) averaged over the last 10k steps for the training results $\pm$ 1 standard error over 30 independent runs.
  }\label{fig:timeseries_sens}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.95\textwidth]{plots/gvfn/timeseries/learning_curves.pdf}
  \caption{
    Learning curves for the (\textbf{top}) Mackey-Glass and (\textbf{bottom}) Multiple Superimposed Oscillator datasets. The normalized root mean squared error (NRMSE) normalized to the performance of the windowed average baseline is reported. All results are an average of 30 independent runs $\pm$ the standard error.
  }\label{fig:timeseries_lc}
\end{figure}

Finally, I also compare to RNNs with the 128 GVFs as auxiliary tasks. The augmented RNN has the same architecture as above, but with an additional set of output heads. The additional GVF heads are the same as those used by the GVFN, and are trained with TD. The gradient information from the GVFs is back-propagated through the network, influencing the representation. The augmented RNN was tuned over the same values as the RNN. The goal for adding this baseline is to gauge if there is an important difference in using the GVFs to directly constrain the state, as opposed to indirectly as auxiliary tasks. It further ensures that the RNN is given the same prior knowledge as the GVFN---namely the pertinence of these predictions---to avoid the inclusion of prior knowledge as a confounding factor.

All RNNs and GVFNs include a bias unit, as part of the input as well as in all layers. All methods have similar computation per step, particularly as they are run with the same truncation levels $p$.

\paragraph{Results}
I first show overall results across the truncation level in p-BPTT in Figure \ref{fig:timeseries_sens}. Three results are consistent across both datatsets: 1) GVFNs can obtain significantly better performance than RNNs with small $p$; 2) GVFNs are surprisingly robust to truncation level, providing almost the same performance across $p$; and 3) auxiliary tasks in the RNN do not provide consistent benefits across models and datasets. GVFNs provide a strict improvement on the MSO dataset. The result on MG is more nuanced. As truncation levels increase, the RNN's performance significantly improves and then passes the GVFN. This might suggest some bias in the specification of the GVFs. As is typical with regularization or imposing an inductive bias, it can improve learning---here allowing for much more stable learning with small $p$---but can prevent the solution from reaching the same prediction accuracy. In some cases, the inductive bias is strictly helpful, constraining the solution in the right way so as to incur minimal bias but improve learning. In MSO, it's possible the GVF specification was more appropriate and in MG less appropriate.


To gain more detailed insight into the behavior of the algorithms across truncation levels, learning curves for $p \in \{1, 8, 32\}$ are presented in Figure \ref{fig:timeseries_lc}. All the approaches learn more slowly for $p =1$, but the RNNs are clearly impacted more significantly. In MSO, the GVFN has a clear advantage in terms of learning speed. This is not true in MG, where once $p \ge 8$, the RNN performs better and learns faster. The GVFN objective here may actually be difficult to optimize, but it allows the agent to make progress constructing a useful state, whereas the signal from the error to the targets is insufficient.

** Investigating Performance under Longer Temporal Dependencies

\begin{figure}[t]
  \center
  \includegraphics[width=0.95\textwidth]{plots/gvfn/compass_world/trunc_acc_comb_2.pdf}
  \caption{Results averaged over 30 runs $\pm$ one standard error. The dashed lines correspond to each RNN type augmented with auxiliary tasks, namely here the terminating horizon GVFs. The plots on the \textbf{(left)} are for a constant learning rate swept in range $\{0.1\times1.5^i; i \in [-10, 5]\} \cup \{1.0\}$. The plots on the \textbf{(right)} are for the ADAM optimizer with learning rate swept in range $\{0.01\times 1.5^i; i \in \{-18, -16, \ldots, 0\}\}$. The \textbf{(top)} row shows sensitivity over truncation measured by the average root mean squared value error (RMSVE) over the final 200000 steps of training. The \textbf{(bottom)} row shows learning curves for $p=4$ for prediction accuracy. The prediction is the color of five walls with the highest GVF output, where the GVF prediction corresponds to a probability of facing that wall. When averaged over a window (10000 steps in our case) this results in a percentage of correct predictions during that time span.
  } \label{fig:compass}
\end{figure}

In this section, I investigate the utility of constraining states to be predictions, for an environment with long temporal dependencies. Compass World, introduced in Section \ref{GVFNs} (see Figure \ref{fig:gvfn:compass-world-env}), which can have long temporal dependencies, is used because the random behavior can stay in the center of the world for many steps, observing only the color white.
The observation is encoded with two bits per color: one to indicate the agent observes that color, and the other to indicate another color is observed. The behavior policy chooses randomly between moving one-step forward; turning right/left for one step; moving forward until the wall is reached ({\em{leap}}); or randomly selecting actions for $k$ steps ({\em{wander}}). The full observation vector is encoded based on which action was taken, and includes a bias unit.

Five hard-to-learn GVFs are used with predictions corresponding to the wall the agent is facing. These predictions are not learnable without constructing an internal state. These five questions correspond to leap questions. The leap question is defined as having a cumulant of 1 in the event of seeing a specific wall (orange, yellow, red, blue, green), and a continuation function defined as $\gamma = 0$ when any color is observed---when the agent is facing a wall---and $\gamma = 1$ otherwise.


The same architecture is used for both RNNs and GVFNs. The GVFN uses 40 GVFs: 8 GVFs per color. The 8 GVFs for a color correspond to \textbf{Terminating Horizon} GVFs. This means that they have a cumulant of 1 when seeing that color, and zero otherwise; they have a $\gamma = 1- 2^k$ for one of 8 $k \in \{-7, -6, \ldots, -1\}$; they terminate---$\gamma$ becomes zero---when any color is observed; and the policy is to always go forward. These GVFs are similar to the horizon GVFs in time series prediction, except that termination occurs when a wall is reached and the policy is off-policy.
The RNN similarly uses 40 hidden units for the recurrent layer. For RNNs, the hyperbolic tangent is used. Sigmoids are used instead for GVFNs, because the returns are always nonnegative; otherwise, these two activations represent a similar architectural choice.

Treating the input action $a_t$ specially significantly improved performance of both the RNN and GVFN, as suggested by Chapter ref:chap:arnn. This is done by specifying separate weight vectors $\{w_a \in \mathbb{R}^n ; \forall a \in \mathcal{A}\}$ for each action the agent can take. The hidden state is then calculated as $\svec_{t+1} = \sigma(w_{a_t}^\trans[\xvec_{t+1}, \svec_t])$, where $\sigma$ is the activation function. For the GRUs and LSTMs, this architectural modification is not straightforward; instead the action is passed as a one-hot encoding.

All the approaches share the same structure following the recurrent layer. The state $\svec_t$ is passed to a 32-dimensional hidden layer with relu activation, and then is linearly weighted to produce the predictions for the five hard-to-learn GVFs: $\hat{\mathbf{y}}_t = \text{relu}(\svec_t^\top \mathbf{F}) \mathbf{W}$ where $\mathbf{F} \in \RR^{40 \times 32}$ and $\mathbf{W} \in \RR^{32 \times 5}$. All methods include a bias unit on every layer.


The performance for increasing $p$, as well as learning curves for $p = 8$, are show in Figure \ref{fig:compass}. Again, there are several clear conclusions. 1) The GVFN is again highly robust to truncation level, reaching almost perfect accuracy with $p =1$. 2) The GVFN can learn noticeably faster with smaller $p$, such as $p = 4$, and the differences disappear for larger $p$. 3) The auxiliary tasks do not provide near the same level of benefit as the GVFN, though unlike the time series results, there does in fact seem to be some benefit. 4) All the methods are improved when using ADAM---especially the LSTMs and GRUs---though GVFNs are effective even with constant stepsizes.

** Investigating Poorly Specified GVFNs
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:emp:poorlyspecified
:END:

In the previous Compass World and Forecasting experiments, the GVFNs were robust to truncation. In fact, computing one-step gradients was sufficient for good performance. A natural question is when we can expect this to fail. I hypothesize that this robustness to truncation relies on appropriately specifying the GVFs in the GVFN. Poorly specified GVFs could both (a) make it so that the GVFN is incapable of constructing a state that can accurately predict the target and (b) make training difficult or unstable. In this section, this hypothesis is tested by evaluating several choices for the GVFs in the GVFN in Compass World.

Three additional GVFN specifications are considered: two that include intentional (but realistic) misspecifications and one that should be an improvement on the Terminating Horizon GVFN. The first misspecification, which is called the \textbf{Horizon} GVFN, causes the hidden states to have widely varying magnitudes. These GVFs are similar to the Terminating Horizon GVFs, except that they do not include termination when a color is observed. This means the true expected returns can be quite large, up to $\tfrac{1}{1-\gamma}$ (e.g.,  $\frac{1}{1-0.99} = 100$) if the agent is already immediately in front of the wall with that color. The policy is to go forward, and so if the agent is already facing the wall and receives a cumulant of 1, it will see a 1 forever onward, resulting in a return of $\sum_{i=0}^\infty \gamma^i = \tfrac{1}{1-\gamma}$.

The second misspecification provides a minimal set of sufficient predictions, but ones that are harder to learn. A natural choice for this is to use the five hard-to-learn predictions themselves, which is clearly sufficient but may be ineffective because they cannot be learned quickly enough to be a useful state. This is called the \textbf{Naive} GVFN, because it naively assumes that representability is enough, without considering learnability.

Finally, I also consider a specification that could improve on the more generic Terminating Horizon GVFN, that is called the \textbf{Expert Network}. This network also has 40 GVFs, but ones that are hand-designed for Compass World. This GVFN is a modified version of the TD network designed for Compass World cite:&sutton2005temporaldifference. The GVFs are defined similarly for the 5 colours. There are 3 myopic GVFs: a myopic GVFs consists of a myopic termination ($\gamma = 0$ always) and a cumulant of the color bit. Each myopic GVF has a persistent policy, which takes one action forever. Since there are three actions there are three myopic GVFs. These myopic GVFs indicate whether the agent is right beside the color (ahead, to the left or to the right). There is 1 leap GVF where the policy goes forward always, the cumulant is again the color bit and $\gamma=1$ except when a color is observed, giving $\gamma =0$. There are 2 GVFs with a persistent policy (left, right) with myopic termination and a cumulant of the previous leap GVF's. These compositional GVFs let the agent know if they were to first turn right (or left) and then go forward, would they see the color. There are 2 leap GVFs with cumulants of the myopic GVFs. Finally, there is 1 GVF with uniform random policy with $\gamma = 0$ at a wall event and $\gamma = 0.5$ otherwise.

As a baseline, a \textbf{Forecast} network is included, which uses $k$-horizon predictions for the hidden state instead of GVFs. The architecture of the Forecast network is otherwise the same as the GVFN. The \textbf{Forecast} network uses a set of horizons $\mathcal{K} = \{1, 2, \ldots, 8\}$, for each of the non-white observations, resulting in a hidden state size of 40. To train these networks online a buffer is kept of $p+\max(\mathcal{K})$ observations, using the first $p$ observations in the BPTT calculation and the next $k$ observations to determine the targets of the network. The most recent hidden state is recovered to train the evaluation GVFs as done with the RNN and GVFN architectures. More specifically, at time step $t$, the state $\svec_{t-k}$ is updated with observations $\mathbf{o}_{t-k+1}, \ldots, \mathbf{o}_{t}$.


\begin{figure}[t]
  \center
  \includegraphics[width=0.95\textwidth]{plots/gvfn/compass_world/gvfn_lc.pdf}
  \caption{
  Learning curves for \textbf{(dashed)} $p=1$ and \textbf{(solid)} $p=8$ for various GVFN specifications and the Forecast networks. The GVFN is labeled TermHorizon, to highlight that it is composed of terminating horizon GVFs. Learning rates were chosen as in Figure \ref{fig:compass}, where the left plot corresponds to using a constant stepsize and the right to using the ADAM optimizer. The errors were averaged over 30 independent runs, to get the final learning curves $\pm$ standard error.}\label{fig:compass_poor}
\end{figure}


Learning curves for all the GVFN specifications, as well as the Forecast network, with $p = 1$ and $p = 8$, are reported in Figure \ref{fig:compass_poor}. The results indicate that the specification can have a big impact. The two misspecified GVFNs perform noticeably worse than the Terminating Horizon GVFN. As expected, the Naive GVFN is eventually able to learn, with enough steps, $p = 8$ and the ADAM optimizer. It is sufficient to obtain a good state, but poor learnability prevents it from playing a useful role. The Horizon GVFN, which has potentially high magnitude GVF predictions, is closer in performance to the Terminating Horizon GVFN, but clearly worse. The Expert GVFN, on the other hand, can get to a lower error, though it does not have a clear advantage in terms of learning speed or robustness to $p$; this slower learning could again be potentially due to the fact that these expert GVFs were more difficult to learn than the simpler terminating horizon GVFs. Finally, the Forecast network performed very poorly. This is not too surprising in this environment. When considering a $k$-horizon prediction, the target is often zero, with the occasional one. This is generally a hard learning problem, as the resulting prediction loss does not provide a useful constraint. These results clearly show specifying the GVFs used to constrain the hidden state is an important consideration when using GVFNs, and could be the difference between learnable and not learnable representations.

** Comparing Recurrent GTD and Recurrent TD

TD networks with a simple TD network update rule---no backprop through time---have been shown to have divergence issues on a simple six-state domain, called Ringworld~cite:&tanner2005temporal. In fact, Gradient TD networks cite:&silver2013gradient  were introduced precisely to solve this problem. Because GVFNs are a strict generalization of TD networks, the GVFN can be set to get the same problematic setting if a simple TD update is used (RTD with $p=1$). This raises a natural question of if Recurrent TD (RTD) similarly has divergence issues, and if Recurrent GTD (RGTD) is needed.

In all of the experiments so far, I have opted for the simpler RTD algorithm, rather than the full gradient algorithm RGTD, because empirically little difference is found between the two. RTD, unlike the simple TD update rule, does in fact compute gradients back-in-time, and so should be a more sound update. Further, once truncated BPTT is used, even RGTD is providing a biased estimate of the gradient. But nonetheless RTD---which is built on the semi-gradient TD update---does drop more of the gradient than RGTD. It is likely that RGTD is needed in some cases. But it is possible that for most settings, RTD provides a reasonable interim choice between the simple TD network learning rule, and the more complex RGTD.

In this section, I test RTD and RGTD on Ringworld, to see if they perform differently on this known problematic setting.  Note that for $p=1$, RTD reduces to the simple TD network learning rule, and so expect poor performance.

Ring World is a six-state domain~cite:&tanner2005temporal where the agent can move left or right in the ring. All the states are indistinguishable except state six. The observation vector is simply a two bit binary encoding indicating if the agent is in state six or not. The agent behaves uniformly randomly. The goal is to predict the observation bit on the next time step. The environment itself is not too difficult for state-construction; rather a particular TD network causes divergence from the simple TD update rule. The corresponding GVFN consists of two chains of compositional GVFs: one chain for always go right and one chain for always going left. In the first chain, the first GVF is a myopic GVF, that has as cumulant the observed bit after taking action Right, with $\gamma =0$. This first GVF predicts the observation one step into the future. The second GVF has the first GVFs prediction as a cumulant after taking action Right, with $\gamma = 0$. This second GVF predicts the observation two steps into the future. There are five GVFs in each chain, for a total of 10 GVFs in the GVFN.

\begin{figure}[t]
  \center
  \begin{subfigure}{0.55\textwidth}
    \includegraphics[width=\textwidth]{plots/gvfn/ring_world/learning_curve_rgtd.pdf}
  \end{subfigure}
  \caption{
    Learning curves for $p=1$ and $p=2$ averaged over 10 runs with fixed window smoothing of 1000 steps, in the Ringworld environment. Learning rates chosen from a sweep over $\alpha \in \{0.1\times1.5^i; i\in\{-10, -9, \ldots, 6\}\}$ for the RNN and learning rates $\alpha \in \{0.1\times1.5^i; i\in\{-6, -9, \ldots, 8\}\}$ and $\beta = \{0.0, 0.01\}$ corresponding to RTD and RGTD respectively. All approaches needed only $p = 2$ to learn, including the baseline RNN included for comparison.
  }\label{fig:ring}
\end{figure}

Figure \ref{fig:ring} shows the results of the Ring World experiments for truncation $p=1$ and $p=2$. The GVFNs for both RTD and RGTD needed only $p \ge 2$ to learn effectively. A baseline RNN of the same architecture is included, that indicates that the GVFN specification does negatively impact performance. But, with even just \(p = 2\), any convergence issues seem to disappear. In fact, RTD and RGTD perform very similarly. The fact that Ringworld is not problematic for RTD is by no means a proof that RTD is sufficient, especially since Ringworld was designed to be a counterexample for the simple TD network update not for RTD. But, it is one more datapoint that RTD and RGTD perform similarly. In future work, counter examples should be investigated for RTD to better understand when it might be necessary to use RGTD.

** Summary

This chapter contains some initial evidence to the effect of using GVFs to constrain the state of a recurrent network. While this provides some evidence for the /Prediction Representation hypothesis/, I do not perform a explicit test of this hypothesis. While these experiments show the initial promise of the GVFN approach to learning a state-update function, they are not extensive. In all the empirical results, the agent's were trained in a sequential manner without experience replay buffers. This possibly limited the performance possible for both the RNNs and GVFNs. The results also relied on hand designed GVFs. While several networks used simple heuristics to construct a collection of GVFs, it is yet to be tested whether these results generalize beyond the simple bit domains and synthetic forecasting domains used here.

* The set of Predictive Questions in General Value Functions 
CLOSED: [2023-02-22 Wed 14:34]
:PROPERTIES:
:CUSTOM_ID: chap:composite
:END:

In Chapter ref:chap:gvfn:empirical, I explored several hand designed network configurations. This lead to several observations, including compositional GVFs have considerable representational power if the predictive questions can be answered. In the following Chapter, I propose constraints we should apply to the set of GVFs that should be considered in a egocentric agent. These constraints are born from the constraints on a continual learning agent, and inform future explorations in using GVFs and GVFNs for planning and control. I follow up this discussion by analyzing a specific type of GVF oft unexplored in the literature, composite GVFs. Specifically, I explore the open question of "what do GVFs composed together in chains predictively represent?" While I don't answer this question conclusively for all possible chains GVFs, I provide explorations for two important types of composite GVFs: those which have constant discounts, and those which have terminating discounts.

** Expanding the set of predictive targets in a continual learning agent :noexport:

*************** DONE Motivate what specific targets do we want.   :noexport:
CLOSED: [2023-04-19 Wed 11:18]
*************** END


In this thesis, I have taken the perspective that an agent is situated inside its environment and observes its world from an egocentric perspective, continually. While this is not a particularly novel interpretation of the machine intelligence problem [[cite:&ring1994continual;&ring1997child;&sutton2011horde]], it is worthwhile to clarify the terms we will use throughout intuitively before moving onto formal descriptions. 

#+caption: Diagram of the agent in the environment
#+name: fig:bg:agent-env-interaction
#+attr_latex: :width 0.8\linewidth
[[./plots/world-body-agent-diagram.pdf]]

The *agent* exists inside the *environment*. The *environment* is large. So large the agent faces a vast array of detail and regularity, which can not be captured in any finite way. This big world view [[cite:&sutton2020tea]] is a hallmark of continual learning [[cite:&ring1994continual;&ring1997child]]. The environment could be filled with other agents, areas of high reward or risk, or a seemingly infinite number of objectives about which to learn. The agent views the environment through a set of analog-to-digital sensors positioned in an egocentric orientation [fn:: While we take the egocentric perspective, nothing is preventing our artificial agents from accessing non-egocentric senses. A clear example of this would be a camera positioned in a different room, which can be plugged directly through the agents perceptual system directly. This is unlike a biological agent, who must observe the camera through their eyes from a screen.]. The agent must maintain a set of current beliefs about the world internally. This set of beliefs (also known as a summary of the agent's history) is referred to as *agent-state* (we use state and agent-state interchangeably, making sure to emphasize when discussing the environment state where necessary). The agent's life goal is to maximize the cumulation of *rewards* [fn:: In machine intelligence, the *reward* is used as a broad term to encompass both positive and aversive outcomes. While these signals are often combined in MI research, there is evidence to suggest they are encoded and processed in organic agents differently [[cite:&niv2009reinforcement]]] either internally or externally defined [[cite:&silver2021reward]].


The *practitioner* is you and me. Specifically, a practitioner is one who is creating the *agent* for an environment. In machine intelligence (MI) research, the goal is to construct agents which can behave in an environment, usually to accomplish a specific goal set by the practitioner. We seek solutions, algorithms, and systems which can do solve goals with as little imbued assumptions by the practitioner as possible [[cite:&sutton2019bitter]]. One possible way to accomplish this goal is through setting goals for the agent in terms of reward functions. This form of machine intelligence has been discussed before with a hypothesis termed "Reward is Enough". This hypothesis has been used to form a centralized definition of intelligence and conjectures on how to create such an intelligence through designing reward functions [[cite:&sutton2020john;&silver2021reward]]. While it is typical for the practitioner to construct an explicit reward function for the agent, the agent can also use internal signals to drive its behavior. This is typically known as intrinsic motivations. In this thesis, we use hand constructed reward functions to gain insights into the algorithms and architectures we explore, but overall we are interested in an agent's ability to predict and control its stream of experience. In all, we make sure our approaches provide better prediction as well as lead to maximizing cumulative reward. Given the right experiences (i.e. behavior in the environment) an agent should be able to accurately make predictions in a computationally constrained way. For more on ways to improve behavior for learning predictions see [[cite:&mcleod2021continual]].


# One common example consistently used in this thesis is that of a small robot on wheels behaving in a room. This is a common example throughout MI and RL research and is also known as a vacuum robot [[cite:&russell2010artificial]], or lovingly known as the critterbot [[cite:&sutton2011horde;&modayil2014multitimescale;&white2015developing]]. While we don't explore the full robotic setting, we often use simulations which approximate this setting to ask fundamental questions about the learning process. The hallmarks of this setting include a squat, mostly circular robot with wheels. This robot has a large set of egocentric sensors including light sensors, a camera, a bump sensor, and many more senses to view its environment. It can roll forward and backwards and rotate clockwise or counterclockwise. This agent lives continuously and has several potential goals depending on the practitioner's desires. Approximations of this example can be seen in several experiments throughout this thesis (Section ref:sec:arnn:experiments, and Chapters ref:chap:gvfn:empirical, ref:chap:composite, ref:chap:gvfn:discovery).

*************** DONE [#B] GVFs set the stage for expanding the set of predictive targets, but we must constrain. :noexport:
CLOSED: [2023-04-19 Wed 11:18]
*************** END

General value functions [[cite:&sutton2011horde;&white2015developing]] provide a concrete framework for proposing predictive questions. This framework extends the set of predictions which can be learned through temporal-difference style learning algorithms to any real-valued signal available to the agent. But often this framework is discussed from the perspective of the practitioner, where privileged information can be incorporated into the parameter functions. This can be seen in much of the work above. While this is a convenient strategy to incorporate expert knowledge into the agent's representation---which should be done if it is available for applications---in many settings this information is either costly to access or entirely unavailable. All of the agent's question parameters should be derived from the observations or signals the agent reliably has access to. In other words, all parameter functions are mappings from the agent's perceptual system to real-values. For event based terminations and cumulants, the agent must be able to recognize when an event is happening from its perceptual system alone.

While I advocate for constraining the set of functions used to define the question parameter functions, learning predictions of any real-valued signal the agent has access to also opens the possibility of asking composite predictive questions [[cite:&white2015developing]]. A composite question is one whose target is dependent on another prediction internal to the agent. Compositions expand the possible range of predictive questions we can specify as a GVF [[cite:&sutton2005temporaldifference;&rafols2006temporal;&zheng2021learning;&white2015developing;&schlegel2021general]]. While this may suggest the GVF framework is limited in what questions can be asked, the limitations are necessary so the predictions can be trained /independent of span/ [[cite:&vanhasselt2015learning]]. Learning independent of span means the target can be learned using online algorithms regardless of the effective horizon of the prediction. Adding layers of composite questions have improved the learning in predictive representations [[cite:&rafols2006temporal;&schlegel2021general]], and improved the performance of deep reinforcement learning through auxiliary tasks [[cite:&zheng2021learning]]. In the automatic specification of learning targets compositions are thought to provide a way for the agent to build complexity [[cite:&schlegel2021general;&veeriah2019discovery;&zheng2021learning;&kearney2022what]], but often these architectures don't leverage compositions for stability concerns [[cite:&tanner2005temporal;&tanner2005td;&schlegel2017stable;&schlegel2021general]]. 

As well as improving behavior empirically, compositions can provide semantic depth. An excellent example of this can be seen in option-extended temporal difference networks [[cite:&rafols2006temporal]], and later explored again in [[cite:&schlegel2021general]]. The example is centered in an environment where the agent has a low-powered visual sensor and needs to learn its directionality from the painted walls. Each cardinal direction has a different colored wall. The first layer of predictions the agent makes is to predict what color it will observe if it were to drive straight. The second layer are myopic predictions which ask what the first layer's prediction will be after turning clockwise (or counter-clockwise). The second layer allows the agent to predict which walls are to its sides as well as the wall in the direction the agent is facing. These predictions cannot be specified in the usual GVF framework, but can be easily constructed through compositions. While this may be ``repeated information'' in a sense, the extra learning objectives makes the learning properties of the predictive representation better as compared to other specifications [[cite:&schlegel2021general]].

As algorithms for the automatic discovery of complex question networks continue to push the boundaries of what questions are considered by the agent, the properties of compositions should be better studied. When searching for what to learn the questions an agent eventually retains will be dependent on the agent's ability to learn the predictions. While it is clear questions that naturally diverge (say setting the discount $\gamma=1$) should be avoided, other problems, such as the scale of a target, could be equally as problematic when using function approximation (i.e. end-to-end neural networks). This could mean important predictions are disregarded because the agent is unable to learn the answer without proper strategies to normalize the prediction's magnitude. Better strategies for learning and normalizing predictive targets will come from understanding the effective discount schedule (or emphasis) composite predictions will have on the targets.

In this chapter, I consider the effect of compositions on the sequence of discounts, and relegate the effect of off-policy importance weights to future work. I first analyze the sequence of discounts over any number of compositions and constant discounts. I then analyze this sequence to better understand how it emphasizes parts of the data stream. Surprisingly, the effective discount for constant discount compositions have a form which can be described analytically. While this does not include the full spectrum of discount functions, it provides a first step towards understanding compositions. Next I evaluate more complex state-dependent discount functions using a simple consistent sequence and two timeseries datasets in empirical simulations. In these simulations I focus on the effect of applying the same discount function a large number of times, looking to see if the shape of the returns become regular over the compositions.
{{{c}}}
{{{c}}}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/compgvfs/seq_plus_cw.pdf}
  \caption{(\textbf{left}) The effective discount for $n$ compositions
    normalized by the maximum value found in section
    \ref{sec:compgvfs:analyze}. (\textbf{middle, right}) The cycle world
    simulations, with top graph as the cumulant and subsequent plots
    $n$ compositions with constant and terminating discounts
    respectively.} \label{fig:compgvfs:seq-cw}
\end{figure}
** Composite GVFs
:PROPERTIES:
:CUSTOM_ID: sec:compgvfs:analyze
:END:

Consider the setting where there is an infinite sequence of sensor readings $\mathbf{x} = \{x[0], x[1], \ldots,
x[t], \ldots, x[\infty]\}$ where $x[i] \in [x_{\text{min}} , x_{\text{max}}]$ and a constant discount $\gamma$. The return of this signal starting at a time step $t$ is
{{{c}}}
$V[t] = \sum_{k=0}^\infty x[k] \gamma[k-t]$
{{{c}}}
where $\gamma[k] = \gamma^{k-1}$ for $k >= 1$ and $0$ otherwise. This framing of the return is slightly different from the typical presentation. Specifically, the return can be interpreted as a convolution beteween $\gamma$ and $x$ [fn:: In digital signal processing [[cite:&oppenheim2010discrete]] often the convolution, in this case $\gamma$, is mirrored across $t$ and the inifinte sequence of sensor readings is $\mathbf{x} = \{x[-\infty], \ldots, x[t], \ldots, x[\infty]\}$. The corresponding convolution would be $V[t] = \sum_{k=-\infty}^\infty x[k] \gamma[t-k]$ which would change how the sequence of $\gamma$ is defined. To be consistent with the reinforcement learning literature, $\gamma$ is implicitly defined as the mirrored version and only consider the sequence starting at $k=0$.]
and shift the discount sequence over the sensor readings. This implicitly defines an infinite sequence of predictions $V[t]$. In the above equation, the sequence $x$ is replaced with the sequence of predictions $V$ getting a new set of predictions, and for any number of compositions $n$
{{{c}}}
$V^n[t] = \sum_{k=0}^\infty V^{n-1}[k] \gamma[k-t]$.
{{{c}}}
Expanding this equation the general sequence of effective discounts for $n$ compositions and the corresponding return can be defined as
{{{c}}}
{{{c}}}
\[
  \gamma^n[k] = \begin{cases}
    0 \quad \mbox{ if } k < n \\
    \frac{\prod_{i=1}^{n-1} (k-i)}{(n-1)!} \gamma^{k - n}
  \end{cases} \quad\quad V^n[t] = \sum_{k=0}^\infty x[k] \gamma^n[k - t]
\]
{{{c}}}
where $\gamma^1[k] = \gamma[k]$ defined above and $V^n[t]$ is the target of the $n$th composition at timestep $t$. For any value $n$ there are two sequences multiplied together. The original discounting shifted by the number of applications $\gamma^1[k-n]$ and a diverging series
{{{c}}}
\[
  Q^n[k] = \frac{\prod_{i=1}^{n-1} (k-i)}{(n-1)!} = \frac{\Gamma(k)}{\Gamma(k-n+1)\Gamma(n)}
\]
{{{c}}}
where $\Gamma(k) = (k-1)!$ for $k \in \mathbb{Z}$ is known as the Gamma function, and can be used to analyze the function with $k \in \mathbb{R}$.

For any particular application of the convolution $\gamma$ on a series with known domain $[x_{\text{min}} , x_{\text{max}}]$ the value function can take values bounded by $V^1[t] \in [\frac{x_{\text{min}}}{1-\gamma}, \frac{x_{\text{max}}}{1-\gamma}]$. This extends to $n$ compositions in a straightforward way where the range of the value function becomes $V^n[t] \in [\frac{x_{\text{min}}}{(1-\gamma)^n}, \frac{x_{\text{max}}}{(1-\gamma)^n}]$. While normalizing the value function to take values within in the range $[0,1]$ has been used in various settings [[cite:&schlegel2021general]], as more compositions are added the effective range of values shrinking considerably.

Given the effective discounting sequence above, which observations are emphasized in the predictions can be uncovered. The first 100 steps of the effective discount function for several values of $n$ can be seen in figure ref:fig:compgvfs:seq-cw. These sequences are normalized to be in the range $[0,1]$ for a visual comparison. The emphasis becomes increasingly spread as $n$ increases, with the peak of this function moving further to the future at a consistent rate.

To find the maximum value, take the derivative of the log of the sequence with respect to $k$ getting
{{{c}}}
\[\frac{\delta}{\delta k} \ln \gamma^n[k] = \psi(k) - \psi(k-n+1) + \ln\gamma\]
{{{c}}}
where $\psi(z+1) = H_{z} - C$ is the digamma function, $H_{z} = \sum_{i=1}^z \frac{1}{i} \leq \int_{1}^z \frac{1}{x} dx = ln(z)$ is the Euler harmonic number, and $C$ is the Euler-Mascheroni constant. Using the approximation above, the maximal value is (to an approximation) $k = hn - (h-1)=h(n-1)+1$, where $h=\frac{1}{1-\gamma}$ is sometimes known as the horizon of discount $\gamma$. Of course this is an approximation from above and the real value falls in $k \in [h(n-1), h(n-1) + 1)]$.

** Empirical observations

While one can describe the effective discount for composing constant discount predictions, the same techniques are difficult to apply to a non time-invariant discount (i.e. state-dependent discounts [[cite:&white2015developing;&white2017unifying;&sutton2011horde]]). Instead, in this section I look at the ideal returns of various signals using constant discounting and a terminating discounting functions. I use three datasets moving from highly synthetic to real-world robot sensori-motor data. The goal of this section is to show the non-intuitive behavior of compositions to motivate further analysis and exploration. All code can be found at \url{https://github.com/mkschleg/CompGVFs.jl}. Below $\gamma = 0.9$ unless otherwise stated.
{{{c}}}
{{{c}}}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/compgvfs/mso_cb.pdf}
  \caption{(\textbf{left two}) Returns of the multiple sinusoidal
    oscillator (MSO) synthetic data set with constant and terminating
    discount respectively. The gray vertical lines are where the
    return terminates. (\textbf{right two}) Returns of Critterbot
    data set over the light3 sensor with constant and terminating
    discount respectively.} \label{fig:compgvfs:critterbot}
\end{figure}
{{{c}}}
{{{c}}}
The first series is based off the cycle world, where the agent observes a sequence of a single active bit followed by $9$ inactive bits, where the length of the sequence is $m=10$. The cumulant is the observation itself, and in this section all learning is done through TD($\lambda=0.9$) with learning rate $\alpha=0.1$ and an underlying tabular representation where each component is the place in the sequence. Two chains of compositions were trained. The first is that of the continuous discounting described above, and the second is a series of discounts which terminate (i.e. $\gamma[t]=0$) when the observation is active. The predictions of a single run can be seen in figure ref:fig:compgvfs:seq-cw. For the constant discount, as the number of compositions increases the prediction sequence converge to what looks to be a sinusoid with frequency of $10$, and amplitude driven by the analysis above. For the terminating discount, the wave form is more interesting. The first layer of predictions look very similar to the constant discount with amplitude shifted by $\frac{\gamma^{m}}{1 - \gamma^{m}}$. But as there are more compositions the effect seems to be the prediction is at its height farther away from the active bit. As the agent gets closer to the observation, the sequence of summed values is shorter leading to smaller values. Given the sequence used, it is easy to mistake this as the agent creating a trace of the cumulant, but remember the prediction is about future cumulants.

Next the Critterbot dataset [[cite:&modayil2014multitimescale;&white2015developing]] is used, focusing on light sensor 3 in figure ref:fig:compgvfs:critterbot. This gives a sequence of spikes similar to the cycle world sequence and a long pause in-between consistent saturations of the light sensor. The predictions look more like shifted and spread spikes as compared to the cycle world results. But with many more compositions, the return reverts to a similar form as before. The terminating discounts (with termination at sensor saturation $x[t+1] > 0.99$) provides a nice demonstration of how the returns are predicting the signal, just with a decaying prediction instead of the usual growing prediction. The results are similar in the multiple sinusoidal oscillator [[cite:&jaeger2004harnessing]]. A slightly different terminating discount is used where the return terminates when the previous normalized prediction is $y^{n-1}[t+1] > 0.9$ rather than when the observation is saturated. While there are decays as the MSO sequence peaks, as the depth of the composition is increased, these periods are less frequent. Deep compositions may indicate parts of the sequence where there are fewer saturations in the original sequence.

** Future Directions

This work suggests a number of interesting research directions and questions. While I mostly analyzed the sequence on discrete steps and applications of the filter, the general form does lend itself to continuous and complex values of $n$ and $k$. In a similar vein, I focused on real valued exponential discounting while several discounting schemes exist which could be applied to our formulation. I am particularly interested in complex discounting [[cite:&deasis2018predicting]] and hyperbolic discounting [[cite:&ainslie1992hyperbolic;&fedus2019hyperbolic]]. Applying a diverse set of discounting schemes in compositions provide an interesting way to extend the power of value functions while maintaining learnability through efficient algorithms like temporal-difference learning. 

The approach used in this chapter is unable to analyze state-dependent discount functions. One way around this might be in analyzing truncated sequences and taking an expectation over a distribution of sequence lengths. This might lead to a expected effective discounting sequence, but how this will interact with an underlying Markov process is unclear. This is an important next step for understanding the effects of compositions in general value functions, and could also help in analyzing off-policy compositions.

Finally, the return can be re-interpreted as a convolution over the infinite sequence of observations. While this interpretation was only used to better the notation in this manuscript, further connections to convolutions and digital signal processing should be explored. Better filter designs might inspire different discounting schedules to squeeze more information from the data stream. I have also only analyzed these convolutions in the time domain. The frequency domain might give us more insight into how consistent signals like the cycle world dataset will be effected by compositions. 
* Discovery of GVFNs through Generate and Test
:PROPERTIES:
:CUSTOM_ID: chap:gvfn:discovery
:END:

In Chapter ref:chap:gvfn:empirical, I explored several hand designed network configurations. While a necessary first step to judge the GVFN's potential, discovering the predictive questions used to construct the state is essential to more aptly apply GVFNs to large complex problems. In chapter ref:chap:composite, I discussed the considerations needed when designing cumulant, discount, and policy functions for GVFs in a continual learning agent. In this chapter, I consider the discovery question for GVFNs in a continual learning setting. While this chapter only serves as the tip of the iceberg of what is possible with discovery, I aim to provide a baseline and structure to approach the question in future work. I also describe several approaches to discovery used throughout the predictive learning literature and discuss how they might apply to the GVFN architecture directly.

Previous approaches to discovery in predictive representations have focused on finding a set of predictions that would enable the agent to answer all predictive questions accurately. This objective is trying to find a sufficient statistic of the history for all predictions, and has been discussed in various forms cite:&subramanian2022approximate. This is the approach typically taken in PSRs and a usual criteria when approaching a POMDP problem. This criteria falls naturally from the POMDP specification, where the assumption is there is a true underlying latent state which the agent can determine from enough interactions with the system.  I conjecture that finding such a state is not feasible in large complex problems, and searching for such a state would be a poor use of a finite set of computational resources. Instead, the agent should focus on finding a set of questions which is useful for the agents overarching goals---for example, maximizing the return in the control problem. In the following section, I describe several prior approaches to discovery applicable to the GVFN framework, develop a simple approach to a discovery framework for future testing, and discuss various ways of specifying GVFs by hand for the GVFN.

** Previous Approaches

There are two main families of approaches to discovery of GVFs for GVFNs: generate-and-test and gradient descent.


*Generate and test*
is a natural algorithmic approach when considering a search problem through a complex unordered (or not obviously ordered) space. The core of the approach is to propose GVFs through a generator and approximate their utility for the downstream task through a proxy measure. This approach has been used for representation discovery cite:&mahmood2013representation,javed2020learning. The simplest setting where such a generate-and-test approach could be used is time series forecasting, as the predictions are on-policy and so policies do not have to be proposed by the generated. Further, practitioners can apply their prior knowledge in creating the cumulant and continuation functions considered by the generator. There are, however, some simple strategies for generating policies, which is discussed in Section ref:sec:gvfn:simple-disc.

A generate and test algorithm has been developed for TD networks [[cite:&makino2008online]]. The process of discovery involves creating new predictions built entirely from existing structures: senses or predictions. By building new predictions from existing predictions, it facilitates the creation of composite structures. The system proposed in citeA:&makino2008line determines when a node (i.e. a prediction or sense) should be expanded on using three criteria. They then expand these nodes in specific ways to ask a broad set of composite questions. TD networks do not include policies---rather they include action primitives---so the approach does not directly extend. However, the idea of iteratively creating such composite structures does extend. For example, in this work, the expert network considered in Section ref:sec:gvfn:emp:poorlyspecified was composed of composite GVFs. Composite GVFs could be generated simply by using existing GVFs as the cumulant for the new GVF.


*Meta-gradient descent* uses gradient descent to learn meta-parameters that affect learning performance. The meta-parameters could correspond to initialization of a model for later fine tuning [[cite:&finn2017modelagnostic]], a set of GVF auxiliary tasks to improve representation learning in Atari cite:&veeriah2019discovery or parameterized options [[cite:&bacon2017option]]. This approach splits the problem into two optimization problems: an inner problem and an outer problem. The inner optimization consists of the usual control or prediction procedure, where the agent seeks to maximize the discounted return or lower prediction error. The outer optimization calculates gradients through this procedure, with respect to the meta-parameters.

For example, to learn a set of GVFs as auxiliary tasks, citeauthor:&veeriah2019discovery (citeyear:&veeriah2019discovery) parameterized the cumulant and continuation functions. They did not need to parameterize the policies for the GVFs as they assumed on-policy prediction: the policy \(\pi\) for the GVF is the current policy. These meta-parameters are optimized in the outer loop to produce auxiliary tasks that improve control performance in the inner loop. For this setting, one could similarly parameterize GVF questions, including the policy. This meta-gradient approach was reasonably effective for discovering GVFs as auxiliary tasks, though the procedure is expensive and has some trainability issues. Nonetheless, it is a reasonable direction for pursuing discovery for GVFNs.

** Investigating a Simple Generate and Test Strategy for GVF Discovery
:PROPERTIES:
:CUSTOM_ID: sec:gvfn:simple-disc
:END:


\begin{wrapfigure}{R}{0.5\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{plots/gvfn/discovery/evaluator_generator.pdf}
  \caption{The discovery framework.}\label{fig:gvfn:discovery}
\end{wrapfigure}
This simple discovery framework is based on algorithms described for representation
search cite:&mahmood2013representation focusing on two main components: an
evaluator, and a generator. The evaluator is responsible for testing GVFs and
removing unused GVFs. The generator proposes new GVFs from a set of possible
GVFs. The framework is summarized in Figure ref:fig:gvfn:discovery. The key questions are how GVFs are evaluated and how new ones proposed. Our goal here is simply to demonstrate one avenue for discovery in GVFNs, rather than to develop an algorithm for discovery; this section opts for what are some of the simplest choices.


The magnitude of the associated weight in the external tasks using the GVFN is used to evaluate a GVF. The state vector is assumed to be used linearly to make predictions, with $\theta_j$ corresponding to state $s_j$ and so to the $j$th GVF. All GVFs are evaluated every $N \in \Naturals$ steps and prune the lowest $\epsilon \in [0,1]$ percentage, i.e., prune $\lfloor n \epsilon \rfloor$ least useful GVFs of the full set of $\numgvfs$ GVFs. Other criteria have been proposed for evaluation, such as using traces of the weight magnitudes and considering internal weights cite:&mahmood2013representation. As mentioned above, the simplest choice that is still reasonably effective is opted for over more complex evaluation metrics.


New GVFs are generated randomly from a set of GVF primitives. A set of basic types of cumulants, continuations and policies are randomly sampled from. For continuations, the set includes
\textit{myopic discounts} ($\gamma = 0$),
\textit{horizon discounts} ($\gamma \in (0,1)$) and
\textit{terminating discounts} (the discount is set to $\gamma \in (0,1]$ everywhere, except for at an
event, which consists of a transition $(o, a, o')$).
For cumulants, the set includes
\textit{stimuli cumulants} (the cumulant is one of the observations,
or taking on 0 or 1 if the observation fulfills some criteria (e.g. a threshold))
and \textit{composite cumulants} (the cumulant is the prediction of another GVF).
\textit{Random cumulants} (the cumulant is a random number generated from a zero-mean Gaussian with a random variance sampled from a uniform distribution) are also included; these are not expected to be useful, but rather use it to define what is called here a dysfunctional GVF to test pruning.
For the policies, \textit{random policies} (an action is chosen at random) and
\textit{persistent policies} (always follows one action) are used.

The resulting GVF primitives consist of a triplet $(c, \gamma, \pi)$ where each is randomly chosen from these basic types. For example, a randomly generated GVF could consist of a myopic continuation, a stimuli cumulant on observation bit one and a random policy. This would correspond to predicting the first component of the observation vector on the next step, assuming a random action is taken. As another example, a randomly generated GVF could consist of a termination continuation with $\gamma = 0.9$, a stimuli cumulant which is 1 when the observation is zero and is otherwise zero otherwise and a persistent policy with action forward. This GVF corresponds to predicting the likelihood of seeing the observation change from active (`1') to inactive (`0'), given the agent persistently moves forward, within the horizon of about $(1-\gamma)^\inv = 10$ steps.

Parameterized continuations, cumulants and policies and randomly could have been considered. This set, however, is large. The GVF primitives can be seen as a prior over the full set of GVFs, which is too large from which to randomly generate. Without this prior the discovery approach is expected to still work but to take even longer than the experiments presented below.

The performance of the framework is evaluated on two experiments in Compass World cite:&sutton2005temporaldifference. Both experiments use the five hard-to-learn GVFs as the targets for the GVFN, introduced in Chapter ref:chap:gvfn:empirical. These questions correspond to a question of ``which wall will I hit if I move forward forever?''.
The first experiment, Figure ref:fig:gvfn:compass-disc (left), provides a sanity check that the evaluation strategy prunes dysfunctional representational units. The GVFN is initialized with 200 GVFs: 45 used to form the expert crafted TD network cite:&sutton2005temporaldifference, and 155 defective GVFs predicting noise $\sim \mathcal{N}(0,\sigma^2)$. In Figure ref:fig:gvfn:compass-disc the learning curve and pruned GVFs over 12 million steps are reported. The second experiment, Figurere f:fig:gvfn:compass_disc (right), uses the full discovery approach to find a representation useful for learning the evaluation GVFs. The learning curves of the evaluative GVFs over 100 million steps are reported.

These experiments have many similarities to the experiments above, but there is one key differences worth noting. Instead of using RTD or RGTD, TD($\lambda$) is used; see Appendix ref:sec:gvfn:tdlambda for the update equations. TD($\lambda$) is sufficient to learn the expert network specification in a reasonable number of steps, and is significantly simpler than the other algorithms. Note that TD($\lambda$) was not used in the above comparisons with RNNs. This was excluded for two reasons. First in the cases where the target was not a return, it is not possible to use eligibility traces, as they are designed for predicting expected returns. Second, as far as I am aware, the eligibility trace calculation for neural networks with several output nodes has not been formally derived nor tested.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{plots/gvfn/discovery/combined_cull_gen.pdf}
  \caption{\textbf{(left)} Pruning predictive units occurs every million steps with no regeneration $\alpha=0.001, \lambda=0.9, \epsilon=0.1, \sigma^2 = 1$ \textbf{(right)} Learning curves of the evaluative GVFs $N = 1000000, \epsilon=\text{labeled}, \alpha=0.001, \lambda=0.9, n = 100$, over 5 runs with standard error denoted by the shaded region.
    }\label{fig:gvfn:compass-disc}
\end{figure}

The results indicate that even a simple generate and test approach can be effective for discovery in GVFNs. The first figure shows that the pruning approach gradually removes the dysfunctional GVFs, without pruning the expert GVFs. Eventually, once the agent has mostly removed all the dysfunctional GVFs, it is then forced to prune the expert GVFs and prediction performance begins to drop. Of course, in practice, the agent would not prune all its GVFs; in this experiment pruning is simply continued until the end.
The second plot shows that iteratively pruning and generating new GVFs significantly improves on using an initial random set. For $\epsilon = 0.2$, which means about \(20\%\) of GVFs are pruned in each pruning phase, the prediction error continues to decrease until it almost reaches 0 and is almost as good as the set of hand-design GVFs used in previous experiments.

The goal of this experiment was to answer: is it possible to discover useful GVFs for a GVFN, even in simple settings? A negative answer would mean that GVFNs might have limited applicability. A  demonstration that it is possible provides some evidence that this is a tractable problem for which even simple solutions can help us make traction. This demonstration, however, by no means shows an ideal or even efficient algorithm and there is ample room for improvement. Primarily, the random generation strategy does not take into account the current set of proposed predictions, potentially resulting in redundancy. A more principled method would look to generate a wide variety of predictions dependent on the current set of predictions.
%This would involve measuring how related GVF questions are from their specification is not particularly straightforward.
Another issue is the proxy used to determine a prediction's usefulness. Currently, the system will prune GVFs that are not directly useful, even if they are the cumulant for a useful GVF. The cumulant for the useful GVF is replaced by a new random GVF. This could reduce the quality of the predictive state or cause other instabilities within the GVFN. A simple approach is to define usefulness based also on compositional utility, not just on utility for the prediction task. The usefulness of a GVF should be higher if it is used by a GVF that is itself heavily relied on for accurate predictions, versus if it is only used by less useful GVFs.

** Heuristics to specify GVFNs
Through testing GVFNs in several domains there are some rules of thumb for choosing GVFs which can be applied today. In the time-series experiments, selecting GVFs with constant \(\gammaj{j} \in [1 - 2^{-j}]\) is surprisingly effective across the settings with fixed policies---namely the time series datasets. This is encouraging as these specifications on the surface seem simpler to discover than something as complex as the Expert network in Compass World. A set of discounts selected linearly across a range was also effective. Also including GVFs which have a pseudo-termination at a known event (known due to expert knowledge) and a cumulant which is only active at this event improved learning performance considerably (see the performance of the Terminating-Horizon network in Section ref:sec:gvfn:emp:poorlyspecified).

* IN-PROGRESS [#B] Importance Resampling for Prediction in Reinforcement Learning
:PROPERTIES:
:CUSTOM_ID: chap:resampling
:END:

One lingering issue that plagues not only GVFNs but the community of learning answers to predictive questions generally is the variance induced by off-policy prediction through importance sampling. There have been several alternatives to straight importance sampling ratios, many of which have been proposed for a more standard policy evaluation setting (i.e. where the target and behavior policies are close together on the simplex). In this Chapter, I propose importance resampling as a off-policy prediction algorithm to mitigate the update variance of importance sampling while also being a consistent estimator. I characterize the bias of this new estimator. Finally, I empirically investigate IR on three microworlds and a racing car simulator, learning from images, highlighting that (a) IR is less sensitive to learning rate than IS and V-trace (IS with clipping) and (b) IR converges more quickly in terms of the number of updates.

** Introduction

An emerging direction for reinforcement learning systems is to learn many predictions, formalized as value function predictions contingent on many different policies. The idea is that such predictions can provide a powerful abstract model of the world. Some examples of systems that learn many value functions are the Horde architecture composed of General Value Functions (GVFs) [[cite:&sutton2011horde;&modayil2014multitimescale]], systems that use options [[cite:&sutton1999mdps;&schaul2015universal]], predictive representation approaches [[cite:&sutton2005temporaldifference;&schaul2013better;&silver2017predictron;&schlegel2021general]] and systems with auxiliary tasks [[cite:&jaderberg2017reinforcement]].
Off-policy learning is critical for learning many value functions with different policies, because it enables data to be generated from one behavior policy to update the values for each target policy in parallel. 

The typical strategy for off-policy learning is to reweight updates using importance sampling (IS). For a given state $\state$, with action $\action$ selected according to behavior $\mu$, the IS ratio is the ratio between the probability of the action under the target policy $\pi$ and the behavior: $\frac{\tpolicy(\action|\state)}{\bpolicy(\action|\state)}$. The update is multiplied by this ratio, adjusting the action probabilities so that the expectation of the update is as if the actions were sampled according to the target policy $\pi$. Though the IS estimator is unbiased and consistent [[cite:&kahn1953methods;&rubinstein2016simulation]], it can suffer from high or even infinite variance due to large magnitude IS ratios, in theory [[cite:&andradottir1995choice]] and in practice [[cite:&precup2001offpolicy;&mahmood2014weighted;&mahmood2017multistep]].


There have been some attempts to modify off-policy prediction algorithms to mitigate this variance.[fn:resampling1]
Weighted IS (WIS) algorithms have been introduced cite:&precup2001offpolicy;&mahmood2014weighted;&mahmood2015off, which normalize each update by the sample average of the ratios. These algorithms improve learning over standard IS strategies, but are not straightforward to extend to nonlinear function approximation. In the offline setting, a reweighting scheme, called importance sampling with unequal support [[cite:&thomas2017importance]], was introduced to account for samples where the ratio is zero, in some cases significantly reducing variance.  
Another strategy is to rescale or truncate the IS ratios, as used by V-trace [[cite:&espeholt2018impala]] for learning value functions and Tree-Backup [[cite:&precup2000eligibility]], Retrace [[cite:&munos2016safe]] and ABQ [[cite:&mahmood2017multistep]] for learning action-values. Truncation of IS-ratios in V-trace can incur significant bias, and this additional truncation parameter needs to be tuned.


An alternative to reweighting updates is to instead correct the distribution before updating the estimator using weighted bootstrap sampling: resampling a new set of data from the previously generated samples [[cite:&smith1992bayesian;&arulampalam2002tutorial]]. Consider a setting where a buffer of data is stored, generated by a behavior policy. Samples for policy $\tpolicy$ can be obtained by resampling from this buffer, proportionally to $\frac{\tpolicy(\action|\state)}{\bpolicy(\action|\state)}$ for state-action pairs $(\state,\action)$ in the buffer. 
In the sampling literature, this strategy has been proposed under the name Sampling Importance Resampling (SIR) [[cite:&rubin1988using;&smith1992bayesian;&gordon1993novel]], and has been particularly successful for Sequential Monte Carlo sampling [[cite:&gordon1993novel;&skareimproved]]. Such resampling strategies have also been popular in classification, with over-sampling or under-sampling typically being preferred to weighted (cost-sensitive) updates [[cite:&lopez2013insight]].

A resampling strategy has several potential benefits for off-policy prediction [fn:resampling2]. Resampling could even have larger benefits for learning approaches, as compared to averaging or numerical integration problems, because updates accumulate in the weight vector and change the optimization trajectory of the weights. For example, very large importance sampling ratios could destabilize the weights. This problem does not occur for resampling, as instead the same transition will be resampled multiple times, spreading out a large magnitude update across multiple updates. On the other extreme, with small ratios, IS will waste updates on transitions with very small IS ratios. By correcting the distribution before updating, standard on-policy updates can be applied. The magnitude of the updates vary less---because updates are not multiplied by very small or very large importance sampling ratios---potentially reducing variance of stochastic updates and simplifying learning rate selection. I hypothesize that resampling (a) learns in a fewer number of updates to the weights, because it focuses computation on samples that are likely under the target policy and (b) is less sensitive to learning parameters and target and behavior policy specification.

[fn:resampling1] There is substantial literature on variance reduction for another area called off-policy policy evaluation, but which estimates only a single number or value for a policy (e.g., see [[cite:&thomas2016dataefficient]]). The resulting algorithms differ substantially, and are not appropriate for learning the value function.

[fn:resampling2] We explicitly use the term prediction rather than policy evaluation to make it clear that we are not learning value functions for control. Rather, our goal is to learn value functions solely for the sake of prediction.

** Background

This chapter considers the problem of learning General Value Functions (GVFs) [[cite:&sutton2011horde]]. The agent interacts in an environment defined by a set of states $\States$, a set of actions $\Actions$ and Markov transition dynamics, with probability $\Pfcn(\state'|\state,\action)$ of transitions to state $\state'$ when taking action $\action$ in state $\state$. A GVF is defined for policy $\pi: \States \!\times \!\Actions \!\rightarrow\! [0,1]$, cumulant $\cumul: \States\! \times \!\Actions \!\times\! \States\! \rightarrow\! \RR$ and continuation function $\gamma: \States \!\times\! \Actions \!\times \!\States \rightarrow [0,1]$, with $\cumulr_{t+1} \defeq  \cumul(\stater_t, \actionr_t, \stater_{t+1})$ and  $\gamma_{t+1} \defeq  \gamma(\stater_t, \actionr_t, \stater_{t+1})$ for a (random) transition $(\stater_t, \actionr_t, \stater_{t+1})$. The value for a state $s \in \States$ is
{{{c}}}
{{{c}}}
\begin{align*}
  \Value(\state) \defeq \mathbb{E}_\pi\left[ G_t | \stater_t = \state \right] &
&\text{where }  G_t \defeq \cumulr_{t+1} + \gamma_{t+1} \cumulr_{t+2} + \gamma_{t+1} \gamma_{t+2} \cumulr_{t+3} + \ldots
.
\end{align*}
{{{c}}}
The operator $\mathbb{E}_\pi$ indicates an expectation with actions selected according to policy $\pi$. GVFs encompass standard value functions, where the cumulant is a reward.
Otherwise, GVFs enable predictions about discounted sums of others signals into the future, when following a target policy $\pi$. 
These values are typically estimated using parametric function approximation, with weights $\theta \in \RR^d$ defining approximate values $\Value_\theta(\state)$. 

In off-policy learning, transitions are sampled according to behavior policy, rather than the target policy. 
To get an unbiased sample of an update to the weights, the action probabilities need to be adjusted. Consider on-policy temporal difference (TD) learning, with update \(\alpha_t\delta_t\nabla_\theta \Value_{\theta}(s)$ for a given $S_t = s\), 
for learning rate $\alpha_t \in \RR^+$ and TD-error $\delta_t \defeq C_{t+1} + \gamma_{t+1}\Value_{\theta}(S_{t+1}) -  \Value_{\theta}(s)$. 
If actions are instead sampled according to a behavior policy $\mu: \States \times \Actions \rightarrow [0,1]$, then importance sampling (IS) to modify the update, giving the off-policy TD update $\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\theta}(s)$
for IS ratio $\rho_t \defeq \frac{\tpolicy(\actionr_t | \stater_t)}{\bpolicy(\actionr_t | \stater_t)}$. 
Given state $\stater_t = \state$, if $\mu(a | s) > 0$ when $\pi(a | s) > 0$, then the expected value of these two updates are equal. To see why, notice that
{{{c}}}
\[
  \mathbb{E}_\mu\left[\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\theta}(s) |S_t = s\right]
  =  \alpha_t\nabla_\theta \Value_{\theta}(s)\mathbb{E}_\mu\left[\rho_t\delta_t |S_t = s\right]
\]
{{{c}}}
which equals $\mathbb{E}_\pi\left[\alpha_t\rho_t\delta_t\nabla_\theta \Value_{\theta}(s) |S_t = s\right]$ because
{{{c}}}
\[
\mathbb{E}_\mu\left[\rho_t\delta_t |\stater_t = \state\right] 
= \sum_{\action \in \Actions} \mu(\action | \state) \frac{\tpolicy(\action | \state)}{\bpolicy(\action | \state)} \mathbb{E}\left[\delta_t |\stater_t = \state, \actionr_t = \action \right]
\]
{{{c}}}

Though unbiased, IS can be high-variance. A lower variance alternative is Weighted IS (WIS). For a batch consisting of transitions \(\{(s_i, a_i, s_{i+1}, c_{i+1}, \rho_i)\}_{i=1}^n\), batch WIS uses a normalized estimate for the update.
For example, an offline batch WIS TD algorithm, denoted WIS-Optimal below, would use update \(\alpha_t \frac{\rho_t}{\sum_{i=1}^n \rho_i} \delta_t\nabla_\theta \Value_{\theta}(s)\).
Obtaining an efficient WIS update is not straightforward, however, when learning online and has resulted in algorithms in the SGD setting (i.e. $n=1$) specialized to tabular [[cite:&precup2001offpolicy]] and linear functions [[cite:&mahmood2014weighted;&mahmood2015off]].
Nonetheless, WIS is used as a baseline in the experiments and theory.

** Importance Resampling 
:PROPERTIES:
:CUSTOM_ID: sec:resampling:IR
:END:
# \label{sec:resampling_offpolicy}

In this section, Importance Resampling (IR) for off-policy prediction is introduced and its bias and variance is characterized. 
A resampling strategy requires a buffer of samples, from which to resample. Replaying experience from a buffer was introduced as a biologically plausible way to reuse old experience [[cite:&lin1992selfimproving;&lin1993reinforcement]], and has  become common for improving sample efficiency, particularly for control [[cite:&mnih2015humanlevel;&schaul2015prioritized]]. In the simplest case---which is assumed here---the buffer is a sliding window of the most recent $n$ samples, $\{(s_i, a_i, s_{i+1}, c_{i+1}, \rho_i)\}_{i=t-n}^t$, at time step $t > n$. 
{{{c}}}
Samples are generated by taking actions according to behavior $\bpolicy$. The transitions are generated with probability $d_\bpolicy(s) \bpolicy(a | s) \Pfcn(s' | s, a)$, where $d_\bpolicy : \States \rightarrow [0,1]$ is the stationary distribution for policy $\bpolicy$. The goal is to obtain samples according to  $d_\bpolicy(s) \tpolicy(a | s) \Pfcn(s' | s, a)$, as if actions were sampled according to policy $\tpolicy$ from states [fn:resampling3]  $\state \sim d_\bpolicy$.

The IR algorithm is simple: resample a mini-batch of size $k$ on each step $t$ from the buffer of size $n$, proportionally to $\rho_i$ in the buffer. Using the resampled mini-batch the value function can be updated using standard on-policy approaches, such as on-policy TD or on-policy gradient TD.
The key difference to IS and WIS is that the distribution itself is corrected, before the update, whereas IS and WIS correct the update itself. This small difference, however, can have larger ramifications practically, as shown in this chapter.

Two variants of IR are considered: with and without bias correction. For point $i_j$ sampled from the buffer, let $\Delta_{i_j}$ be the on-policy update for that transition. For example, for TD, $\Delta_{i_j} = \delta_{i_j} \nabla_\theta V_\theta(s_{i_j})$. The first step for either variant is to sample a mini-batch of size $k$ from the buffer, proportionally to $\rho_i$. Bias-Corrected IR (BC-IR) additionally pre-multiplies with the average ratio in the buffer $\bar{\rho} \defeq \tfrac{1}{n} \sum_{i=1}^n \rho_i$, giving the following estimators for the update direction 
{{{c}}}
\[
\xiwer \defeq \tfrac{1}{k} \sum_{j=1}^k \Delta_{i_j} \hspace{2.0cm}
\xbciwer \defeq \tfrac{\bar{\rho}}{k} \sum_{j=1}^k \Delta_{i_j}
\]

BC-IR negates bias introduced by the average ratio in the buffer deviating significantly from the true mean. For reasonably large buffers, \(\bar{\rho}\) will be close to 1 making IR and BC-IR have near-identical updates[fn:resampling4].
Nonetheless, they do have different theoretical properties, particularly for small buffer sizes \(n\), so both are characterized. 

Across most results, the following assumption is applied.
#+name: assum:resampling:id
#+begin_assumption
A buffer $B_t = \{X_{t+1}, ..., X_{t+n}\}$ is constructed from the most recent $n$ transitions sampled by time $t+n$, which are generated sequentially from an irreducible, finite MDP with a fixed policy $\mu$.
#+end_assumption

To denote expectations under $p(x) \!=\! d_\bpolicy(s) \bpolicy(a | s) \Pfcn(s' | s, a)$ and $q(x) \!=\! d_\bpolicy(s) \tpolicy(a | s) \Pfcn(s' | s, a)$, the notation from above is overloaded, using operators $\E_\bpolicy$ and $\E_\tpolicy$ respectively. To reduce clutter, $\E$ is used to mean $\E_\bpolicy$, because most expectations are under the sampling distribution.
# All proofs can be found in Appendix  \ref{sec:theory_appendix}.

[fn:resampling3] The assumption that states are sampled from $d_\bpolicy$ underlies most off-policy learning algorithms. Only a few attempt to adjust probabilities $d_\bpolicy$ to $d_\tpolicy$, either by multiplying IS ratios before a transition [[cite:&precup2001offpolicy]] or by directly estimating state distributions [[cite:&hallak2017consistent;&liu2018breaking]]. In this chapter, resampling is used to correct the action distribution---the standard setting. However, several insights are expected extend to how to use resampling to correct the state distribution, particularly because wherever IS ratios are used it should be straightforward to use our resampling approach.

In the sections below, I provide the bias characterizations of the two estimators defined above with proofs provided in the appendix. In the appendix, you can also find a characterization of the update variance.

[fn:resampling4] \(\bar{\rho} \approx \mathbb{E}[\rho(a|s)] = \mathbb{E}[\frac{\pi(a|s)}{\mu(a|s)}] = \sum_{s,a} \frac{\pi(a|s)}{\mu(a|s)}\mu(a|s) d_{\mu}(s) = 1\).

*** Bias of IR

Below I show IR is biased, and that its bias is actually equal to WIS-Optimal, in Theorem ref:thm:resampling:bias-IR
{{{c}}}

#+name: thm:resampling:bias-IR
#+ATTR_LATEX: :options [Bias for a fixed buffer of size $n$]
#+begin_theorem
Assume a buffer $B$ of $n$ transitions sampled i.i.d according to $p(x = (s,a,s')) = d_\bpolicy(s) \bpolicy(a | s) \Pfcn(s' | s, a)$.
Let $\xwis \defeq \sum_{i=1}^{n} \frac{\rho_i}{\sum_{j=1}^n \rho_j} \Delta_i$ be the WIS-Optimal estimator of the update.
Then, 
\[
\E[\xiwer] = \E [\xwis]
\]
and so the bias of $\xiwer$ is proportional to 
\begin{equation}
\mathrm{Bias}(\xiwer) = \E [\xiwer] - \E_\tpolicy[\Delta] \propto \frac{1}{n} (\E_\tpolicy[\Delta] \sigma_\rho^2 - \sigma_{\rho, \Delta} \sigma_\rho \sigma_\Delta) \label{eq:resampling:bias}
\end{equation}
where $\E_\tpolicy[\Delta]$ is the expected update across all transitions, with actions from $S$ taken by the target policy $\tpolicy$; $\sigma_\rho^2 = \Var(\tfrac{1}{n}\sum_{j=1}^n \rho_j)$; 
$\sigma_\Delta^2 = \Var(\tfrac{1}{n}\sum_{i=1}^{n} \rho_i \Delta_i)$; and covariance $\sigma_{(\rho,\Delta)}  = \Cov(\tfrac{1}{n}\sum_{j=1}^n \rho_j,\tfrac{1}{n}\sum_{i=1}^{n} \rho_i \Delta_i)$.
#+end_theorem


Theorem ref:thm:resampling:bias-IR is the only result which follows a different set of assumptions, primarily due to using the bias characterization of $\xwis$ found in [[cite:&owen2013monte]]. The bias of IR will be small for reasonably large $n$, both because it is proportional to $1/n$ and because larger $n$ will result in lower variance of the average ratios and average update for the buffer in Equation eqref:eq:resampling:bias. In particular, as $n$ grows, these variances decay proportionally to $n$. Nonetheless, for smaller buffers, such bias could have an impact. The bias can be easily mitigated with a bias-correction term, as shown in the next corollary and proven in Appendix \ref{app_bcir_unbiased}. 
{{{c}}}
{{{c}}}
#+name: cor:resampling:bias-BCIR
#+begin_corollary
BC-IR is unbiased:  $\E [\xbciwer] = \E_\tpolicy[\Delta]$.
#+end_corollary

*************** TODO [#A] Add proof for IR bias.                 :noexport:
*************** END

*** Consistency of IR

Consistency of IR in terms of an increasing buffer, with $n \rightarrow \infty$, is a relatively straightforward extension of prior results for SIR, with or without the bias correction, and from the derived bias of both estimators (see Theorem \ref{lem:convergence_dist} in Appendix \ref{app_consistency_bign}). More interesting, and reflective of practice, is consistency /with a fixed length buffer/ and increasing interactions with the environment, $t \rightarrow \infty$. IR, without bias correction, is asymptotically biased in this case; in fact, its asymptotic bias is the one characterized above for a fixed length buffer in Theorem ref:thm:resampling:bias-IR.

#+name: thm:resampling:sliding-window
#+begin_theorem
Let $B_t = \{X_{t+1}, ..., X_{t+n}\}$ be the buffer of the most recent $n$ transitions sampled according to Assumption ref:assum:resampling:id. Define the sliding-window estimator $\xwindow{t} \defeq \frac{1}{T} \sum_{t=1}^T \xbciwerb{t}$. 
Then, if $\E_\pi [|\Delta| ] < \infty$, then $\xwindow{T}$ converges to $\E_\pi [\Delta ]$ almost surely as $T \rightarrow \infty$.
#+end_theorem

*************** TODO Add proof for sliding-window thrm           :noexport:
*************** END


** Weighted Importance Sampling
:PROPERTIES:
:CUSTOM_ID: appendix:wis
:END:

In this section, I detail the several variants of weighted importance sampling (WIS) used throughout the empirical evaluation (Section ref:sec:resampling:empirical). All of these methods are either obvious variants of WIS when using an experience replay buffer in reinforcement learning or defined previously.

# \section{Weighted Importance Sampling}\label{appendix:wis}
*** Mini-Batch Algorithms
# \subsection{Mini-Batch Algorithms}

Three weighted importance sampling updates are considered as competitors to IR. $n$ is the size of the experience replay buffer, $k$ is the size of a single batch. WIS-Minibatch and WIS-Buffer both follow a similar protocol as IS, in that they uniformly sample a mini-batch from the experience replay buffer and use this to update the value functions. The difference comes in the scaling of the update. The first, WIS-Minibatch, uses the sum of the importance weights $\rho_i$ in the sampled mini-batch, while WIS-Buffer uses the sum of importance weights in the experience replay buffer. WIS-Buffer is also scaled by the size of the buffer and brought to the same effective scale as the other updates with $\frac{1}{k}$. WIS-Optimal follows a different approach and performs the best possible version of WIS where the gradient descent update is calculated from the whole experience replay buffer. Analysis on the bias or consistency of WIS-Minibatch or WIS-Buffer is not provided, but are natural versions of WIS one might try.

\begin{align*}
  \Delta\theta &= \frac{\sum_i^k \rho_i \delta_i \nabla_\theta \Value(s_i;\theta)}{\sum_j^k\rho_j}
                 &\hspace{1cm} \text{WIS-Minibatch}\\
  \Delta\theta &= \frac{n}{k} \frac{\sum_i^k \rho_i \delta_i \nabla_\theta \Value(s_i;\theta)}{\sum_j^n\rho_j}
                 &\hspace{1cm} \text{WIS-Buffer}\\
  \Delta\theta &= \frac{\sum_i^n \rho_i \delta_i \nabla_\theta \Value(s_i;\theta)}{\sum_j^n\rho_j}
                 &\hspace{1cm} \text{WIS-Optimal}
\end{align*}

*** Incremental Algorithm
:PROPERTIES:
:CUSTOM_ID: appendix:inc_updates
:END:
# \subsection{Incremental Algorithm}\label{appendix:inc_updates}

While implementing an efficient true WIS algorithm for mini-batch updating is beyond the scope of this work, WIS-TD(0) is compared to the incremental versions of IR, IS, VTrace, and WISBatch. The difference between the mini-batch and incremental algorithms is how the updates are calculated. In the incremental scheme a random mini-batch of data is similarly sampled from the buffer. Each sample is used individually to update the value function. This is done to more naturally compare our baselines to WIS-TD(0) [[cite:&mahmood2015off]]. WIS-TD(0) has parameters $u_{0} \in \{\frac{1}{64}, 1, 5, 10, 50\} * 64, \mu \in 10^{-2:0.25:1}$, and $\eta = \frac{\mu}{u_0}$. WIS-TD(0) follows the update equations:

\begin{align*}
  \uvec_{i+1} &= (\mathbf{1} -\eta \phivec_i \circ \phivec_i) \circ \uvec_i + \rho_i \phivec_i \circ \phivec_i  \quad \triangleright \circ \defeq \text{ element wise product}\\
  \alpha_{i+1} &= \mathbf{1} \oslash \uvec_{t+1} \quad \triangleright \oslash \defeq \text{ element wise division}\\
  \bar{\delta}_i &= C_i + \gamma_i \thetavec_i^\trans\phivec_i^\prime - \thetavec_{i-1}^\trans \phivec_i \\
  \thetavec_{i+1} &= \thetavec_t +
                    \alphavec_{i+1} \circ \rho_i(\thetavec_{i-1}^\trans \phivec_i - \thetavec_i^\trans \phivec_i)\phivec_i +
                    \rho_i \bar{\delta}_i \alphavec_{i+1} \circ \phivec_i
\end{align*}

where $\thetavec \in \Reals^d$ is the weight vector of the value function, $\phivec_i \in \Reals^d$ is the feature vector of the i-th transition in the experience replay buffer, and $\phivec_i^\prime \in \Reals^d$ is the feature vector of the next state of the i-th transition in the experience replay buffer.

** Empirical Results
:PROPERTIES:
:CUSTOM_ID: sec:resampling:empirical
:END:

Two hypothesized benefits of resampling as compared to reweighting are investigated: improved sample efficiency and reduced variance. These benefits are tested in two microworld domains---a Markov chain and the Four Rooms domain---where exhaustive experiments can be conducted. Finally, a demonstration that IR reduces sensitivity over IS and VTrace in a car simulator, TORCs, when learning from images is provided[fn:: Experimental code for every domain except Torcs can be found at [[https://mkschleg.github.io/Resampling.jl][https://mkschleg.github.io/Resampling.jl]]].

IR and BC-IR are compared against several reweighting strategies, including importance sampling (IS); two online approaches to weighted important sampling, WIS-Minibatch with weighting $\rho_i/\sum_{j=1}^k \rho_j$ and WIS-Buffer with weighting $\rho_i/\tfrac{k}{\bsize}\sum_{j=1}^\bsize \rho_j$; and V-trace[fn:: Retrace, ABQ and TreeBackup also use clipping to reduce variance. But, they are designed for learning action-values and for mitigating variance in eligibility traces. When trace parameter \(\lambda = 0\)---as assumed here---there are no IS ratios and these methods become equivalent to using Sarsa(0) for learning action-values.], which corresponds to clipping importance weights [[cite:&espeholt2018impala]].
WIS-TD(0) [[cite:&mahmood2015off]] is also used as a baseline, when applicable, which uses an online approximation to WIS, with a stepsize selection strategy (as described in Appendix \ref{appendix:inc_updates}). This algorithm uses only one sample at a time, rather than a mini-batch, and so is only included in Figure \ref{fig:frc}. Where appropriate, baselines using On-policy sampling are included; WIS-Optimal which uses the whole buffer to get an update; and Sarsa(0) which learns action-values---which does not require IS ratios---and then produces estimate $V(s) = \sum_a \pi(s,a) Q(s,a)$. WIS-Optimal is included as an optimal baseline, rather than as a competitor, as it estimates the update using the whole buffer on every step.

In all the experiments, the data is generated off-policy. The absolute value error (AVE) or the  absolute return error (ARE) is computed on every step. For the sensitivity plots the average over all the interactions as specified for the environment are taken---resulting in MAVE and MARE respectively. The error bars represent the standard error over runs, which are featured on every plot---although not visible in some instances. For the microworlds, the true value function is found using dynamic programming with threshold $10^{-15}$, and AVE is taken over all the states. For TORCs and continuous Four Rooms, the true value function is approximated using rollouts from a random subset of states generated when running the behavior policy $\mu$, and the ARE is computed over this subset. For the Torcs domain, the same subset of states is used for each run due to computational constraints and report the mean squared return error (MSRE). Plots showing sensitivity over number of updates show results for complete experiments with updates evenly spread over all the interactions. A tabular representation is used in the microworld experiments, tilecoded features with 64 tilings and 8 tiles is used in continuous Four Rooms, and a convolutional neural network is used for TORCs, with an architecture previously defined for self-driving cars [[cite:&bojarski2016enda]].

*** Investigating Convergence Rate
:PROPERTIES:
:CUSTOM_ID: sec:resampling:exp-conv
:END:
# \subsection{Investigating Convergence Rate} \label{sec:exp_conv}

The convergence rate of IR is investigated first. Learning curves and sensitivity to the learning rate are reported in the Four Rooms domain. 
The Four Rooms domain [[cite:&stolle2002learning]] has four rooms in an 11x11 grid world. The four rooms are positioned in a grid pattern with each room having two adjacent rooms. Each adjacent room is separated by a wall with a single connecting hallway.
The target policy takes the down action deterministically. The cumulant for the value function is 1 when the agent hits a wall and 0 otherwise. The continuation function is $\gamma=0.9$, with termination when the agent hits a wall. The resulting value function can be thought of as distance to the bottom wall. The behavior policy is uniform random everywhere except for 25 randomly selected states which take the action down with probability 0.05 with remaining probability split equally amongst the other actions. The choice of behavior and target policy induce high magnitude IS ratios.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{{plots/resampling/fourrooms/grouped_avg_16_v2}.pdf}
  \caption{Four Rooms experiments ($n=2500$, $k=16$, 25 runs): {\bf left} Learning curves for each method, with updates every $16$ steps. IR and WIS-Optimal are overlapping. {\bf center} Sensitivity over the number of interactions between updates. {\bf right} Learning rate sensitivity plot. }\label{fig:fourrooms_tabular}
\end{figure*}


As shown in Figure \ref{fig:fourrooms_tabular}, IR has noticeable improvements over the reweighting strategies tested. 
The fact that IR resamples more important transitions from the replay buffer seems to significantly increase the learning speed. Further, IR has a wider range of usable learning rates. The same effect is seen even as the total number of updates is reduced, where the uniform sampling methods perform significantly worse as the interactions between updates increases---suggesting improved sample efficiency. WIS-Buffer performs almost equivalently to IS, because for reasonably size buffers, its normalization factor $\tfrac{1}{\bsize}\sum_{j=1}^\bsize \rho_j \approx 1$ because $\E[\rho] = 1$. WIS-Minibatch and V-trace both reduce the variance significantly, with their bias having only a limited impact on the final performance compared to IS. Even the most aggressive clipping parameter for V-trace---a clipping of 1.0--- outperforms IS. 
The bias may have limited impact because the target policy is deterministic, and so only updates for exactly one action in a state.
Sarsa---which is the same as Retrace(0)---performs similarly to the reweighting strategies.

The above results highlight the convergence rate improvements from IR, in terms of number of updates, without generalization across values. Conclusions might actually be different with function approximation, when updates for one state can be informative for others. For example, even if in one state the target policy differs significantly from the behavior policy, if they are similar in a related state, generalization could overcome effective sample size issues. Therefore, this phenomena is further investigated under function approximation with RMSProp learning rate selection.

\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\textwidth]{{plots/resampling/four_rooms_cont/train_gap/combo_v2}.pdf}
  \caption{Convergence rates in Continuous Four Rooms averaged over 25 runs with 100000 interactions with the environment. {\bf left} uniform random behavior policy and target policy which takes the down action with probability $0.9$ and probability $0.1 / 3$ for all other actions. Learning used incremental updates (as specified in appendix \ref{appendix:inc_updates}). {\bf right} uniform random behavior and target policy with persistent down action selection learned with mini-batch updates with RMSProp.
  }\label{fig:frc}
\end{figure*}
Two experiments similar to above are conducted in a continuous state Four Rooms variant. The agent is a circle with radius 0.1, and the state consists of a continuous tuple containing the x and y coordinates of the agent's center point. The agent takes an action in one of the 4 cardinal directions moving $0.5 \pm \mathcal{U}(0.0, 0.1)$  in that directions with random drift in the orthogonal direction sampled from $\mathcal{N}(0.0,0.01)$. The representation is a tile coded feature vector with 64 tilings and 8 tiles. Results are provided for both mini-batch updating (as above) and incremental updating (i.e. updating on each transition of a mini-batch incrementally, see appendix \ref{appendix:inc_updates} for details). For the mini-batch experiment, the target policy deterministically takes the down action. For the incremental experiment, the target policy takes the down action with probability $0.9$ and selects all other action with probability $0.1 / 3$.

Generalization can mitigate some of the differences between IR and IS above in some settings, but in others the difference remains just as stark (see Figure \ref{fig:frc} and Appendix \ref{appendix:four_rooms_cont}). If the behavior policy from the tabular domain, which skews the behavior in a sparse set of states, is used the nearby states mitigate this skew. However, if a behavior policy that selects all actions uniformly is used, then again IR obtains noticeable gains over IS and V-trace, for reducing the required number of updates, as shown in Figure \ref{fig:frc}.

Similar results are found for the incremental setting Figure \ref{fig:frc} (left), where resampling still outperforms all other methods in terms of convergence rates. Given WIS-TD(0)'s significant degrade in performance as the number of updates decreases, I also compare with using WIS-TD(0) when sampling according to resampling IR+WIS-TD(0). Interestingly, this method outperforms all others  --- albeit only slightly against IR with constant learning rate. This result leads us to believe RMSProp may be a optimizer poor choice for this setting. Expanded results can be found in Appendix \ref{appendix:four_rooms_cont}.

*** Investigating Variance
:PROPERTIES:
:CUSTOM_ID: sec:resampling:var-prop
:END:
# \subsection{Investigating Variance} \label{sec:var_prop}

To better investigate the update variance a Markov chain is used, this domain can more easily control dissimilarity between $\mu$ and $\pi$, and so control the magnitude of the IS ratios. The Markov chain is composed of 8 non-terminating states and 2 terminating states on the ends of the chain, with a cumulant of 1 on the transition to the right-most terminal state and 0 everywhere else. The policies with probabilities [left, right] equal in all states: $\mu = [0.9, 0.1], \pi=[0.1,0.9]$ are considered; further policy settings can be found in Appendix \ref{appendix:markov}.


First, the variance of the updates for fixed buffers is measured. I compute the variance of the update---from a given weight vector---by simulating the many possible updates that could have occurred. I am interested in the variance of updates both for early learning---when the weight vector is quite incorrect and updates are larger---and later learning. To obtain a sequence of such weight vectors, the sequence of weights generated by WIS-Optimal is used. As shown in Figure \ref{fig:markov_sens}, the variance of IR is lower than IS, particularly in early learning, where the difference is stark. Once the weight vector has largely converged, the variance of IR and IS is comparable and near zero.
\begin{wrapfigure}{r}{0.45\textwidth}
\centering
  \includegraphics[width=0.45\columnwidth]{{plots/resampling/torcs/lr_sens.pdf}}
  \caption{Learning rate sensitivity in TORCs, averaged over 10 runs. V-trace has clipping parameter 1.0. All the methods performed worse with a higher learning rate than shown here.
  }\label{fig:torcs}
\end{wrapfigure}

The update variance can be measured by proxy using learning rate sensitivity curves. 
As seen in Figure \ref{fig:markov_sens} (left) and (center), IR has the lowest sensitivity to learning rates, on-par with On-Policy sampling. IS has the highest sensitivity, along with WIS-Buffer and WIS-Minibatch. Various clipping parameters with V-trace are also tested. V-trace does provide some level of variance reduction but incurs more bias as the clipping becomes more aggressive.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{plots/resampling/markov/grouped_neurips.pdf}
  \caption{Learning Rate sensitivity plots in the Random Walk Markov Chain, with buffer size $n = 15000$ and mini-batch size $k=16$. Averaged over 100 runs. The policies, written as [probability left, probability right] are $\mu = [0.9,0.1], \pi=[0.1,0.9]$ {\bf left} learning rate sensitivity plot for all methods but V-trace. {\bf center} learning rate sensitivity for V-trace with various clipping parameters {\bf right} Variance study for IS, IR, and WISBatch. The x-axis corresponds to the training iteration, with variance reported for the weights at that iteration generated by WIS-Optimal. These plots show a correlation between the sensitivity to learning rate and magnitude of variance.
  } \label{fig:markov_sens}
\end{figure*}

*** Demonstration on a Car Simulator

The TORCs racing car simulator is used to perform scaling experiments with neural networks to compare IR, IS, and V-trace. The simulator produces 64x128 cropped grayscale images. A deterministic steering controller is used. The controller produces steering actions $a_{det} \in [-1,+1]$ and take an action with probability defined by a Gaussian $a \sim \mathcal{N}(a_{det}, 0.1)$.
The target policy is a Gaussian $\mathcal{N}(0.15, 0.0075)$, which corresponds to steering left. Pseudo-termination (i.e., $\gamma = 0$) occurs when the car nears the center of the road, and the cumulant becomes 1. Otherwise, the cumulant is zero and $\gamma = 0.9$. The policy is specified using continuous action distributions and results in IS ratios as high as $\sim 1000$ and highly variant updates for IS.


Again, IR provides benefits over IS and V-trace, in Figure \ref{fig:torcs}. There is even more generalization from the neural network in this domain, than in Four Rooms where generalization did reduce some of the differences between IR and IS. Yet, IR still obtains the best performance, and avoids some of the variance seen in IS for two of the learning rates. Additionally, BC-IR actually performs differently here, having worse performance for the largest learning rate. This suggest IR has an effect in reducing variance.

** Conclusions

In this Chapter, I introduced a new approach to off-policy learning: Importance Resampling. I showed that IR is consistent, and that the bias is the same as for Weighted Importance Sampling. I also provided an unbiased variant of IR, called Bias-Corrected IR. I empirically showed that IR (a) has lower learning rate sensitivity than IS and V-trace, which is IS with varying clipping thresholds; (b) the variance of updates for IR are much lower in early learning than IS and (c) IR converges faster than IS and other competitors, in terms of the number of updates. These results confirm the theory presented for IR, which states that variance of updates for IR are lower than IS in two settings, one being an early learning setting. Such lower variance also explains why IR can converge faster in terms of number of updates, for a given buffer of data. 

The algorithm and results in this Chapter suggest new directions for off-policy prediction, particularly for faster convergence. Resampling is promising for scaling to learning many value functions in parallel, because many fewer updates can be made for each value function. A natural next step is a demonstration of IR, in such a parallel prediction system. Resampling from a buffer also opens up questions about how to further focus updates. One such option is using an intermediate sampling policy. Another option is including prioritization based on error, such as was done for control with prioritized sweeping [[cite:&peng1993efficient]] and prioritized replay [[cite:&schaul2015prioritized]].

* DONE [#A] [8/8] Conclusions and Final Remarks

This thesis set out to uncover some of the key discrepancies between how recurrent models work in the supervised learning setting and the continual reinforcement learning setting. In this chapter, I will summarize the main contributions made to agent-state construction in the continual RL setting. I will discuss potential future directions and propose open questions and directions for developing methods of agent-state construction based on the work presented in this thesis.

** DONE Summary of Contributions
CLOSED: [2023-03-03 Fri 12:18]

The work presented in this thesis takes a deep look into using recurrent networks in the reinforcement learning setting. I focus on two assumptions brought over from supervised learning and construct an architecture to take advantage of temporal-difference learning to learn agent state. This thesis presented work to provide evidence for the following statement
#+BEGIN_QUOTE
The architectural intuitions and assumptions developed in supervised learning for agent-state search are limiting in the reinforcement learning and continual reinforcement learning settings.
#+END_QUOTE
Through controlled experimentation and a focused approach the contributions which make up this thesis show several limitations of recurrent networks as used in the supervised learning setting when applying them to the reinforcement learning and continual reinforcement learning settings.

First, the incorporation of actions and other information was investigated in the DRQN framework in Chapter ref:chap:arnn. In this chapter, I discussed several approaches to incorporate information into the cell of a recurrent network found throughout the literature. I incorporated these approaches into both simple recurrent cells and gated-recurrent units to uncover the advantages of each. I showed that in most cases we should be applying the action through a multiplicative update, and that combining architectures can be used to find the appropriate cell for a problem.

Next, I approached the assumption that the hidden units of a recurrent network should be learned through only the loss at the final layer of the network and developed a predictive approach to learning state which restricts the agent-state to predictions of the observation stream. Approaching this question resulted in a new predictive framework for agent-state construction known as /General Value Function Networks/ (Chapter ref:chap:gvfn). I explored the similarities of this approach to previous predictive approaches, and showed GVFNs encompasses the predictive targets of several previous directions.

To fully develop the predictive approach found in Chapter ref:chap:gvfn, I defined a set of learning updates following in the footsteps of gradient temporal-difference algorithms and previous predictive architectures in ref:chap:gvfn:algs. These algorithms follow naturally from the gradient temporal-difference learning updates [[cite:&maei2009convergent;&sutton2009fast]] and gradient temporal-difference networks [[cite:&silver2013gradient]]. Our learning update extends beyond these approaches to incorporate composite predictive questions into the objective as well as arbitrary discounting (with the usual limitations [[cite:&white2017unifying]]).

In Chapter ref:chap:gvfn:empirical, I empirically compared the usual approaches to agent-state construction in supervised learning with our predictive approach in several continual reinforcement learning and continual timeseries domains. I showed the incorporation of predictions which can be learned through temporal-difference learning results in a recurrent network which can learn absent of gradients rolled back through time. These predictions were also competitive to predictions made by other recurrent architectures when allowed large truncations in BPTT. I investigated several predictive questions which result in a wide range of performance and also highlight GVFs learned through temporal-difference as a key component. Finally, I reproduced a known counter example structure in ring world and showed this can be solved through the use of recurrent semi-gradient temporal difference learning rather than the full gradient update rule derived in section ref:chap:gvfn.

The empirical results discussed in ref:chap:gvfn:empirical lead to questions about the underlying targets presented by composite predictive questions and what an algorithm to discover structures like this may look like. I posed and provided initial explanations and explorations in Chapters ref:chap:composite and ref:chap:gvfn:discovery respectively. These results create a strong future research direction in developing new methods for discovering predictive questions and combing GVFs to create novel hand designed structures which might be relevant across a wide range of problems

Finally, posing off-policy prediction questions for learning an agent-state requires us to re-think the core approach to off-policy prediction. Due to traditional importance sampling can quickly result in large variance unstable updates, I proposed importance resampling as a technique to avoid the instability induced by large importance sampling ratios in Chapter ref:chap:resampling. This new approach to off-policy prediction effectively manages the update variance found when using importance sampling ratios directly in the update, while also improving the efficiency of each update towards learning the target prediction.

In this thesis, I not only contributed to our understanding and art in learning agent-state for the partial observable reinforcement learning setting, I developed a general purpose approach to learning agent-state through temporal-difference and showed the utility of such an approach as compared to the usual approach found in supervised learning. This thesis lays the ground work for a wide range of interesting research questions about learning agent-state. More details of these future directions can be found below.


** DONE Conclusion
CLOSED: [2023-03-03 Fri 11:59]

This thesis provides evidence that the intuitions developed about recurrent neural networks in supervised learning might not directly apply to the continual reinforcement learning setting. While many of the approaches derived in SL could be useful in RL, these should be re-investigated, validated, and understood in the continual reinforcement learning setting. While I propose several directions for improvement in recurrent networks for CRL, this thesis lays the groundwork for many avenues to contribute to the use and understanding of recurrent networks in CRL. The future of recurrent networks CRL will likely share many similarities with those found in SL, but will likely have radically different approaches to construction, learning, and analysis which cannot be developed in the SL setting alone.

* Postamble                                                          :ignore:

#+begin_export latex
\printbibliography
#+end_export

* Appendix :ignore:

#+begin_export latex
\appendix
#+end_export


* DONE [5/5] Future directions and work
** DONE Open Problems for Recurrent Architectures in RL
CLOSED: [2023-03-03 Fri 11:55]
:PROPERTIES:
:CUSTOM_ID: sec:conc:arnn:open-problems
:END:

Recurrent architectures are often taken off-the-shelf from the supervised learning setting for use in reinforcement learning. While this has been moderately successful, the RL problem poses challenges not often considered by supervised learning. Below we discuss three interesting properties of an RL system, and how they affect learning using recurrent networks.

*Practical Online Recurrent Learning:*

In reinforcement learning, it is desirable to learn as much about the most recent experience before selecting an action (i.e. to learn online and incrementally). Learning efficiently online enables adapting behavior in real time and scaling to massive data-streams and architectures. This puts pressure on the learning system to update the weights within a set amount of time so the system can act cite:&sutton2011horde;&white2015developing, which is often not a concern in the supervised setting. In settings where an agent must move around its environment independently, the on-board computational system can be heavily constrained by the power of the processor as well as limited energy from the battery. An algorithm whose computational and memory complexity scales independently of the sequence length (without the quadratic complexity on size of the network as real time recurrent learning cite:&williams1989learning) and could be applied online-incrementally would be a major breakthrough in using recurrent architectures for RL and computationally constrained systems generally. A detailed discussion on relevant literature is in Appendix \ref{app:ltd}.

*Active Data Collection Matters:*

Imagine an agent in a hallway with recognizable observations only at the beginning and end of the hallway, much like our TMaze environments. The agent must learn a state update which spans at least the length of the hallway. But this is in the best case scenario when the agent prioritizes making it to the end of the hallway. In reality, our agents will randomly explore the hallway until the end, often extending the length of the sequence the agent needs to learn over. The interaction between the agent's behavior (or exploration) and the difficulty of training under partial observability with a recurrent agent is currently unexplored. Active data collection strategies could mitigate the length of long-temporal dependencies, which would show massive improvements in our agent's learning efficiency and ability.

\begin{wrapfigure}[20]{r}{0.4\textwidth}
  \includegraphics[width=0.4\textwidth]{./plots/arnns/figures/dirtmaze_learn_intervention.pdf}
  \caption{Average success over the intervention taking the go forward activation and starting in the eastward position. {\bf (Naive Strategy)} Using the evaluated intervention over 60k steps for training, {\bf (Hand Designed)} a sequence of hand designed interventions to build up to the final evaluation intervention over 60k steps.} \label{fig:dirtmaze_inter}
\end{wrapfigure}


To show the potential of active data collection, we will briefly revisit the Directional TMaze. Start with an agent who has learned the base task of the Directional TMaze environment. If we force the agent to take specific actions and to start in a specific orientation at the beginning of the episode, we could feasibly teach the agent to artificially extend the horizon of its policy without increasing the length of the training sequence. See Figure \ref{fig:dirtmaze_inter} for preliminary results of how behavior can extend the horizon of the agent's policy. In this experiment, trained multiplicative agents are paced through a set of interventions. The naive strategy uses epsilon greed after forcing the agent to step forward twice down the hallway. The hand designed sequence, instead guides the agent through a series of forced actions to build to the final desired policy. This simple experiment shows the potential for slowly extending the temporal horizon of a policy without adjusting the truncation value by intervening on the agent's behavior.


*Insight Beyond Learning Curves:*


Learning curves provide little understanding of an agent's learning process and this likely limits algorithmic progress in partially observable settings. Unfortunately, such metrics can't be used to address more complicated questions about the agent and its behavior. While searching for SOTA is admirable, deeper questions about the internal learned structures and behaviors of our agents are often, but not always, ignored. Analyzing the internal dynamics of an agent with recurrent architectures is uniquely challenging in reinforcement learning. Some challenges include (see Appendix \ref{app:understanding} for details):
# \begin{itemize} % [topsep=0pt,itemsep=-1ex]
#   \setlength\itemsep{-0.0em}
#   \setlength\topsep{0.0pt}
- Generating data for evaluating and analyzing representation learning is an especially difficult problem for agents with recurrence. The data generated must be coherent trajectories the agent may potentially experience in the environment, meaning a (or several) data generating policy must be selected to provide coverage over the space of agent-environment interactions
- Current tools for analyzing state representations are designed for NLP cite:&karpathy2015visualizing;&ming2017understanding and are ill-suited for analyzing the link between the environment, the agent's state, and the behavior policy.
- Analyzing the behavior of our agents through performance metrics leaves many questions unanswered: How the agent might be behaving? When does the agent make a long-term decision? In what circumstances might the agent's policy fail?
# \end{itemize}

# \section{Insight Beyond Learning Curves} \label{app:understanding}
Learning curves showing the agent's performance, usually through episodic return or prediction error, over the agent's lifetime has been the primary method algorithms are compared. Unfortunately, such metrics can't be used to address more complicated questions about the agent and its behavior. While searching for SOTA is admirable, deeper questions about the internal learned structures and behaviors of our agents are often, but not always, ignored. Analyzing the internal dynamics of an agent with recurrent architectures is uniquely challenging in reinforcement learning.

One challenge is how data is generated. Unlike SL whose data is usually a dataset designed ahead of time, RL generates data through interactions with an environment whose underlying dynamics are likely unaccessible to the system designer. While randomly generated data, in combination with tools from NLP cite:&karpathy2015visualizing;&ming2017understanding, can give us some insight into how our agent's perform, see section \ref{sec:learnability}, extending the analysis to larger domains could leave large parts of the agent-environment interactions unseen. 

While we provide some representational analysis in the prediction setting, further results in the control setting would be even more beneficial. Unfortunately, many of the analysis tools we considered require the ``correct'' target at a given time. In the control setting, even when the underlying dynamics of an environment can be fully specified (say in lunar lander) a notion of what the right action is at a given time can be extremely difficult to discover. Future work should go into analyzing the link between histories, agent state, environment state, and behavior.

Even when analyzing the behavior of our agent, using the performance metric as the primary measure is deeply flawed. This type of analysis fails to address questions such as: How the agent might be behaving? When does the agent make a long-term decision? In what circumstances might the agent's policy fail? Analyzing the agent as a non-linear coupled system with the environment through a series of dynamical equations could lead to further insight on conditions which lead the agent to behave in certain ways or when certain decisions are made by the agent. cite:&beer2003dynamics develop a series of questions and experiments to analyze an artificial agent from this perspective in a simple catcher like domain. While these tools would be difficult to apply to real-world problems, using them in simulations could provide useful insight into a full description of the agent's learned policy.

Because of the above challenges there are several lingering questions about these types of agents left unanswered in these domains. 1) Under what conditions will the agent's policy fail in an environment? 2) How robust are the policies to out-of-distribution events and how does this effect the hidden state?  3) What algorithms do the learning process discover to solve the domains reliably? 4) Is the model stable over a long training period or in a continual domain? 5) When does the agent make a decision, and does the agent stick to this decision? We believe answering these questions and more can lead to better understanding of recurrent agents as well as pathways to better algorithms for training such agents.

** DONE Learning how to encode action in recurrent architecture through experience
CLOSED: [2023-03-03 Fri 11:55]
:PROPERTIES:
:CUSTOM_ID: sec:conc:arnnbias
:END:

So far, we've focused on architectures which have static architectures, where the agent has no agency in learning the appropriate structure. While this strategy seems to be reasonable as a starting point, in the future an architecture which can learn these different networks would be more desirable. We propose one such architecture here and an initial empirical evaluation of this architecture, leading to a discussion on the problematic properties such an algorithm might have.

\begin{align*}
  \interstate_t^i &= f_{\text{update}}(\weights, \xvec_{t}, \avec_{t-1}) \\
  \psi_t &= f_{\text{GN}}(\xvec_t, \avec_{t-1}) \\
  \state_t &= \interstate_t \odot psi_t.
\end{align*}

Where $f_{\text{GN}}: \Actions \times \States \times \Observations \rightarrow \Reals^{|\state|}$ is a parameterized function which is used to create a mixture over the experts state $\interstate \in \Reals ^{|\state|}$ produced by a state update function $f_{\text{update}}$. Both the gating function and the expert rnn state update function can be arbitrarily constructed. In this section we focus on the simple RNN update and a feed forward ANN with relu activations and a softmax activation on the final layer.


\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{plots/arnns/figures/moe.pdf}
  \caption{ Directional TMaze sweep over size of the gating network (i.e how many units in a single hidden layer with relu activations and an output network w/ softmax output) for ({\bf left}) RNN and ({\bf right}) GRU. All experiments follow the procedures from previous results, except the MoE networks use 20 runs.}
\label{fig:moe}
\end{figure}

The results are presented in Figure \ref{fig:moe}. A sweep over various number of experts and a simple gating network with a single layer and softmax activation. As compared to the additive and multiplicative the mixture of experts RNN network performs in-between the two networks. The GRU, on the other hand, fails to perform well in this domain. This might be related to the results seen in section \ref{sec:combining}, where both the combined GRUs failed to outperform the multiplicative.

** DONE Understanding the intersection between predictive and non-predictive agent-state construction
CLOSED: [2023-03-03 Fri 11:55]

While this thesis creates a dichotomy between predictive and non-predictive approaches to learning agent-state, this is generally not a necessary distinction. One key question is can we combine the two approaches into a network that both takes advantage of temporal-difference for learning state as well as has the flexibility to learn state objects which aren't dictated by predictive questions. Such an architecture could improve on the sensitivity to truncation for many recurrent networks while also providing a depth of representation not available in the current state of the GVFN framework. Future work should go into understanding such an architecture.

** DONE Discovery and the form of GVF prediction targets
CLOSED: [2023-03-03 Fri 11:55]

As explored in Chapters ref:chap:composite and ref:chap:gvfn:discovery, little is known about the shape of composite predictive targets or of the space of predictive questions generally. while several works appeal to the ability of GVFs to specify questions about everything the agent could imagine, we must be careful that we are avoiding prior knowledge into the space of questions an agent can specify. Instead, questions should be built on the sequence of observations from the environment. This can be done through a meta-learning process or generate and test (see Chapter ref:chap:gvfn:discovery), but little effort has gone into understanding the complex structures of these predictive question networks. 

** DONE Beyond a synchronous predictive network
CLOSED: [2023-03-03 Fri 11:55]

Like co-agent networks before them, the GVFN framework provides a mechanism for de-linking the final objective of the agent and its internal representation learning. Using this framework to derive a asynchronous set of architectures could provide a lower-variance scheme as compared to the co-agent network work. The key reason this is possible is because the targets of the individual units are now predictions of the underlying data stream (or any other available signal), meaning the variance in predictions should be the direct result of the underlying uncertainty or variance in the actual data stream. This is unlike co-agent networks as they rely on random sampling for each co-agent's activations. An approach which combines prediction and control demons in the GVFN framework could be key to creating such an asynchronous network.

* TODO [#B] Details and extended ideas for incorporating actions in recurrent networks
:PROPERTIES:
:CUSTOM_ID: app:arnns
:END:

** Online Setting
# \subsection{Online Setting} \label{app:online}

In this section, we test to see if our conclusions from the previous sections generalize to the fully online setting. We report some results for Ring World and DirectionalTMaze here, with further results in appendix \ref{app:emp_rw} and \ref{app:emp_dtm} respectively.
For both environments, all applicable settings are the same as in the replay counter parts. The only difference is in how the network is updated. Instead of sampling from an experience replay, we store a history of the truncation length and update the network on every step using the same semi-gradient updates.

We present the results for the online setting in figure ref:app:arnns:fig:online. Compared to the replay setting, we can see all the variants performed worse across the board. For DirectionalTMaze the AAGRU and MAGRU have a reasonable median performance. The MARNN and FacGRU are the only other cells which have runs reaching good performance, but overall perform poorly. We expect initialization plays a large role in the networks performance and should be investigated. We also see similar trends in Ring World, except the RNN variants outperform the GRUs. Another interesting consequence in the online setting, is the need to increase the truncation value and hidden state size to perform reasonably for both DirectionalTMaze and Ring World.

\begin{figure}[t]
  \centering
    \includegraphics[width=\linewidth]{plots/figures/Online.pdf}
  \caption{ Online: {\bf (left + middle) } Directional TMaze percent success in reaching the goal over the final $10\%$ of episodes with 100 independent runs for CELL (hidden size): RNN (46), AARNN (46), MARNN (27), FacRNN (46) $\factors=24$, GRU (26), AAGRU (26), MAGRU (15), FacGRU (26) $\factors=21$. {\bf (right)} Ring World learning curves over RMSVE with 100 independent runs for: RNN (20), AARNN (20), MARNN (15), GRU (12), AAGRU (12), MAGRU (9). Ribbons show standard error and a window averaging over 10k steps was used. Factored variants were excluded for clarity, due to high variance results. All agents were trained over 300k steps.}
\label{app:arnns:fig:online}
\end{figure}

** Masked Grid World


\begin{wrapfigure}[21]{l}{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/arnns/figures/masked_gw.pdf}
  \caption{Average number of steps to goal over truncation for Masked Grid World {\bf left} over the entire
    learning process {\bf right} from a set of representative states after
    training. Cell (state size) {\bf top} RNN (24), AARNN (20),
    MARNN (10) {\bf bottom} GRU (24), AAGRU (20), MAGRU (10).}\label{fig:maskedgw}
\end{wrapfigure}

While the TMaze and DirTMaze give some insight into when different encodings might be preferable, the DirTMaze and Ring World share similar dynamics in how the actions effect the unobserved state of the MDP. Specifically, there are two actions which effect a state component symmetrically. This prompts the question on whether this property is driving the benefits of the multiplicative update's success, or whether there are other scenarios where the multiplicative does better. We propose a new environment which is a simple grid world with border wrapping. The agent can take a step in all the cardinal directions, and observes when it enters a random subset of the states (all aliased together). The goal state is also randomly selected at the beginning of an agent's life. This creates random action observation patterns the agent must notice and act on to get to the goal. The border wrapping prevents the agent from moving to a corner of the environment and then going to the goal.


In figure \ref{fig:maskedgw}, we confirm the hypothesis that the improvement with multiplicative update can be meaningful even when the state-action sequences are randomly placed in the environment. While the improvement is much less drastic than the Ring World and DirTMaze, the improvement is still significant with standard error bars. Another interesting observation is the difference matters much more for the simple recurrent update than the GRU.

** Further Results for the Deep Additive and Deep Multiplicative Architectures
:PROPERTIES:
:CUSTOM_ID: app:arnns:deep-action
:END:
# \subsection{Further Results for the Deep Additive and Deep Multiplicative Architectures}\label{app:sec:deep_action}

\begin{figure}
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{plots/arnns/figures/dirtmaze_deep_a_layers.pdf}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{plots/arnns/figures/dirtmaze_deep_multiplicative.pdf}
  \end{subfigure}
  \caption{{\bf (left)} Resulting average success over the last $10\%$ of episodes for various number of hidden units in the action encoding network for the deep additive networks with standard error intervals. Each layer (denoted by the title of the plot) contains the number of hidden denoted by the x-axis. {\bf (right)} Comparing the deep multiplicative operation with the base cells used in the main text. }\label{app:arnns:fig:deep_action}
\end{figure}

We provide two more experiments with results reported in figure \ref{app:arnns:fig:deep_action}. First we provide results over various action encoding sizes for the Directional TMaze environment using the Deep Additive network from the main paper. Overall, we found the size of the encoding network to not make a large difference in the final performance. In effect, this result suggests there is still a core limitation with the deep additive operation that can't be overcome by larger encoding networks. We also provide an experiment in the Directional TMaze for a \textbf{deep multiplicative} update. The deep multiplicative update uses the multiplicative update as a base cell but first passes the action through a feed forward network like the deep additive network. The results of this network were quite poor overall, likely as a result of having to learn the action encoding rather than being given it as prior information. From these results we decided to abandon the deep multiplicative extension of \cite{zhu2017improving}. Future work should consider how to better learn action encodings for such a network.

** Further Empirical Details
:PROPERTIES:
:CUSTOM_ID: app:arnns:emp
:END:

# \section{Further Empirical Details} \label{app:emp}
In this section we discuss the experiments from the paper in greater detail. In the following tables the common programming notation $(x:y:z)$ is used to denote an array of elements starting from $x$ increasing by $y$ until $z$. For example $(1:2:5) = [1, 3, 5]$. When an operation is performed on an array it is done element wise. For example $2^{(1:2:5)} = [2, 4, 32]$. All hyperparameters are reported in agent steps (which are the same as environment steps for all domains).

*** Ring World
:PROPERTIES:
:CUSTOM_ID: app:arnns:emp:rw
:END:

\begin{wrapfigure}[10]{r}{0.4\textwidth}
  \vspace{-1cm}
  \begin{tabular}{l | r}  
    Parameter & Value \\ [0.5ex] 
    \hline
    Steps & 300,000 steps \\
    Optimizer & RMSprop \\
    RMSProp $\eta$ & $0.1 \times 1.6^{(-16:3:-2)}$ \\ %$\eta$ & $0.01*(2.0.^{(-11:2:-2)})$\\
    RMSprop $\rho$ & 0.9\\
    Buffer Size & 1000 \\
    Buffer Warmup & 1000 \\
    Batch Size & 4 \\
    Update freq & 4 steps \\
    Target Network Freq & 1000 steps \\
    Independent Runs & 50 \\
  \end{tabular}
  \caption{Ring World Hyperparameters} \label{app:fig:ringworld_params}
\end{wrapfigure}

Table \ref{app:fig:ringworld_params} gives the hyperparameters used in the ringworld experiments. We also provide full sensitivity curves over truncation for all cell types and hidden state sizes tested in Figure \ref{fig:app_er_rw_sens}. 


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{plots/arnns/ringworld/ringworld_er_truncation_sens.pdf}
  \caption{Truncation sensitivity curves for the Experience Replay setting in ring world. Results are RMSVE and error bars are $95\%$ confidence, as in the main paper.)}\label{fig:app_er_rw_sens}
\end{figure}

*** TMaze
:PROPERTIES:
:CUSTOM_ID: app:arnns:emp:tm
:END:

# \subsection{TMaze} \label{app:emp_tm}

In Figure \ref{fig:app_er_tm} we provide details of the experience
replay of the Bakker's TMaze experiments.

*** DirectionalTMaze
:PROPERTIES:
:CUSTOM_ID: app:arnns:emp:dtm
:END:
# \subsection{DirectionalTMaze} \label{app:emp_dtm}

The experiments presented in the paper used an experience replay buffer size of 10000 for all the cells. We also ran experiments using an experience replay size of 20000 with similar conclusions. These results (and the associated parameters) can be found in Figure \ref{fig:app_er_dtm}

*** Image Directional TMaze
# \subsection{Image Directional TMaze} \label{app:emp_idtm}

We detail all hyperparameter settings, and give results for different network sizes and truncation values in Figure \ref{fig:app_idtm}.

*** Lunar Lander
# \subsection{Lunar Lander} \label{app:emp_ll}

We provide all hyperparameter settings (Figure \ref{fig:app_ll}) and further results (Figures \ref{fig:app_ll_mr} and \ref{fig:app_ll_median})

\begin{figure}
  \begin{subfigure}{0.5\textwidth}
    \begin{tabular}{l | r}  
     Parameter & Value \\ [0.5ex] 
     \hline
      Steps & 300,000 steps \\
      Optimizer & RMSprop \\
     % RMSprop learning rate & $0.1 \times 1.6^{(-16:3:-4)}$ \\
      RMSProp RNN: $\eta$ & $0.01 \times (2.0^{(-11:2:-2)})$\\
      RMSProp GRU: $\eta$ & $0.01 \times (2.0^{(-11:2:-6)})$ \\
      RMSprop $\rho$ & 0.99\\
      Discount $\gamma$ & 0.99 \\
      Truncation $\tau$ & 12 \\
      Buffer Size & 10000 \\
      Batch Size & 8 \\
      Update freq & 4 steps \\
      Target Network Freq & 1000 steps \\
      Independent Runs & 50 \\

    \end{tabular}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=\linewidth]{plots/arnns/figures/tmaze_with_fac.pdf}
    \end{subfigure}
    
  \begin{subfigure}{1\textwidth}
    \centering
  \begin{tabular}{l | c | c | c}  
   Cell & RMSprop Learning Rate & Hidden State Size & Number of Model Parameters \\ [0.5ex] 
   \hline
    GRU    & 0.005     & 6          & 214 \\
    AAGRU  & 0.0003125 & 6          & 286 \\
    MAGRU  & 0.0003125 & 6          & 754 \\
    FacGRU & 0.0003125 & 6, $M=21$  & 757 \\
    DAGRU  & 0.00125   & 6, $a=4$   & 306 \\
    RNN    & 7.8125e-5 & 20         & 584 \\
    AARNN  & 7.8125e-5 & 20         & 664 \\
    MARNN  & 7.8125e-5 & 20         & 2024\\
    FacRNN & 0.0003125 & 20, $M=40$ & 2064\\
    DARNN  & 7.8125e-5 & 20, $a=4$  & 684 \\
   \end{tabular}
\end{subfigure}
  \caption{TMaze Experience Replay experiments: {\bf (top left)} The hyperparameters used across all cells {\bf (bottom)} The cell specific hyperparameters {\bf (top right)} Percent success over the final $10\%$ of episodes. Same as Figure \ref{fig:tmazes}}%
\label{fig:app_er_tm}
\end{figure}


\begin{figure}
  \begin{subfigure}{0.49\textwidth}
  \includegraphics[width=0.95\linewidth]{plots/arnns/figures/dirtmaze_with_fac_10k.pdf}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
  \includegraphics[width=0.95\linewidth]{plots/arnns/figures/dirtmaze_with_fac_20k.pdf}
\end{subfigure}

  \begin{subfigure}[l]{0.4\textwidth}
    \begin{tabular}{l | r}  
      Parameter & Value \\ [0.5ex] 
      \hline
      Steps & 300,000 steps \\
      Optimizer & RMSprop \\
      RMSprop $\eta$ & $0.01 \times 2.0^{(-11:2:-2)}$ \\
      RMSprop $\rho$ & 0.99\\
      Discount $\gamma$ & 0.99 \\
      Truncation $\tau$ & 12 \\
      Buffer Size & [10000, 20000] \\
      Batch Size & 8 \\
      Update freq & 4 steps \\
      Target update freq & 1000 steps \\
      Independent Runs & 100 \\
    \end{tabular}
  \end{subfigure}  
  \begin{subfigure}[r]{0.5\textwidth}
  \begin{tabular}{l | c | c | c }  
   Cell & $\eta$ & Hidden State Size & Num Weights \\ [0.5ex] 
   \hline
    GRU    & 0.00125   & 17         & 1142 \\
    AAGRU  & 0.0003125 & 17         & 1295 \\
    MAGRU  & 0.0003125 & 10         & 1303 \\
    FacGRU & 0.0003125 & 15, $M=17$ & 1320 \\
    DAGRU  & 0.0003125 & 15, $a=8$  & 1310 \\
    RNN    & 0.00125   & 30         & 1143 \\
    AARNN  & 0.0003125 & 30         & 1233 \\
    MARNN  & 7.8125e-5 & 18         & 1263 \\
    FacRNN & 0.0003125 & 25, $M=15$ & 1018 \\
    DARNN  & 0.0003125 & 25, $a=15$ & 1263 \\
   \end{tabular}
 \end{subfigure} 
  \caption{Directional TMaze Experience Replay results: {\bf (top right)} Percent success over the final $10\%$ of episodes for buffer size of 10000 {\bf (bottom right)} for buffer size of 20000. Learning rates chosen from best final performance on the final $10\%$ of episodes for 20 runs. Results for buffer size of 10000 are over 100 independent runs, with  buffer size of 20000 in appendix only over the 20 seeds used for the sweep. {\bf (bottom left)} The hyperparameters used across all cells. {\bf (bottom right)} The cell specific hyperparameters.}
\label{fig:app_er_dtm}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{1\textwidth}
    \includegraphics[width=0.95\linewidth]{plots/arnns/figures/idtm.pdf} 
   \end{subfigure}
   \begin{subfigure}{0.45\textwidth}
     \centering
     \begin{tabular}{l | r}  
       Parameter & Value \\ [0.5ex] 
       \hline
       Steps & 400,000 \\
       Optimizer & RMSprop \\
       ADAM $\eta$ & $2.0^{(-20:2:0)}$ \\
       ADAM $\beta$ & 0.9, 0.999\\
       Discount $\gamma$ & 0.99 \\
       Truncation $\tau$ & 12 (20) \\
       Warm up & 1000 steps \\
       Replay Size & 50,000 \\
       Batch size & 16 \\
       Target update freq & 10000 \\ 
       Update freq & 4 \\ 
       Runs & 20 \\
     \end{tabular}
   \end{subfigure}
   \begin{subfigure}{0.5\textwidth}
     \centering
     \begin{tabular}{l | c | c | c | c}  
       Cell & $\eta$ $\tau=12 (20)$ & $|s|$ + M & $|\weights|$ \\ [0.5ex] 
       \hline
       MAGRU & 9.76e-4 (9.76e-4)  & 32          & 154247 \\
       MAGRU & 9.76e-4 (9.76e-4)  & 64          & 223175 \\
       AAGRU & 2.44e-4 (0.0625)   & 70          & 155201\\
       AAGRU & 9.76e-4 (0.0156)   & 132         & 225323 \\
       FacGRU & 0.0625 (2.44e-4)  & 32, M=259   & 154224 \\
       FacGRU & 9.76e-4 (2.44e-4) & 64, M=350   & 223009 \\
       FacGRU & 9.76e-4 (2.44e-4) & 70, M=164   & 155041 \\
       FacGRU & 9.76e-4 (2.44e-4) & 132, M=208  & 225515\\
       DAGRU & 9.76e-4  (9.76e-4) & 45,  a=128  & 150838 \\
       DAGRU & 9.76e-4  (9.76e-4) & 55,  a=64   & 152022 \\
       DAGRU & 9.76e-4  (9.76e-4) & 60,  a=32   & 151399 \\
       DAGRU & 9.76e-4  (9.76e-4) & 100, a=128  & 224263 \\
       DAGRU & 9.76e-4  (9.76e-4) & 112, a=64   & 220935 \\
       DAGRU & 9.76e-4  (9.76e-4) & 122, a=32   & 223195 \\
     \end{tabular}
   \end{subfigure}
   \caption{Image Directional TMaze: {\bf (top)} Percent success over final $10\%$ of episodes for the image tmaze for $\tau=12$ and $\tau=20$ (labeled). See labels for size of hidden state with left being small networks, and right being larger. {\bf (bottom left)} The hyperparameters used across all cells in Image Directional TMaze {\bf (bottom right)} The cell specific hyperparameters.} \label{fig:app_idtm}
\end{figure}


\begin{figure}
  \begin{subfigure}{0.5\textwidth}
    \begin{tabular}{l | r}  
     Parameter & Value \\ [0.5ex] 
     \hline
     Steps & 4,000,000 steps \\
     Steps before learning starts & 1000 steps \\
     Optimizer & RMSprop \\
     RMSprop $\eta$ & $0.1 \times 1.6^{(-20:2:-6)}$ \\
     RMSprop $\rho$ & 0.99\\
     Discount $\gamma$ & 0.99 \\
     Truncation $\tau$ & 16 \\
     Replay Size & 100,000 \\
     Batch size & 32 \\
     Target update freq & 1000 steps \\ 
     Update frequency & 8 steps\\ 
     Hidden state learnable & True \\ 
     Independent Runs & 20 \\
    \end{tabular}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
  \includegraphics[width=\linewidth]{plots/arnns/figures/lunarlander_with_fac.pdf}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \begin{tabular}{l | c | c | c | c}  
   Cell & RMSprop Learning Rate & Hidden State Size (Factors/Action Encoding) & Number of Model Parameters \\ [0.5ex] 
   \hline
    GRU    & 0.0003553   & 154          & 156,414 \\
    AAGRU  & 0.0001387   & 152          & 155,004 \\
    MAGRU  & 0.0003553   & 64           & 153,732 \\
    FacGRU & 0.0001387   & 152 (M=170)  & 152,668 \\
    FacGRU & 0.0003553   & 100 (M=265)  & 153,808 \\
    FacGRU & 0.0003553   & 64 (M=380)   & 153,716 \\
    DAAGRU & 0.000138778 & 152 (a=64) & 182,684 \\
  \end{tabular}
\end{subfigure}
\caption{Lunar Lander experimental details: {\bf (top left)} The hyperparameters used across all cells in Lunar Lander {\bf (bottom)} The cell specific hyperparameters  {\bf (top right)} Average final reward over all episodes (same as figure \ref{fig:scaling_up})}
\label{fig:app_ll}%
\end{figure}


\begin{figure}
  \centering
    \begin{subfigure}{0.49\textwidth}
\includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_steps_curve_sans_fac.pdf}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
\includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_steps_curve_fac.pdf} 
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
\includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_lr_sensitivity.pdf} 
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_lc_fac_only.pdf} 
    \end{subfigure}
  \caption{Lunar Lander further results: {\bf (top left) } Average final reward over the final $10\%$ of episodes for 20 runs {\bf (top middle) } Total steps per episode for non-factored cells for 20 runs {\bf (top right) } Total steps per episode for factored cells for 20 runs  {\bf (bottom left) } learning rate sensitivity curves for 10 runs {\bf (bottom middle)} Learning curves per episode for non-factored cells over total reward for 20 runs {\bf (bottom right)} Learning curves per episode for factored cells over total reward for 20 runs} \label{fig:app_ll_mr}
\end{figure}



\begin{figure}
  \centering
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_ind_gru.pdf}
      \caption{GRU}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_ind_magru.pdf}
      \caption{MAGRU}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_ind_aagru.pdf}
      \caption{AAGRU}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=0.95\linewidth]{plots/arnns/lunarlander/lunar_lander_ind_daagru.pdf}
      \caption{DAAGRU}
    \end{subfigure}
  \caption{Individual learning curves. Line is the median over 1000 episodes, with the shaded region as the 1st and 3rd quantile over the same window.} \label{fig:app_ll_median}
\end{figure}

* TODO Further details and supporting proofs for Importance Resampling
** Variance of Updates
:PROPERTIES:
:CUSTOM_ID: app:ir:var
:END:
# \subsection{Variance of Updates}\label{sec:variance}

It might seem that resampling avoids high-variance in updates, because it does not reweight with large magnitude IS ratios. 
The notion of /effective sample size/ from statistics, however, provides some intuition about why large magnitude IS ratios can also negatively affect IR, not just IS. 
%For data with several high magnitude ratios, and many small ratios, the IS estimator will likely suffer from high-variance updates. IR, however, may not perform well in this setting either: it will prevent high magnitude updates, but will be sampling from an effectively smaller dataset. 
Effective sample size is between 1 and $n$, with one estimator \(\left(\sum_{i=1}^{n} \rho_i \right)^2/\sum_{i=1}^{n} \rho_i^2\) \citep{kong1994sequential, martino2017effective}. When the effective sample size is low, this indicates that most of the probability is concentrated on a few samples.
For high magnitude ratios, IR will repeatedly sample the same transitions, and potentially never sample some of the transitions with small IS ratios. 

Fortunately, we find that, despite this dependence on effective sample size, IR can significantly reduce variance over IS. 
In this section, we characterize the variance of the BC-IR estimator. We choose this variant of IR, because it is unbiased and so characterizing its variance is a more fair comparison to IS.
We define the mini-batch IS estimator $\xis \defeq \frac{1}{k}\sum_{j=1}^k\rho_{\unisample}\Delta_{\unisample}$, where indices $\unisample$ are sampled uniformly from $\{1, \ldots, \bsize \}$. This contrasts the indices $i_1, \ldots, i_k$ for $\xbciwer$ that are sampled proportionally to $\rho_i$. 

We begin by characterizing the variance, under a fixed dataset $B$. 
For convenience, let \(\muB{B} = \E_\pi[\Delta | B]\). 
We characterize the sum of the variances of each component in the update estimator, which equivalently corresponds to normed deviation of the update from its mean,
{{{c}}}
\begin{align*}
\V(\Delta \ | \ B) \defeq \tr\Cov(\Delta \ | \ B) = {\textstyle\sum_{m=1}^d \Var(\Delta_m \ |\  B)}
= \E[\| \Delta - \muB{B} \|_2^2  \ |\  B ]
\end{align*}
{{{c}}}
for an unbiased stochastic update ${\small \Delta \in \RR^d}$. 
We show two theorems that BC-IR has lower variance than IS, with two different conditions on the norm of the update. We first start with more general conditions, and then provide a theorem for conditions that are likely only true in early learning.
{{{c}}}
\begin{theorem} \label{thm:var_reduc}
	Assume that, for a given buffer $B$,  $ \| \Delta_j \|_2^2 > \frac{c}{\rho_j}$ for samples where $\rho_j \ge \bar{\rho}$, and that $ \| \Delta_j \|_2^2 < \frac{c}{\rho_j}$ for samples where $\rho_j < \bar{\rho}$, for some $c > 0$. Then the BC-IR estimator has lower variance than the IS estimator: $\V(\xbciwer \ | \ B) < \V(\xis \ | \ B)$.
\end{theorem}
{{{c}}}
{{{c}}}
The conditions in Theorem \ref{thm:var_reduc} preclude having update norms for samples with small $\rho$ be quite large---larger than a number \(\propto \frac{1}{\rho}\)---and a small norm for samples with large $\rho$. These conditions can be relaxed to a statement on average, where the cumulative weighted magnitude of the update norm for samples with $\rho$ below the median needs to be smaller than for samples with $\rho$ above the mean (see the proof in Appendix \ref{app_bc_var}).
% The conditions only require a lower bound on the update norm for samples with larger $\rho_j$, with that lower bound proportional to $1/\rho_j$ which is small for larger $\rho_j$. Similarly, the conditions only require an upper bound on the update norm for samples with smaller $\rho_j$, with that upper bound proportional to $1/\rho_j$ which is large when $\rho_j$ is smaller. To violate these conditions, we would need the update norm for samples with small $\rho$ to be quite large---larger than a number \(\propto \frac{1}{\rho}\)---and the norm to be quite small for samples with large $\rho$. 

We next consider a setting where the magnitude of the update is independent of the given state and action. We expect this condition to hold in early learning, where the weights are randomly initialized, and thus randomly incorrect across the state-action space. As learning progresses, and value estimates become more accurate in some states, it is unlikely for this condition to hold.

\begin{theorem} \label{thm:var_indep}
  Assume $\rho$ and the magnitude of the update $\| \Delta \|_2^2$ are independent
  \begin{equation*}
  \E[\rho_j \|\Delta_j \|_2^2  \ | \  B] = \E[\rho_j \ | \ B]  \ \E[ \|\Delta_j \|_2^2 \ | \ B]
  \end{equation*} 
  Then the BC-IR estimator will have equal or lower variance than the IS estimator: $\V(\xbciwer \ | \ B) \le \V(\xis \ | \ B)$.
\end{theorem}


These results have focused on variance of each estimator, for a fixed buffer, which provided insight into variance of updates when executing the algorithms. We would, however, also like to characterize variability across buffers, especially for smaller buffers. Fortunately, such a characterization is a simple extension on the above results, because variability for a given buffer already demonstrates variability due to different samples. It is easy to check that $\E[\E[\mu_{IR} \ | \ B]] = \E[\mu_{IS} \ | \ B] = \E_\pi[\Delta]$. The variances can be written using the law of total variance
\begin{align*}
\V(\xbciwer) 
&= \E[\V(\xbciwer \ | \ B) ] + \V(\E[\xbciwer \ | \ B])
= \E[\V(\xbciwer \ | \ B) ] + \V(\muB{B})\\
\V(\xis) 
% &= \E[\V(\xis \ | \ B) ] + \V(\E[\xis \ | \ B]) \\
&= \E[\V(\xis \ | \ B) ] + \V(\muB{B})\\
\implies \V(\xbciwer ) & - \V(\xis) = \E[\V(\xbciwer \ | \ B) - \V(\xis \ | \ B)]
\end{align*}  
{{{c}}}
with expectation across buffers. Therefore, the analysis of $\V(\xbciwer \ | \ B)$ directly applies.

** Additional Theoretical Results and Proofs
:PROPERTIES:
:CUSTOM_ID: app:sec:arnns:theory
:END:





* Sections Looking for a home                                      :noexport:

*************** TODO [#A] Figure out where Learning long-temporal dependencies goes?
*************** END

This chapter
- goes through state construction using predictions
- discusses the large literature related to learning long-temporal dependencies.



** Something on state

For a machine intelligent system with an egocentric perspective, sufficiently summarizing the history of interactions is critical to success in its lifetime. While a unique state can be defined as the set of all histories which induce the same predictions over all futures [[cite:&littman2002predictive;&zhang2021learning]], an agent only has a single lifetime and must make due without living through multiple histories. Awash in the stream of sensor readings available to the agent, it is not always clear what regularities are important for the agent to capture for long-term success in the world or how to capture such regularities. Many other constrained definitions of a sufficient summary of state exists [[cite:&subramanian2022approximate]]. Many approaches focus on the capability of the agent to predict the reward function to develop a policy of behavior, but this approach might not be sufficient if its rewards are non-stationary or if the agent has multiple goals. Another idea is the agent's beliefs should be grounded in predictions about the sensorimotor stream directly. This includes predictions about the prescribed reward function, but also encompasses all real-valued signals for which the agent has access.

** DONE [1/1] Learning Long-temporal Dependencies
:PROPERTIES:
:CUSTOM_ID: sec:bg:perception:tempdepend
:END:

*************** DONE edit LLTD section :noexport:
CLOSED: [2023-02-21 Tue 15:48]
*************** END


Learning long-temporal dependencies is the primary concern of both RL and SL applications of recurrent networks. While great work has been done to coalesce around a few potential architectures and algorithms for SL settings, these are often found lacking in the online-incremental RL context cite:&sodhani2020training;&rafiee2022eyeblinks;&schlegel2021general. 
# discussed in section \ref{sec:open_problems}.
Not only do agents need to learn from the currently stored data (i.e. in an experience replay buffer), they must also continually incorporate the newest information into their decisions (i.e. update online and incrementally). The importance of learning state from an online stream of data has been heavily emphasized in the past through predictive representations of state cite:&littman2002predictive, temporal-difference networks [[cite:&sutton2005temporaldifference]] and GVF networks [[cite:&schlegel2021general]], and in modeling trace patterning systems [[cite:&rafiee2022eyeblinks]]. From a supervised learning perspective, several problems like saturating capacity and catastrophic forgetting are cited as the most pressing for any parametric continual learning system [[cite:&sodhani2020training]]. Below we suggest a few alternative directions needing further exploration in the RL context.

The current standard in training recurrent architectures in RL is truncated BPTT. This algorithm trades off the ability to learn long-temporal dependencies with computation and memory complexity. Currently, the system designer must set the length of temporal sequences the agent needs to model (as would be needed for truncated BPTT to be effective [[cite:&mozer1995focused;&ke2018sparse;&tallec2018unbiased;&rafiee2022eyeblinks]]). Setting this length is a difficult task, as it interacts with the underlying environment and the agent's exploration strategy
# (see section \ref{sec:open_problems} for more details).
As the truncation parameter increases it is known that the gradient estimates become wildly variant [[cite:&pascanu2013difficulty;&sodhani2020training]], which can make learning slow.

An alternative to (truncated) BPTT is real time recurrent learning (RTRL) cite:&williams1989learning. Unfortunately RTRL is known to suffer high computational costs for large networks. Several approximations have been developed to alleviate these costs [[cite:&tallec2018unbiased;&mujika2018approximating]], but these algorithms often struggle from high variance updates making learning slow. The approximation to the RTRL influence matrix proposed by cite:&menick2020practical shows significant promise in sparse recurrent networks, even outperforming BPTT when trained fully online. citeauthor:&ke2018sparse (citeyear:&ke2018sparse) propose a sparse attentive backtracking credit assignment algorithm inspired by hippocampal replay, showing evidence the algorithm has beneficial properties of both BPTT and truncated BPTT. The focused architecture was often able to compete with the fully connected architecture on length of learned temporal sequence and prediction error on several benchmark tasks. Another line of search/credit assignment algorithms is generate and test [[cite:&kudenko1998feature;&mahmood2013representation;&dohare2022continual;&samani2021learning]]. These search algorithms aren't as tied to their initialization as other systems as they intermittently inject randomness into their search to jump out of local minima. Many of these approaches combine both gradient descent and generate and test to gain the benefits of both. While a full generate and test solution is possible, finding the right heuristics to generate useful state objects quickly could be problem dependent.

Learning long-temporal dependencies through regularizing objectives on the state has shown promise in alleviating the need for unrolling the network over long-temporal sequences. citeauthor:&schlegel2021general (citeyear:&schlegel2021general) use GVFs to make the hidden state of a simple RNN predictions about the observations showing potential in lightening the need for BPTT. This approach is sensitive the GVF parameters to use as targets on the state of the network. Predictive state recurrent neural networks [[cite:&downey2017predictive]] combine the benefits of RNNs and predictive representations of state [[cite:&littman2002predictive]] in a single architecture. They show improvement in several settings, but don't explore the model when starved for temporal information in the update. Another approach is through stimulating traces, as shown by [[cite:&rafiee2022eyeblinks]], where traces of observations are used to bridge the gap between different stimuli. Instead of traces, an objective which learns the expected trace [[cite:&hasselt2021expected]] of the trajectory could provide similar benefits as a predictive objective. One can even change the requirements on the architecture in terms of final objectives. [[cite:&mozer1991induction]] propose to predict only the contour or general trends of a temporal sequence, reducing the resolution considerably. Value functions are another object which takes an infinite sequence and reduces resolution to make the target easier to predict [[cite:&sutton1995td;&sutton2011horde;&modayil2014multitimescale;&vanhasselt2015learning]].

It is also possible to reduce or avoid the need for BPTT for modeling long-temporal sequences by adjusting the internal mechanisms of the recurrent architecture. Echo-state Networks [[cite:&jaeger2002adaptive]] are one possible direction. Related to the generate and test idea, echo-state networks rely on a random fixed "reservoir" network, where predictions are made by only adjusting the outgoing weights. Because the recurrent architecture is fixed, no gradients flow through the recurrent connections meaning no BPTT is needed to estimate the gradients. Unfortunately, these networks are dependent on their initializations making them hard to deploy in practice. [[citeauthor:&mozer1995focused]] ([[citeyear:&mozer1995focused]]) propose a focused architecture design, where recurrent connections are made more sparsely (even just singular connections). This significantly reduces the computational complexity of RTRL and allows for a focused version of BPTT.

Transformers [[cite:&vaswani2017attention]] are a widely used alternative to recurrent architectures in natural language processing. Transformers have also shown some success in reinforcement learning but either require the full sequence of observations at inference and learning time [[cite:&mishra2018simple;&parisotto2020stabilizing]] or turn the RL problem into a supervised problem using the full return as the training signal [[cite:&chen2021decision]]. Because of these compromises, it is still unclear if transformers are a viable solution to the state construction problem in continual reinforcement learning.
* Graveyard                                                        :noexport:
** Old Introduction                                               :noexport:
Learning to behave and predict using partial information about the world is critical for applying reinforcement learning (RL) algorithms to large, complex domains. For example, consider a deployed automated spacecraft with a faulty sensor that is only able to read signals intermittently. For the spacecraft to stay in service it needs to deploy a learning algorithm to maintain helpful information (or state) about the history of intermittent sensor readings as it relates to the other sensors and how the spacecraft is behaving. Game playing systems such as StarCraft [[citep:&vinyals2019grandmaster]] provides another good example. An agent who plays StarCraft must build a working representation of the map, it's base and strategy, and any information about its rival's base and strategy as it focuses its observations on specific locations to perform actions.

*************** TODO [#B] Define Representations
*************** END

*************** TODO [#B] Define continual reinforcement learning.
*************** END


Deep reinforcement learning has expanded reinforcement learning to wide spectrum of domains, specifically those with complex observations from the environment  [[cite:&mnih2015humanlevel;&vinyals2019grandmaster]]. Significant work has gone into engineering primarily feed-forward networks [[cite:&hessel2018rainbow;&espeholt2018impala]]. Applications of recurrent networks to reinforcement learning problems typically apply the assumptions constructed in the supervised learning (SL) setting [[cite:&bakker2002reinforcement;&hausknecht2015deep;&onat1998recurrent;&kapturowski2019recurrent;&zhu2018improving]]. While the intuitions developed in SL set the stage for the reinforcement learning problem, many of these architectural and algorithmic intuitions might not translate to the unique characteristics of the reinforcement learning problem. A unifying justification for this view is actively behaving in the world may have a large impact on perception learning. [[citeauthor:&noe2004action]] ([[citeyear:&noe2004action]]) lays out this point clearly, suggesting behavior and decision making are linked to our (and other biological system's) strategies for perception. This is re-iterated in the deep reinforcement learning context in [[cite:&ostrovski2021difficulty]], where they show the difficulty in learning a representation in a deep Q-network (DQN) if done in tandem with a separate controlling DQN.

# Finally, a related line of reasoning argues complex behavior arises from the interactions between the mind, body and environment [[cite:&chemero2013radical]]. While a clear definition of what constitutes a body over just a separate part of the environment hasn't been discussed in the reinforcement learning context, the importance of agency and acting during the foundation of perception is still highlighted as critical.

While we can--and arguably should--use the intuitions built in the SL setting, these intuitions must be tested and further justified as the problem setting changes to the continual reinforcement learning setting.

For example, often new cell architectures use a full sequence to estimate the gradients using backpropagation, which is not possible in a continual reinforcement learning problem. While several strategies exist to reduce the sequence length in approximating the gradient, these often lead to comprising on the length of temporal-dependencies the network can model (see Section ref:sec:bg:perception:tempdepend). Another example is in the incorporation of information into the recurrent cell. A reinforcement learning agent has three primary forms of information to incorporate into the state: sensations (or observations), prior beliefs, and action. The most straightforward (but possibly not best) solution is to combine these forms of information through a concatenation operation, passing the action and observation through separate feed-forward networks (see Chapter ref:chap:arnn).

A critical structural assumption in many approaches to learning in partial observability is that all learning is driven by the gradients (or the importance of features) of the system with respect to the final error (i.e. the agent's beliefs are free from constraints external to the main objective). One opposing idea is to use the answers to predictive questions about the stream of observations. Several approaches to learning under partial observability using predictions have been proposed and studied. One approach is to use short histories to find a collection of observation action sequences to infer the probability of seeing another trajectory given the agent's current histry, also known as predictive state representations (PSRs) [[cite:&littman2002predictive;&singh2004predictive]]. Another architecture, known as Temporal-difference Networks [[cite:&sutton2005temporaldifference;&tanner2005temporal;&tanner2005td;&tanner2005thesis]], is highly related to the work presented in this dissertation. A TDN is a combination of a question network (built on the base observations) and an answer network (the parameter used for answering the questions). The TDN then used TD to learn the parameters of the answer network from experience.


This dissertation contributes to the problem of learning in partial observability in the continual reinforcement learning problem through careful empirical evaluation of current architectures and the development of novel algorithmic and architectural approaches. In this thesis, I use gradients and predictions (in the form of general value functions) to inform the learning process, and investigate some of the underlying principles of using recurrent networks to form the basis of a history summary mechanism in reinforcement learning. Finally, I develop deeper intuitions in using predictions as the primary driver of summarizing histories, leading to several algorithmic and architectural improvements in this line of work.


The thesis seeks to explore solution methods and architectures for learning in partial observability through gradients and other methods. Particularly, we seek to answer:
#+BEGIN_QUOTE
What algorithmic modifications are needed to make reinforcement learning methods more performant in partially observable, continual decision making tasks?
#+END_QUOTE


The thesis seeks to explore solution methods and architectures for learning in partial observability through gradients and other methods. Particularly, we seek to explore the consequences of the thesis statement:
#+BEGIN_QUOTE
The architectural intuitions and assumptions developed in supervised learning for learning in partial observability are limiting in the continual reinforcement learning setting.
#+END_QUOTE
I seek evidence through a slow study of several assumptions in different conditions for a reinforcement learning agent. The approach taken in this thesis centers on clearly formulating various architectures and algorithms for state search and developing in-depth investigative experiments to uncover the dynamics of both predictive and non-predictive approaches. All work presented in this dissertation uses gradient descent--using truncated BPTT (either through experience replay or full online systems) to estimate gradients--to clearly test different choices and architectures using a shared learning platform.
** Contributions                                                  :noexport:


In this section, I outline the specific contributions made to the field of machine intelligence and reinforcement learning to satisfy the requirements of the doctoral degree at the University of Alberta.

1. Developing and empirically validating recurrent cells which incorporate action into their update functions (Chapter ref:chap:arnn).
   This contribution focuses on empirically evaluating several architectural choices in how the agent encodes action in the state-update function of a recurrent network. We start with an in-depth analysis of an agent's predictions and learned hidden state in a small example domain. We then explore when this choice is important for the control problem in a series of experiments using the DRQN [[cite:&hausknecht2015deep]] framework.
2. Define a predictive approach to learning agent-state update functions through learning answer to predictive questions posed by General Value Functions (Chapter ref:chap:gvfn). This chapter defines the core framework of the general value function network (GVFN). The key contributions include the restriction of predictive questions to be composed in acyclic graphs, and extensions to include the set of predictions made by PSRs, TDNets, and other forecasting networks.
3. Derive a gradient algorithm, from which several learning rules are generated, for learning GVFNs (Chapter [[ref:chap:gvfn:algs]]). The key contribution of this chapter is the derivation of gradient recurrent temporal-difference learning for GVFNs. This learning rule minimizes the /Mean-Squared Projected Bellmen Network Error/ and extends the gradient temporal-difference network learning update [[cite:&silver2013gradient]] to GVFNs. This is a sound gradient algorithm taking into account the possible composite structure of questions and the recurrent nature of answering the questions. This algorithms is then simplified to produce a recurrent semi-gradient temporal-difference updating rule. Further details on calculating the gradient and Hessian of the value function back through time are included.
4. Empirically validating the GVFN approach, comparing to other auxiliary task approaches in the prediction setting, and developing intuitions on suggested predictive questions to use (Chapter ref:chap:gvfn:empirical). This chapter evaluates whether restricting the hidden state of a recurrent network to be predictions learned through temporal-difference can learn without the need of backpropagation through time. I evaluate several collections of predictive questions in both timeseries prediction and in the reinforcement learning prediction setting. I test whether the use of predictions using temporal-difference are a critical component. Finally, we discuss the full gradient algorithm and discuss when it may be necessary to use.
5. Investigating and developing grounding for the targets of composite GVF questions ref:chap:composite. This chapter details the targets induced by composite GVF questions. I contribute a closed form for the effective emphasis of future cumulants for composite predictions with constant discounts. Finally, I detail empirical observations of more complex composite forms in various timeseries datasets.
6. Outlining a baseline discovery algorithm for generating questions through experience online in GVFNs (Chapter ref:chap:gvfn:discovery). This chapter develops a baseline discovery algorithm using generate-and-test to discover predictive questions for agent-state construction. We successfully use this approach to find a set of questions which form the basis to learn in an illustrative domain.
7. Contributing to learning predictions off-policy more efficiently in the deep learning setting through the use of importance resampling ref:chap:resampling. In this chapter, I propose importance resampling {{{citeplease}}} to learn off-policy predictions in the case we have an experience replay buffer. I show this approach is more amenable to the types of off-policy regimes present when using GVFs while also remaining consistent in its estimates.
** Graveyard                                                      :noexport:
*** DONE [#B] What is my thesis statement now?
CLOSED: [2022-09-06 Tue 13:59]
The proposal is centered on what GVFs can bring to the table in terms of learnability in recurrent networks. Now we want to incorporate RNNs more into the discussion. What should we do?
- Focus on understanding: The goal of my work generally is to understand. What are RNNs brining to the table, what are GVFNs brining to the table. Are they compatible?
- partial observability
- some History of RNNs in RL/online data.
- some History of pred reps.
- some History of perception.
*** What Am I writing the document about?

This document is primarily about partial observability in reinforcement learning.

Why focus on partial observability?

State Construction is...?
- Levels of state construction:
  - Reactive/low-level state vs abstractions for state?
  - What do we want to learn in a state? -> We don't know!
  - There isn't a clear set of criteria for determining what makes for a good state in reinforcement learning
    - Separability? Good Representations properties? Predictive of final task?

- At what abstraction should we be focused?
  - Low level: predictions in the sensor space.
  - High level: predictions/planning in the abstract/concept space.
  - Are these different??

Perception as a series of modules:
- "Is this a face?" much easier than "Is this x's face?"
- The brain is not just one big classification network, submodules are used to specialize. But "how to use submodules" is a hard question.
- Separate the conscious brain from the acting brain.
  - Audio circuit which short circuits the brain to act in the face of a loud noise -> no "control"
  - Other short circuits that bring visual stimuli towards the mid brain for control signals.
- RL is studying the algorithms of the mid brain/cerebellum. We should avoid extending the lessons we learn here to the entire functioning of the brain. In our studies of intelligence we need to be multi-modal. There isn't a single way to conceptualize the concepts, and finding the true underlying properties of the brains algorithms are beyond our capabilities to model mathematically.
- To understand intelligence, we must take the whole embodiment into consideration.

Two philosophies in state building:
- predictive approach
- summaries of histories

Both are valid, this is an exploration of what both bring to the table in terms of state construction and provide ideas for future work.

Ease of use of the history approaches, potential improvement in learnability (as shown in GVFNs, and discussed in the PSR literature).

Methods to deal with partial observability:
- Static histories based approaches
- PoMDPs/Belief States
- PSRs/TDNets
- Recurrent networks
  - RNNs
  - RNNs/models in them
  - TDNets?
  - Predictive state recurrent networks

**** What is my current thinking?
What is the problem:
- Partial observability in an embodied environment?
- Partial observability in an agent based system.
- Taking state construction seriously.
- Retrospective on state construction techniques.
- 

What is the set of solution methods:

*** More structured thinking/outline

- goal of the document is to think about "state construction".
  - Decompose the terms "state" and "construction" in context of the literature
  - Construction is not limited to composing fixed random functions or the schema mechanism.
- Searching and sorting. Q: What are we searching for? A: Something which helps us maximize return.
- What could we want when maximizing reward
  - Markov state?
  - sufficient statistic of the history of observations?
  - core tests -> ability to predict anything?

- Thesis statement: While many authors have proposed different algorithms for state construction, we take the attitude that little is known about how each of these work in prediction and control. This thesis will be focused on understanding and developing on current algorithms for state construction.

- This document is meant to:
  - Explore potential state constructing methods, discuss extensions, propose future research.
  - History based approaches, prediction based approaches
  - Understanding, understanding, understanding. Sensible recommendations for the current state of state construction.
  - What can we do to further the two approaches? What do both give? Problems with both?


What sections do I want to write?
- Introduction (1):
  - What specific research question are we addressing?
- Reinforcement Learning (2)
  - Agent perspective
  - Goal of an agent
  - Parts of an agent
- Predictions (Horde) (3/4)
  - Learning Predictions (resampling)
- Perception and Partial Observability (5)
- Recurrent neural networks in and out of RL (6)
- We have a long way to go in understanding and using rnns in RL (7/8/8.5?)
- Predictive state representations in and out of RL (9)
- Applying GVFs to learn state representations (10/11/12)
- Future Work (13)


