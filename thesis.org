#+title: Developing algorithms for learning predictive representations of state
#+FILETAGS: :THESIS:
#+author: Matthew Schlegel
#+STARTUP: overview
#+STARTUP: latexpreview
#+OPTIONS: toc:nil
#+OPTIONS: title:nil
#+OPTIONS: ':t
#+LATEX_CLASS: thesis
#+LATEX_HEADER: \input{variables.tex}
#+MACRO: c #+latex: %


*************** TODO [#A] Re-organize the thesis document
Outline:
1. Introduction
2. Background - RL
3. Background - RNNs
4. State construction in reinforcement learning
5. Recurrent neural networks for RL + Known problems
6. Empirical comparisons of various cells
7. General Value Function Networks for state construction
8. Empirical Comparison of various architectures
9. Discovery
10. Off-policy prediction?
11. Conclusions
*************** END


* Preamble                                                           :ignore:
#+begin_comment
Preamble for UofA thesis. Needed to make thesis compliant. I use this in my candidacy as well, with specific
details commented out for brevity. This makes:
- title page
- abstract page
- table of contents
- list of tables
- list of figures

and sets formatting up for main text.
#+end_comment

#+BEGIN_EXPORT LaTeX

\renewcommand{\onlyinsubfile}[1]{}
\renewcommand{\notinsubfile}[1]{#1}

\preamblepagenumbering % lower case roman numerals for early pages
\titlepage % adds title page. Can be commented out before submission if convenient

\subfile{\main/tex/abstract.tex}

\doublespacing % possible options \truedoublespacing, \singlespacing, \onehalfspacing

%%%%%%%
% Additional files for thesis
%%%%%% 

% Below are the dedication page and the quote page. FGSR requirements are not
% clear on if you can have one of each or just one or the other. They do say to
% ask your supervisor if you should have them at all.
%
% The CS Department links to a comparison of pre- and post-Spring 2014 thesis
% guidelines (https://www.ualberta.ca/computing-science/graduate-studies/current-students/dissertation-guidelines)
% The comparison document lists an optional dedication page, but no quote page.

\subfile{\main/tex/preface.tex}
\subfile{\main/tex/dedication.tex}
\subfile{\main/tex/quote.tex}
\subfile{\main/tex/acknowledgements.tex}


\singlespacing % Flip to single spacing for table of contents settings
               % This has been accepted in the past and shouldn't be a problem
               % Now the table of contents etc.
               
\tableofcontents
\listoftables  % only if you have any
\listoffigures % only if you have any

% minimal support for list of plates and symbols (Optional)
%\begin{listofplates}
%...            % you are responsible for formatting this page.
%\end{listofplates}
%\begin{listofsymbols}
%...            % You are responsible for formatting this page
%\end{listofsymbols}
               
% A glossary of terms is also optional
\printnoidxglossaries
               
% The rest of the document has to be at least one-half-spaced.
% Double-spacing is most common, but uncomment whichever you want, or 
% single-spacing if you just want to do that for your personal purposes.
% Long-quoted passages and footnotes can be in single spacing
\doublespacing % possible options \truedoublespacing, \singlespacing, \onehalfspacing

\setforbodyoftext % settings for the body including roman numeral numbering starting at 1

#+END_EXPORT





* Introduction
:PROPERTIES:
:CUSTOM_ID: chap:introduction
:END:


*************** TODO [#B] What is my thesis statement now?
The proposal is centered on what GVFs can bring to the table in terms of learnability in recurrent networks. Now we want to incorporate RNNs more into the discussion. What should we do?
- Focus on understanding: The goal of my work generally is to understand. What are RNNs brining to the table, what are GVFNs brining to the table. Are they compatible?
- partial observability
- some History of RNNs in RL/online data.
- some History of pred reps.
- some History of perception.
*************** END

** What Am I writing the document about?

This document is primarily about partial observability in reinforcement learning.

Why focus on partial observability?

State Construction is...?
- Levels of state construction:
  - Reactive/low-level state vs abstractions for state?
  - What do we want to learn in a state? -> We don't know!
  - There isn't a clear set of criteria for determining what makes for a good state in reinforcement learing
    - Separability? Good Representations properties? Predictive of final task?

- At what abstraction should we be focused?
  - Low level: predictions in the sensor space.
  - High level: predictions/planning in the abstract/concept space.
  - Are these different??

Perception as a series of modules:
- "Is this a face?" much easier than "Is this x's face?"
- The brain is not just one big classification network, submodules are used to specialize. But "how to use submodules" is a hard question.
- Separate the conscious brain from the acting brain.
  - Audio circuit which short circuits the brain to act in the face of a loud noise -> no "control"
  - Other short circuits that bring visual stimuli towards the mid brain for control signals.
- RL is studying the algorithms of the mid brain/cerebellum. We should avoid extending the lessons we learn here to the entire functioning of the brain. In our studies of intelligence we need to be multi-modal. There isn't a single way to conceptualize the concepts, and finding the true underlying properties of the brains algorithms are beyond our capabilities to model mathematically.
- To understand intelligence, we must take the whole embodiment into consideration.

Two philosophies in state building:
- predictive approach
- summaries of histories

Both are valid, this is an exploration of what both bring to the table in terms of state construction and provide ideas for future work.

Ease of use of the history approaches, potential improvement in learnability (as shown in GVFNs, and discussed in the PSR literature).

Methods to deal with partial observability:
- Static histories based approaches
- PoMDPs/Belief States
- PSRs/TDNets
- Recurrent networks
  - RNNs
  - RNNs/models in them
  - TDNets?
  - Predictive state recurrent networks



* Postamble                                                          :ignore:

#+begin_export latex
\printbibliography
\appendix
#+end_export

