#+title: Developing algorithms for learning predictive representations of state
#+FILETAGS: :THESIS:
#+author: Matthew Schlegel
#+STARTUP: overview
#+STARTUP: latexpreview
#+OPTIONS: toc:nil
#+OPTIONS: title:nil
#+OPTIONS: ':t
#+LATEX_CLASS: thesis
#+LATEX_HEADER: \input{variables.tex}
#+MACRO: c #+latex: %


*************** DONE [#C] Thesis re-org
CLOSED: [2022-09-06 Tue 14:00]
Outline:
1. Introduction
2. Background - RL
3. Background - RNNs
4. State construction in reinforcement learning
5. Recurrent neural networks for RL + Known problems
6. Empirical comparisons of various cells
7. General Value Function Networks for state construction
8. Empirical Comparison of various architectures
9. Discovery
10. Off-policy prediction?
11. Conclusions
*************** END


* Preamble                                                           :ignore:
#+begin_comment
Preamble for UofA thesis. Needed to make thesis compliant. I use this in my candidacy as well, with specific
details commented out for brevity. This makes:
- title page
- abstract page
- table of contents
- list of tables
- list of figures

and sets formatting up for main text.
#+end_comment

#+BEGIN_EXPORT latex

\renewcommand{\onlyinsubfile}[1]{}
\renewcommand{\notinsubfile}[1]{#1}

\preamblepagenumbering % lower case roman numerals for early pages
\titlepage % adds title page. Can be commented out before submission if convenient

\subfile{\main/tex/abstract.tex}

\doublespacing % possible options \truedoublespacing, \singlespacing, \onehalfspacing

%%%%%%%
% Additional files for thesis
%%%%%% 

% Below are the dedication page and the quote page. FGSR requirements are not
% clear on if you can have one of each or just one or the other. They do say to
% ask your supervisor if you should have them at all.
%
% The CS Department links to a comparison of pre- and post-Spring 2014 thesis
% guidelines (https://www.ualberta.ca/computing-science/graduate-studies/current-students/dissertation-guidelines)
% The comparison document lists an optional dedication page, but no quote page.

\subfile{\main/tex/preface.tex}
\subfile{\main/tex/dedication.tex}
\subfile{\main/tex/quote.tex}
\subfile{\main/tex/acknowledgements.tex}


\singlespacing % Flip to single spacing for table of contents settings
               % This has been accepted in the past and shouldn't be a problem
               % Now the table of contents etc.
               
\tableofcontents
\listoftables  % only if you have any
\listoffigures % only if you have any

% minimal support for list of plates and symbols (Optional)
%\begin{listofplates}
%...            % you are responsible for formatting this page.
%\end{listofplates}
%\begin{listofsymbols}
%...            % You are responsible for formatting this page
%\end{listofsymbols}
               
% A glossary of terms is also optional
\printnoidxglossaries
               
% The rest of the document has to be at least one-half-spaced.
% Double-spacing is most common, but uncomment whichever you want, or 
% single-spacing if you just want to do that for your personal purposes.
% Long-quoted passages and footnotes can be in single spacing
\doublespacing % possible options \truedoublespacing, \singlespacing, \onehalfspacing

\setforbodyoftext % settings for the body including roman numeral numbering starting at 1

#+END_EXPORT





* Introduction
:PROPERTIES:
:CUSTOM_ID: chap:introduction
:END:

** Objective

The thesis seeks to explore solution methods and architectures for discovering agent state through gradients. Particularly, we seek to confirm the thesis statement

#+BEGIN_QUOTE
The assumptions developed in supervised learning for state discovery are limiting in the reinforcement learning and continual reinforcement learning settings.
#+END_QUOTE


** Approach

The approach taken in this thesis centers on clearly formulating various architectures and algorithms for state discovery and developing in-depth investigative experiments to uncover the dynamics of these algorithms and the resulting discovered state.

1. We approach discovery through gradient descent, articulating the differences between architectures and decisions. Using truncated BPTT (either through experience replay or full online systems) to estimate gradients.
2. Clearly lay out literature in a digestible way to find interesting trends that need further investigation.
3. Adopt the predictive perspecitve of Adam.
4. Perform in-depth experiments in small illustrative domains rather than leaping directly to large domains.

** Contributions

In this section, I outline the specific contributions made to the field of machine intelligence and reinforcement learning to satisfy the requirements of the doctoral degree at University of Alberta.

1. Discuss prediction learning off-policy in light of the experience replay buffer. Investigate the first application of resampling in off-policy learning, and think about its consequences for future development.
2. Explore in detail directly using gradients in recurrent networks to discover state. Specifically, empirically testing various architectures for incorporating actions into the state. (justify these as different??)
2.1 Layout open questions and problems in learning in partially observable domains and specific solution problems in recurrent learning.
2.2 Make a recommendation on a network change for reinforcement learning applications.
2.3 Deep investigative experiments uncovering what the agent state dynamics look like, make recommendations for future work in understanding recurrent agents.
3. Explore one direction layed out in the open problems, specifically encoding the state of a recurrent network as GVFs.
3.1 Develop the GVFN approach and connect it to predictive representation of state literature.
3.2 Develop an extension to the gradient TDN objective for GVFNs.
3.3 Propose a baseline discovery approach for finding GVFs.

- Applied the importance re-sampling technique in learning predictions in the reinforcement learning.
- Extensive empirical analysis of various recurrent architectures for incorporating action.
- Formulating and empirically evaluating a novel predictive state representation, general value function networks (GVFNs), to learn long-temporal dependencies. The first comparison of a TDNet style architecture to a basic recurrent architecture.

** Thesis Layout
** Thoughts



*************** DONE [#B] What is my thesis statement now?
CLOSED: [2022-09-06 Tue 13:59]
The proposal is centered on what GVFs can bring to the table in terms of learnability in recurrent networks. Now we want to incorporate RNNs more into the discussion. What should we do?
- Focus on understanding: The goal of my work generally is to understand. What are RNNs brining to the table, what are GVFNs brining to the table. Are they compatible?
- partial observability
- some History of RNNs in RL/online data.
- some History of pred reps.
- some History of perception.
*************** END

** What Am I writing the document about?

This document is primarily about partial observability in reinforcement learning.

Why focus on partial observability?

State Construction is...?
- Levels of state construction:
  - Reactive/low-level state vs abstractions for state?
  - What do we want to learn in a state? -> We don't know!
  - There isn't a clear set of criteria for determining what makes for a good state in reinforcement learning
    - Separability? Good Representations properties? Predictive of final task?

- At what abstraction should we be focused?
  - Low level: predictions in the sensor space.
  - High level: predictions/planning in the abstract/concept space.
  - Are these different??

Perception as a series of modules:
- "Is this a face?" much easier than "Is this x's face?"
- The brain is not just one big classification network, submodules are used to specialize. But "how to use submodules" is a hard question.
- Separate the conscious brain from the acting brain.
  - Audio circuit which short circuits the brain to act in the face of a loud noise -> no "control"
  - Other short circuits that bring visual stimuli towards the mid brain for control signals.
- RL is studying the algorithms of the mid brain/cerebellum. We should avoid extending the lessons we learn here to the entire functioning of the brain. In our studies of intelligence we need to be multi-modal. There isn't a single way to conceptualize the concepts, and finding the true underlying properties of the brains algorithms are beyond our capabilities to model mathematically.
- To understand intelligence, we must take the whole embodiment into consideration.

Two philosophies in state building:
- predictive approach
- summaries of histories

Both are valid, this is an exploration of what both bring to the table in terms of state construction and provide ideas for future work.

Ease of use of the history approaches, potential improvement in learnability (as shown in GVFNs, and discussed in the PSR literature).

Methods to deal with partial observability:
- Static histories based approaches
- PoMDPs/Belief States
- PSRs/TDNets
- Recurrent networks
  - RNNs
  - RNNs/models in them
  - TDNets?
  - Predictive state recurrent networks

*** What is my current thinking?
What is the problem:
- Partial observability in an embodied environment?
- Partial observability in an agent based system.
- Taking state construction seriously.
- Retrospective on state construction techniques.
- 

What is the set of solution methods:

** More structured thinking/outline

- goal of the document is to think about "state construction".
  - Decompose the terms "state" and "construction" in context of the literature
  - Construction is not limited to composing fixed random functions or the schema mechanism.
- Searching and sorting. Q: What are we searching for? A: Something which helps us maximize return.
- What could we want when maximizing reward
  - Markov state?
  - sufficient statistic of the history of observations?
  - core tests -> ability to predict anything?

- Thesis statement: While many authors have proposed different algorithms for state construction, we take the attitude that little is known about how each of these work in prediction and control. This thesis will be focused on understanding and developing on current algorithms for state construction.

- This document is meant to:
  - Explore potential state constructing methods, discuss extensions, propose future research.
  - History based approaches, prediction based approaches
  - Understanding, understanding, understanding. Sensible recommendations for the current state of state construction.
  - What can we do to further the two approaches? What do both give? Problems with both?


What sections do I want to write?
- Introduction (1):
  - What specific research question are we addressing?
- Reinforcement Learning (2)
  - Agent perspective
  - Goal of an agent
  - Parts of an agent
- Predictions (Horde) (3/4)
  - Learning Predictions (resampling)
- Perception and Partial Observability (5)
- Recurrent neural networks in and out of RL (6)
- We have a long way to go in understanding and using rnns in RL (7/8/8.5?)
- Predictive state representations in and out of RL (9)
- Applying GVFs to learn state representations (10/11/12)
- Future Work (13)




* Background

** Reinforcement Learning
*** Partial Observability
** Temporal Abstractions
*** General Value Functions
*** Options
** Off-policy Learning
** Behaving and Learning
** Linear Function Approximation
** Deep Reinforcement Learning for Prediction and Control
*** Neural Networks
*** Recurrent Neural Networks
** Recurrent Neural Networks in Reinforcement Learning

For effective prediction and control, the agent requires a state representation $\agentstate_t$ that is a sufficient statistic of the past: $\Expected\left[ G^c_t | \agentstate_t \right] = \Expected\left[G^c_t | \agentstate_t, \history_t\right]$. 
{{{c}}}
{{{c}}}
When the agent learns such a state, it can build policies and value functions without the need to store any history. For example, for prediction, it can learn $V(\agentstate_t) \approx \Expected\left[ G^c_t | \agentstate_t \right]$.


An RNN provides one such solution to learning $\agentstate_t$ and associated state update function. The simplest RNN is one which learns the parameters $\weights \in \Reals^\numparams$ recursively
{{{c}}}
\[
  \agentstate_t = \sigma(\weights \xvec_t + \bvec)
\]
{{{c}}}
where $\xvec_t = [\obs_t, \agentstate_{t-1}]$ and $\sigma$ is any non-linear transfer function (typically tanh). While concatenating information (or doing additive operations) has become standard in RNNs, another idea explored earlier in the literature and in more modern cells is using multiplicative operations
{{{c}}}
\[
  (\agentstate_t)_i = \sigma\left(\sum_{j=1}^M \sum_{k=1}^N\weights_{ijk} (\obs_t)_j (\agentstate_{t-1})_k + \bvec_i\right) \quad\quad \triangleright \text{ where } \weights \in \Reals^{|\agentstate| \times |\obs| \times |\agentstate| }.
\]
{{{c}}}
Using this type of operation was initially called second-order RNNs \citep{goudreau1994}, and was also explored in one of the first landmark successes of RNNs \citep{sutskever2011} in a character-level language modeling task.




RNNs are typically trained through the use of back-propagation through time \citep{mozer1995focused}. This algorithm effectively unrolls the network through the sequence and calculates the gradient as if it was one large network with shared weights.
This unrolling is often truncated at some number of steps $\tau$. While this alleviates computational-cost concerns, the learning performance can be sensitive to the truncation parameter \citep{pascanu2013difficulty}. When calculating the gradients through time for a specific sample, we follow \citep{schlegel2020general} and define our loss as
{{{c}}}
\[
  \mathcal{L}_{t}(\weights) = \sum_{i}^{N} (v_i(\agentstate_t(\weights)) - y_{t, i})^2
\]
{{{c}}}
where $N$ is the size of the batch, and $y$ is the target defined by the specific algorithm. This effectively means we are calculating the loss for a single step and calculating the gradients from that step only.


There are several known problems with simple recurrent units (and to a lesser extent other recurrent cells). The first is known as the vanishing and exploding gradient problem \citep{pascanu2013difficulty}. In this, as gradients are multiplied together (via the chain rule in BPTT) the gradient can either become very large or vanish into nothing. In either case, the learned networks often cannot perform well and a number of practical tricks are applied to stabilize learning \citep{bengio2013}. The second problem is called saturation. This occurs when the weights $\weights$ become large and the activations of the hidden units are at the extremes of the transfer function. While not problematic for learning stability, this can limit the capacity of the network and make tracking changes in the environment dynamics more difficult \citep{chandar2019}.

# We focus our experiments around the simple recurrent cells (RNNs) and GRUs.
Long-short term memory cells (LSTM) were developed to address the issues with modeling long-temporal dependencies.

Gated-recurrent units (GRU) are a modification from the LSTM cell which maintains performance in many settings, improves ease of use, and improves computational footprint.


Finally, to improve sample efficiency we incorporate experience replay, a critical part of a deep (recurrent) system in RL \citep{mnih2015human, hausknecht2015}. There are two key choices here: how states are stored and updated in the buffer and how sequences are sampled. We store the hidden state of the cell in the experience replay buffer as apart of the experience tuple. This is then used to initialize the state when we sample from the buffer for both the target and non-target networks. We pass back gradients to the stored state to update them along with our model parameters, see a full discussion in Section \ref{sec:open_problems}. We also stored a separate initial state for the beginning of episodes, which was updated with gradients. If we sampled the beginning of an episode from the replay we used the most up to date version of this vector to initialize the hidden state. For sampling, we allowed the agent to sample states across the episode. For samples at the end of the episode, we simply use a shorter sequence length than $\tau$.
** Backpropagation through time and Temporal Sensitivities
** Summary
* The Predictive Perspective
:PROPERTIES:
:CUSTOM_ID: chap:perd_persp
:END:

In this chapter, I will outline what I mean by the predictive perspective and discuss the commitments this view has on the interactions between an agent and its environment.



** The World and the Agent

*The Environment*
   - The world
   - Environment states
   - Stochasticity or Partial Observability?

*The Problem (header section?)*
   - Maximizing the (discounted?) return.
   - Predicting the return

*The Agent*
   - Smaller than the world
   - Perception, Behavior, Mind-Body Interface
   - State representations

** Order of Prediction
** World Knowledge Representation through Predictions
** Prediction in Reinforcement Learning
** The Effects of Prediction on Control
** Other Theories of Prediction

* Composite General Value Functions
* Learning Predictions Off-policy using Importance Resampling
(Point to paper for theory)

** Algorithm
** Empirical Results

* Perception and Partial Observability (Part 2?)

From here on we will primarily consider the setting where the agent observes its world through limited senses. This setting is often known as the partially observable setting in reinforcement learning. In this thesis, we focus on partial observability in terms of the agent-centric observations, emphasizing the discussion held in 


- State, credit assignment/search through the functional space
- Environment State, Agent State, Representations
- Working towards a better definition of what we want from state -> Better path of discovery for new algorithms which learn state.
- Focus is on understanding prior methods through empirical investigations, developing these methods using modern tools, and making recommendations for the future.
** Problem Formulation


We consider a partially observable setting, where the observations are a function of an unknown, unobserved underlying state.
The dynamics are specified by transition probabilities $\Pfcn = \States \times \Actions \times \States \rightarrow [0,\infty)$ with state space $\States$ and action-space $\Actions$. On each time step the agent receives an observation vector $\obs_t \in \Observations \subset \Reals^\obssize$, as a function $\obs_t = \obs(\state_t)$ of the underlying state $\state_t \in \States$. The agent only observes $\obs_t$, not $\state_t$, and then takes an action $\action_t$, producing a sequence of observations and actions: $\obs_{0}, a_{0}, \obs_{1}, a_1, \ldots$.

The goal for the agent under partial observability is to identify a state representation $\svec_t \in \RR^\numgvfs$ which is a sufficient statistic (summary) of past interaction, for targets $y_t$. More precisely, such a \emph{sufficient state} ensures that $y_t$ given this state is independent of history $\hvec_t = \obs_0, a_{0}, \obs_1, a_1, \ldots, \obs_{t-1}, a_{t-1}, \obs_{t}$,
{{{c}}}
{{{c}}}
\begin{equation}
  p(y_{t} | \svec_t) = p(y_{t} | \svec_t, \hvec_t)
\end{equation}
{{{c}}}
{{{c}}}
or so that statistics about the target are independent of history, such as $\mathbb{E}[Y_{t} | \svec_t] = \mathbb{E}[Y_{t} | \svec_t, \hvec_t]$.
Such a state summarizes the history, removing the need to store the entire (potentially infinite) history.
Note here that this is a less stringent definition of sufficient state than used for PSRs \citep{littman2001predictive}, where the state is constructed for predictions about all future outcomes. We presume that the agent has a limited set of targets of interest, and needs to find a sufficient state for just those targets. For example, a potential set of targets is the observation vector on the next time step.


** Sufficient state
** Discovery, search, and credit assignment
** Long Temporal Abstractions vs embodied state






** Open Problems using RNNs in DRL
*** Open problems for history dependent architectures.
*** Solution method issues
* How do we incorporate action into a recurrent network?


\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{plots/figures/RNN.pdf}
  \caption{Visualizations of the multiplicative and additive RNNs.}
\label{fig:viz_rnn}
\end{figure}

In this paper, we define two broad categories for incorporating action into the state update function of an RNN, and discuss various variations on these ideas (see Figure \ref{fig:viz_rnn} for a visualization of two main architectures).


** Additive

The first category is to use an additive operation. The core concept of additive action recurrent networks is concatenating an action embedding as an input into the recurrent cell. For example, the update becomes
{{{c}}}
\begin{align*}
  \state_t = \sigma\left( \Wmat^\xvec \xvec_t + \Wmat^\avec \avec_{t-1} + \bvec \right) \tag*{\bf (Additive)}
\end{align*}
{{{c}}} 
{{{c}}} 
where $\Wmat^\xvec$ and $\Wmat^\avec$ are appropriately sized weight matrices. This requires no changes to the recurrent cell. This update function has been explored several times before (see \cite{schaefer2007recurrent, zhu2017improving}).

The additive approach was explored in \cite{zhu2017improving} where they modified the architecture slightly to learn a function of the action input $\avec_t = f_a(a_t)$. As in their architecture, we concatenate this representation with observation encoding right before the recurrent network. This enables us to focus on the changes in the basic operation rather than enumerating all possible places the action can be concatenated before the recurrent operation.

** Multiplicative

The second category is inspired by second-order RNNs \citep{goudreau1994} and first appeared as a part of a state update function in \cite{rafols2006}, where the observation, hidden state, and action embedding are integrated using a multiplicative operation: 
{{{c}}}
\begin{align*}
  \state_t = \sigma\left(\Wmat \times_2 \xvec_{t} \times_3 \avec_{t-1}\right),  \tag*{\bf (Multiplicative)}
\end{align*}
{{{c}}}
where $\Wmat \in \Reals^{|\state_t| \times |\xvec_t| \times |\avec_{t-1}|}$ and $\times_n$ is the $n$-mode product. This type of operation is known to expand the types of functions learnable by a single layer RNN \citep{goudreau1994, sutskever2011}, and decreases the networks sensitivity to truncation \citep{schlegel2020general}. 

While this type of update has very clear advantages, there is also a tradeoff in terms of number of parameters and potential re-learning depending on the granularity of the action representation. For example, in the Ring World experiment above the RNN cell with additive used 285 parameters with hidden state size of $15$. The multiplicative version would have used 510 parameters with the same hidden state size. While this doesn't seem like a lot, if we compare what it would be in a domain like Atari (with 18 actions, 1024 inputs, and $|s_t| = 1024$) the number of parameters would be ~2 million vs ~38 million respectively. As shown below in the empirical study, the size of the state can be significantly when using a multiplicative update. In any case, it would be worthwhile to develop strategies to reduce the number of parameters, which we discuss next.

\subsection{Reducing parameters of the Multiplicative}

The first way we can reduce the number of parameters is by using a low-rank approximation of the tensor operations. Like matrices, tensors have a number of decompositions which can prove useful. For example, every tensor can be factorized using canonical polyadic decomposition, which decomposes an order-N tensor $\Wmat \in \Reals^{I_1 \times I_2 \times \ldots \times I_N}$ into n matrices as follows
{{{c}}}
\begin{align*}
  \Wmat_{i_1, i_2, \ldots} &= \sum_{r=1}^\factors \lambda_r \Wmat^{(1)}_{i_1, r}  \Wmat^{(2)}_{i_2, r}  \ldots \Wmat^{(N)}_{i_N, r}
\end{align*}
{{{c}}}
where $\Wmat^{(j)} \in \Reals^{I_j \times \factors}$, and $\factors$ is the rank of the tensor. This is a generalization of matrix rank decomposition and exists for all tensors with finite dimensions, see Appendix \ref{app:tensors} for more details. We can make several simplifications using the properties of n-mode products. Using the  definition of the multiplicative RNN update,
{{{c}}}
{{{c}}}
\begin{align*}
  \Wmat \times_2 \xvec_t \times_3 \avec_{t-1}
  &\approx \boldsymbol{\lambda} \Wmat^{out} \left(\xvec_t\Wmat^{in} \odot \avec_{t-1}\Wmat^{a}\right)^\trans
     \quad \triangleright \boldsymbol{\lambda}_{i,i} = \lambda_i.  \tag*{\bf(Factored)}
\end{align*}

Previous work explored using a low-rank approximation of a multiplicative operation. A multiplicative update was used to make action-conditional video predictions in Atari \citep{oh2015}.  This operation also appears in a Predictive State RNN hidden state update \citep{downey2017a}, albeit it never performed as well as the full rank version. Our low rank approximation is also similar to the network used in \cite{sutskever2011}, where they mention optimization issues (which were overcome through the use of quasi-second order methods).

Another approach to reducing the number of parameters required---and to reduce redundant learning---by using an action embedding rather than a one-hot encoding. For example, in Pong it is known that only ~5 actions matter. By taking advantage of the structure of the action space we could potentially further reduce the number of parameters required to get these benefits. We explore this architecture briefly in Section \ref{app:sec:deep_action}. While this is an important piece of the puzzle, we do not focus on learning good action embeddings in this paper and leave it to future work.

** Empirical Results - ARNNs

In the following sections, we set out to empirically evaluate the three operations for incorporating action into the state update function: {\bf N}o {\bf A}ction input (``{\bf NA}''), {\bf A}dditive (``{\bf AA}''), {\bf M}ultiplicative (``{\bf MA}''), {\bf Fac}tored (``{\bf Fac}''), {\bf D}eep {\bf A}dditive (``{\bf DA}''). We explore all the variants using both stanard RNNs and a GRU cell. Our experiments are primarily driven by the main hypothesis that the multiplicative will strictly outperform the other variants, as suggested by \cite{schlegel2020general}. To explore this hypothesis we focus on two main empirical questions:
\begin{itemize}
\item How do the different cells effect the ``learnability'' of the agent and the properties of the learned state?
\item Are there examples where the other variants outperform the multiplicative variant?
\end{itemize}


In all control experiments, we use an $\epsilon$-greedy policy with $\epsilon=0.1$. All networks are initialized using a uniform Xavier strategy \citep{glorot2010understanding}, with the multiplicative operation independently normalizing across the action dimension (i.e. each matrix associated with an action in the tensor is independently sampled using the Xavier distribution). Unless otherwise stated, we performed a hyperparameter search for all models using a grid search over various parameters (listed appropriately in the Appendix \ref{app:emp}). To best to our ability we kept the number of hyperparameter settings to be equivalent across all models, except the factored variants which use several combinations of hidden state size and number of factors. The best settings were selected and reported using independent runs with seeds different from those used in the hyperparameter search, unless otherwise specified.

All experiments were run using an off-site cluster.
In total, for all sweeps and final experiments we used $\sim 20$ cpu years, which was approximated based off the logging information used by the off-site cluster. 
%We did not use any GPUs for the reported experiments. 
All of our code is written in Julia \citep{bezanson2017julia}, and we use Flux and Zygote as our deep learning and auto-diff backend \citep{innes:2018, Zygote.jl-2018}.


*** Investigating Learnability
:PROPERTIES:
:CUSTOM_ID: sec:arnn:learnability
:END:

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{plots/figures/ringworld_trunc.pdf}
  \caption{Ring World sensitivity curves of RMSVE over the final 50k steps for CELL (hidden size) {\bf (left)} RNN (15), AARNN (15), MARNN (12), FacRNN (12 [solid] and 15 [dashed]), DARNN (12, $|\avec|=2$), and {\bf (right)} GRU (12), AAGRU (12), MAGRU (9), FacGRU (9 [solid] and 12 [dashed]), DAGRU (9, $|\avec|=10$). Reported results are averaged over 50 runs with a $95\%$ confidence interval. FacRNN used factors $\factors=\{12, 8\}$ respectively, and FacGRU used $\factors=\{14, 12\}$. All agents were trained over 300k steps. \vspace{-0.5cm}} \label{fig:rw_sens}
\end{figure}

We explore the first empirical qeustion by revisiting the Ring World environment, specifically to test model performance with various truncations, and to compare the architecture's learned state. In this domain, we set the goal is to predict when the observation will be active, which is deterministically active in the first state and off in the remaining states. The agent can take actions moving either clockwise or counter clockwise in the environment. The agent must keep track of how far it has moved from the active bit. For all experiments we use a Ring World with 10 underlying states.

The agent learns a total of 20 GVFs with state-termination continuation functions of  $\gamma \in \{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\}$. When the agent observes the active bit in Ring World (i.e. enters the first state) the predictions are terminated (i.e. $\gamma = 0.0$). The GVFs use the observed bit as a cumulant. Half follow a persistent policy of going clockwise and the other follow the opposite direction persistently. The agent follows an equiprobable random behavior policy. The agent updates its weights on every step following a off-policy semi-gradient TD update with a truncation value of $\tau=6$ for the ER setting. We train the agent for $300000$ steps and averaged over 50 independent runs. We provide two versions of the factored cells: one each with the state size set as the additive operation (dashed) and multiplicative operation (solid).

*Results:*

For both the RNN and GRU cells the MA variant performs the best, while the additive performs the worst of the cells which include action information. Interestingly, the factored variants for the GRU perform almost identically, while the FacRNN with a smaller hidden state perform marginally better. All factored variants straddled the performance of the additive and multiplicative updates. The DAAGRU performs similarly to the AAGRU, while the DAARNN fails to learn in this setting. Finally, the MARNN performs the best overall, only needing a truncation value of $\tau=6$ to learn, which is shorter than the Ring World. We conclude that with the same number of parameters, the operation used to update the state can have a significant effect on the required sequence length and final performance.

\begin{wrapfigure}[25]{r}{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/figures/ringworld_pred_truth_vert.pdf}
  \caption{Ring World predictions of $\text{seed}=62$ for the multiplicative and additive RNNs. Discounts listed with the target policy persistently going counter-clockwise.} \label{fig:rw_pred}
\end{wrapfigure}

To ground the prediction error reported, we present two representative examples of the learned predictions for the additive and multiplicative RNNs in Figure \ref{fig:rw_pred}. These plots show a single seed (selected as the best for the additive) over a small snippet of time, but are representative of our observations of the general performance for both cells. The multiplicative follows the actual prediction within a small delta being as close to zero error as we should expect, while the additive has many artifacts and other miss-predictions for both the myopic ($\gamma = 0.0$) and long-horizion ($\gamma=0.9$) predictions. In Figure \ref{fig:rw_ind_lcs}, we report all the individual learning curves for the additive and multiplicative.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{plots/figures/ringworld_ind_lcs.pdf}
  \caption{Individual learning curves for the additive (hidden size of 15) and multiplicative (hidden size 12) RNNs in Ring World with truncation $\tau=6$. The plots are smoothed with a moving average with 1000 step window sizes. The gray box denotes the seed used in Figures \ref{fig:rw_pred} and \ref{fig:rw_tsne}. Overall, we see the multiplicative is quite resilient to initialization, but the distance from zero error in Figure \ref{fig:ring_world_example} can be explained by a few bad initializations.
    \vspace{-0.5cm}
  }\label{fig:rw_ind_lcs}
\end{figure}
{{{c}}}
{{{c}}}
\begin{figure}
  \includegraphics[width=\linewidth]{plots/figures/tsne_combined.pdf}
  \caption{TSNE plots for the additive and multiplicative RNNs for truncation $\in \{1, 6\}$. Given the learning objective (described in Section \ref{sec:arnn:learnability}), we would want the state to have 10 distinct clusters for each state of the underlying environment. We should expect the truncation $\tau=1$ to not be able to produce this kind of state for either cell variant. The learning curves correspond to a single seed. The top scatter plots are colored on the underlying state the agent is currently in, the bottom scatter plots are colored based on the previous action the agent took. We initialized TSNE with the same random seed, with max iterations set to 1000, and perplexity set to 30. {\bf (top)} both the {\bf (left)} additive and {\bf (right)} multiplicative use seed=62 (best seed for the additive), {\bf (bottom)} median seeds for both cells {\bf (left)} additive uses seed=55 and {\bf (right)} multiplicative uses seed=67.} \label{fig:rw_tsne}
\end{figure}

*Looking beyond performance:*

A natural question is why might the multiplicative cell perform significantly better than the other cells in this simple setting? One hypothesis is that the multiplicative cell does a better job at separating the histories on action sequence as compared to the additive operation. While this question is difficult to test, we can peer into the learned state of each cell and see if there are qualitative features that appear to help explain the better performance. To do this we take learned agents over different truncation values started using the same seed. After learning (using the same parameters as in Figure \ref{fig:rw_sens}) we collect another 1000 steps of hidden states. With these hidden states we use TSNE \citep{van2008visualizing} to reduce the space of hidden states to two dimensions. The resulting scatter plots for the additive and multiplicative simple RNNs can be seen in Figure \ref{fig:rw_tsne}.

Overall, we observe the additive and multiplicative separate on the previous action equally well, matching our initial hypothesis. While action is important, the additive seems to be hyper-focused on action even as the cell is able to partition on environment state. The multiplicative, on the other hand, is able to cluster the hidden states for various environment states together with only minor separation on action as seen in states 1 and 7. It is possible this is a natural part of th learning process for both the cells, but the multiplicative is able to cluster the states in less samples. If we look at the median performer (seed=55 and seed=67 for the additive and multiplicative respectively) the additive fails to separate on environment state, while the multiplicative looks similarly to the previous seed.


% While action is important, what seems to be happening is that the multiplicative is focusing gradient information on the actual observation histories as the action separation is done as a hand designed feature of the architecture. This separation is extremely beneficial during the learning process for the Ring World domain, but comes with a downside as the hidden states for the multiplicative cell sometimes are separate on both previous action and state. While this is not an issue for ring world, later we will see where this might be problematic.


*** Understanding when Action Encoding Does and Does Not Matter
:PROPERTIES:
:CUSTOM_ID: sec:arnn:control
:END:

In this section, we investigate learning behavior in two environments with slightly differing properties. The first domains is called TMaze \citep{bakker2002} with a size of 10, which was initially proposed to test the capabilities of LSTMs in RL using Q-Learning. The environment is a long hallway with a T-junction at the end. The agent receives an observation indicating whether the goal state is in the north position or south position at the T-junction (which is randomly chosen at the start of the episode). The agent can take actions in the compass directions. On each step the agent receives a reward of -0.1 and in the final transition receives a reward of 4 or -1 depending if the agent was able to remember which direction the goal was in. The agent deterministically starts at the beginning of the hallway.
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{plots/figures/dirtmaze_and_tmaze.pdf}
  \caption{{\bf (left)} Directional TMaze comparison over the performance averaged over the final $10\%$ of episodes with 100 independent runs trained over 300k steps with $\tau=12$ for CELL (hidden size): RNN (30), AARNN (30), MARNN (18), DARNN (25, $|\avec|=15$), GRU (17), AAGRU (17), MAGRU (10), DAGRU (15, $|\avec|=8$). {\bf (right)} Bakker's TMaze box plots and violin plots over the performance averaged over the final $10\%$ with 50 independent runs. Trained over 300k steps with $\tau=10$. All GRUs use a state size 6, while RNNs use a state size 20. The deep additive used an action encoding of $|\avec|=4$.} \label{fig:tmazes}
\end{figure}

Our control agents are constructed similarly to those used in the Ring World environment. The agent's network is a single recurrent layer followed by a linear layer. We perform a sweep over the size of the hidden state and learning rates, and selected all variants of a cell type to have the same value. We train our network over 300000 steps with further details reported in appendix \ref{app:emp_tm}. We report the learned policy's performance over the final $10\%$ of episodes by averaging the agent success in reaching the correct goal. We report our results using a box and whisker plot with the distribution. The upper and lower edges of the box represent the upper and lower quartiles respectively, with the median denoted by a line. The whiskers denote the maximum and minimum values, excluding outliers which are marked.

Shown in Figure \ref{fig:tmazes} (left), all the cells have similar median performance with the GRU (with no action input) performing the best with the least amount of spread. This conclusion is the same across the size of the hidden state, where the multiplicative and factored variants performed poorly (see Appendix \ref{app:emp} for factored results). While this initially suggests the action embedding is not important beyond our simple Ring World experiment, notice the difference in how the environment's dynamics interact with the agent's action. In the TMaze, the underlying position of the agent is effected by only two of the actions (the East and West action), while the North and South actions only transition to a different state at the very end of the maze. Also, the agent's actions have no effect on what the agent needs to remember, no matter what trajectory the agent sees the meaning of the first observation is always the same. Thus, these results are much less surprising. For example, the multiplicative variants will have to learn the update dynamics multiple times for the North and South actions.

\begin{wrapfigure}[25]{r}{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/figures/dirtmaze_fac.pdf}
  \caption{Sensitivity curves over number of factors $\factors$ with standard error for the {\bf (top)} FacRNN (30) and {\bf (bottom)} FacGRU (17). All agents were trained over 300k steps. See Appendix \ref{app:emp_dtm} for sweeps over different state sizes. We use the data generated by a sweep over the learning rate with 40 runs and compare to the data in figure \ref{fig:tmazes}. The red labels on the x-axis indicate when the network has the same number of parameters as the multiplicative.} \label{fig:dirtmaze_fac}
\end{wrapfigure}

To better replicate these dynamics in TMaze we add a direction component to the underlying state. For example, many robotics systems must be able to orient and turn to progress in a maze, which we hypothesize actions will be critical for modeling the state.  The agent can take an action moving forward, turning clockwise, or turning counter-clockwise. Instead of the observations only being a function of the position, the agents direction plays a critical role. In the first state, the agent receives the goal observation when facing the wall corresponding to the goal's direction. In DirectionalTMaze the agent must contextualize its observation by the action it takes before or after seeing the observation. All other walls have the same observation, and when not facing a wall the agent receives another observation. We evaluate the state updates using the same settings as in the TMaze with results reported in Figure \ref{fig:tmazes} (right). 


Now that the agent must be mindful of its orientation, the action again becomes a critical component in learning. We see the multiplicative variants outperforming all other variants in this domain. Without action, the GRU and RNN are unable to learn, and even the additive and deep additive versions are unable to learn in 300000 steps. We also sweep over the number of factors and report the performance compared to the multiplicative and additive variants as shown in Figure \ref{fig:dirtmaze_fac}. We found that as the factors increase, generally the performance increases as well. This matches our expectations, as with increased factors the factored variants should better approximate the multiplicative variances. But there is a tradeoff when adding too many factors, causing performance to decrease substantially. While the factored variant has some interesting properties, we decide to focus the remaining experiments using the base architectures (NA, MA, AA, DA) and report full results with the factored variant in Appendix \ref{app:emp}.

% While the TMaze and DirTMaze give some insight into when different encodings might be preferable, the DirTMaze and Ring World share similar dynamics in how the actions effect the unobserved state of the MDP. Specifically, there are two actions which effect a state component symmetrically. This prompts the question on whether this property is driving the benefits of the multiplicative update's success, or whether there are other scenarios where the multiplicative does better. We propose a new environment which is a simple grid world with border wrapping. The agent can take a step in all the cardinal directions, and observes when it enters a random subset of the states (all aliased together). The goal state is also randomly selected at the beginning of an agent's life. This creates random action observation patterns the agent must notice and act on to get to the goal. The border wrapping prevents the agent from moving to a corner of the environment and then going to the goal.

% In figure \ref{fig:maskedgw}, we confirm the hypothesis that the improvement with multiplicative update can be meaningful even when the state-action sequences are randomly placed in the environment. While the improvement is much less drastic than the Ring World and DirTMaze, the improvement is still significant with standard error bars. Another interesting observation is the difference matters much more for the simple recurrent update than the GRU.


*** Combining Cell Architectures
:PROPERTIES:
:CUSTOM_ID: sec:arnn:combining
:END:


\begin{SCfigure}
  \includegraphics[width=0.6\linewidth]{plots/figures/combo_cell.pdf}
  \caption{Two variants of combining cells. State size chosen based on procedures of previous environments. ({\bf top}) Performance of success rates ({\bf left}) TMaze with same basic parameters as above for CELL (hidden size): Softmax GRU (6), Cat GRU (6), Softmax RNN (20), Cat RNN (20). ({\bf right}) Directional TMaze with same parameters as above for CELL (hidden size): Softmax GRU (8), Cat GRU (12), Softmax RNN (15), Cat RNN (22). ({\bf Bottom}) Average softmax weights of cells over training with standard error over runs.} \label{fig:combination}
  \vspace{-0.4cm}
\end{SCfigure}

In this section, we consider the effects of combining the additive and multiplicative cells through two types of combination techniques. We see these architectures as a minor step toward building an architecture which learns the structural bias currently hand designed.

We combine the hidden state between an additive and multiplicative operation through two techniques. The first is through an element-wise softmax. Both the additive and multiplicative have the same size hidden state ($\state^a$ and $\state^m$ respectively), and each element of the hidden states are weighted by
{{{c}}}
\[
  \state_i = \frac{e^{\theta^a_i} \state^a_i + e^{\theta^m_i} \state^m_i}{e^{\theta^a_i} + e^{\theta^m_i}}
\]
{{{c}}}
where $\boldsymbol{\theta}^a, \boldsymbol{\theta}^m \in \Reals^\statesize$. This should learn which cell to use depending on the structure of the problem. The second combination is through concatenating the two hidden state together $\state = cat(\state^a, \state^m)$. This gives more room for experts to add more state to the different architectures, but in this work we fix the two architectures to have the same state size.



We compare these combinations to the original architectures in TMaze and Directional TMaze following the same procedure as above. We expect these cells to perform as well as either the additive or the multiplicative (which ever is doing the best in the specific domain). The results can be seen in Figure \ref{fig:combination}. Overall, the softmax combination performs similarly or slightly better than the multiplicative version except in the Directional TMaze for the GRUs. In TMaze, concatenating the two states together performed better than the additive and multiplicative cells, but this operation worked slightly worse than the multiplicative in the Directional TMaze. To test the hypothesis that the softmax weighting should emphasize the better cell in a given domain we show the softmax weighting over the training period. For the TMaze the weightings end being approximately equivalent while the Directional TMaze shows a very distinct separation where the multiplicative is weighted significantly more and the additive is continually down-weighted.


*** Learning State Representations from Pixels

Finally, we perform an empirical study in two environments with non-binary observations. We are particularly interested in whether the recurrent architectures perform comparably when the observation needs to be transformed by fully connected layers, or when the observation is an image. We only use the GRU cells in these experiments. Full details can be found in Appendix \ref{app:emp}.

The first domain we consider is a version of DirectionalTMaze which uses images instead of bit observations. The agent receives a gray scale image observation on every step of size $28\times28$. The agent sees a fully black screen when looking down the hallway, and a half white half black screen when looking at a wall. The agent observes an even (or odd) number sampled from the MNIST \citep{lecun2010mnist} dataset when facing the direction of (or opposite of) the goal. The  rewards are -1 on every step and 4 or -4 for entering the correct and incorrect goal position respectively. We report the same statistic as in the prior TMaze environments, with the environment size set to 6. Notice the hallway size is smaller and the negative reward is larger, this was to speed up learning for all architectures.

Results for the Image DirectionalTMaze can be seen in Figure \ref{fig:scaling_up}. In this domains, the multiplicative performs quite well, although not as well as in the simple version. The AAGRU is unable to learn in this setting, and the deep additive variant performs slightly better than the additive.


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{plots/figures/scale.pdf}
  \caption{{\bf (left)} Image Directional TMaze percent success over the final $10\%$ of episodes for 20 runs for CELL (hidden size): AAGRU (70), MAGRU (32), DAGRU (45, $|\avec| = 128$). Using ADAM trained over 400k steps, $(\tau) = 20$. GRU omitted due to prior performance. {\bf (center)} Lunar Lander average reward over all episodes for CELL (hidden size): GRU (154), AAGRU (152), MAGRU (64), DAGRU (152, $|\avec|=64$) and $(\tau) = 16$. {\bf (right)} Lunar Lander learning curves over total reward. Ribbons show standard error and a window averaging over 100k steps was used. Lunar Lander agents were trained for 20 independent runs for 4M steps. \vspace{-0.4cm}}
\label{fig:scaling_up}
\end{figure}

*** Learning State Representations from Agent-Centric Sensors

The second domain is a partially observable version of the LunarLander-v2 environment from OpenAI Gym \cite{brockman2016openai}. The goal is to land a lander on the moon within a landing area. Further details and results can be found in Appendix \ref{app:emp_ll}. To make the observation partially we remove the anglular speed, and we filter the angle $\theta$ such that it is 1 if $-7.5 \le \theta \le 7.5$ and 0 otherwise. We report the average reward obtained over all episodes, and learning curves.

As seen in Figure \ref{fig:scaling_up}, our findings generalize to this domain as well. The multiplicative variant improves over the factored (see Appendix \ref{app:emp}, additive, and deep additive variants significantly. In the LunarLander environment the multiplicative learns faster, reaching a policy which receives on average 100 total reward per episode. Both the additive and factored eventually learn similar policies, while the standard GRU seems to perform less well (although not statistically significant from the additive variant). The average return is ~100 less than some of the best agents on this domains. When we look at the individual median curves we see the agent does this well $50\%$ of the time (see Appendix \ref{app:emp}). This difference can be explained by the failure start states being more frequent than in the fully observed case.


* Predictive State Representations in Perception

#+begin_comment
The idea that an agent's knowledge might be represented as predictions has a long history in machine learning. The first references to such a predictive approach can be found in the work of \citeA{Cunninghambook}, \citeA{becker1973model}, and \citeA{drescher1991made}, who hypothesized that agents would construct their understanding of the world from interaction, rather than human engineering. These ideas inspired work on predictive state representations (PSRs) \citep{littman2001predictive}, as an approach to modeling dynamical systems. Simply put, a PSR can predict all possible interactions between an agent and it's environment by reweighting a minimal collection of core test (sequence of actions and observations) and their predictions, without the need for a finite history or dynamics model.
Extensions to high-dimensional continuous tasks have demonstrated that the predictive approach to dynamical system modeling is competitive with state-of-the-art system identification methods \citep{hsu2012spectral}.
PSRs can be combined with options \citep{wolfe2006predictive}, and some work suggests discovery of the core tests is possible \citep{mccracken2005online}.
One important limitation of the PSR formalism is that the agent's internal representation of state must be composed exclusively of probabilities of action-observation sequences.

A PSR can be represented as a GVF network by using a myopic $\gamma = 0$ and compositional predictions. For a test $q = \action_1\obs_2$, for example, to compute the probability of seeing $\obs_2$ after taking action $\action_1$, the cumulant is $1$ if $\obs_2$ is observed and $0$ otherwise; the policy is to always take action $\action_1$; and the continuation $\gamma = 0$. To get a longer test, say $\action_0\obs_1\action_1\obs_2$, a second GVF can be added which predicts the output of the first GVF. For this second GVF, the cumulant is the prediction from the first GVF (which predicts the probability of seeing $\obs_2$ given $\action_1$ is taken); the policy is to always take action $\action_0$; and the continuation is again $\gamma = 0$. Though GVFNs can represent a PSR, they do not encompass the discovery methods or other nice mathematical properties of PSRs, such as can be obtained with linear PSRs.

TD networks \citep{sutton2004temporal} were introduced after PSRs, and inspired by the PSR approach to state construction that is grounded in observations.
GVFNs build on and are a strict generalization of TD networks.
A TD network \citep{sutton2004temporal} is similarly composed of $\numgvfs$ predictions, and updates using the current observation and previous step predictions like an RNN. TD networks with options \citep{rafols2005using} condition the predictions on temporally extended actions similar to GVF Networks, but do not incorporate several of the recent modernizations around GVFs, including state-dependent discounting and convergent off-policy training methods.
The key differences, then, between GVF Networks and TD networks is in how the question networks are expressed and subsequently how they can be answered.
GVF Networks are less cumbersome to specify, because they use the language of GVFs. Further, once in this language, it is more straightforward to apply algorithms designed for learning GVFs.

More recently, there has been an effort to combine the benefits of PSRs and RNNs. This began with work on Predictive State Inference Machines (PSIMs) \citep{sun2016learning}, for inference in linear dynamical systems. The state is learned in a supervised way, by using statistics of the future $k$ observations as targets for the predictive state. This earlier work focused on inference in linear dynamical systems, and did not state a clear connection to RNNs. Later work more explicitly combines PSRs and RNNs \citep{downey2017predictive,choromanski2018initialization}, but restricts the RNN architecture to a bilinear update to encode the PSR update for predictive state. In parallel, \citeA{venkatraman2017predictive} proposed another strategy to incorporate ideas from PSRs into RNNs, without restricting the RNN architecture, called Predictive State Decoders (PSDs) \citep{venkatraman2017predictive}. Instead of constraining internal state to be predictions about future observations, statistics about future observations are used as auxiliary tasks in the RNN.

Of all these approaches, the most directly related to GVFNs is PSIMs. This connection is most clear from the PSIM objective \citep[Equation 8]{sun2016learning}, where the goal is to make predictive state match a vector of statistics about future outcomes. There are some key differences, mainly due to a focus on offline estimation in PSIMs. The predictive questions in PSIMs are typically about observations 1-step, 2-step up to $k$-steps into the future. To use such targets, batches of data need to be gathered and statistics computed offline to create the targets. Further, the state-update (filtering) function is trained using an alternating minimization strategy, with an algorithm called DAgger, rather than with algorithms for RNNs. Nonetheless, the motivation is similar: using an explicit objective to encourage internal state to be a predictive state.

A natural question, then, is whether the types of questions used by GVFNs provides advantages over PSIMs. Unlike $k$-step predictions in the future, GVFs allow questions about outcomes infinitely far into the far, through the use of cumulative discounted sums. Such predictions, though, do not provide high precision about such future events. As motivated in Section \ref{sec_constraining}, GVFs should be easier to learn online. In our experiments, we include a baseline, called a Forecast Network, that uses $k$-step predictions as predictive features, to provide some evidence that GVFs are more suitable as predictive features for online agents.
#+end_comment

* General Value Function Networks





* An objective function for GVFNs
* Empirical Results (GVFNs)
** Time Series Data sets
** RL problems (compass world, RingWorld, CycleWorld)
* Open Problems using GVFNs in large domains - Solution Methods
** Discovery
** Optimization
** Architecture
* Future directions
** Modules, Modules, Modules
- End to end-to-end learning.
- How do we construct modules such that they 
** Discovery
** Open Problems in learning state
* Postamble                                                          :ignore:

#+begin_export latex
\printbibliography
\appendix
#+end_export

