% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

% environment for abstract.
\begin{abstract}

All intelligent decision making organisms in the real world must be able to make decisions when their is information lacking to make the best decision possible. This is even the case for artificially constructed agents, whose perception---much like biological agents---is constrained by the available sensors on its body. To make better decisions, an agent will have to take advantage of its history to better identify key pieces of information not provided by its sensor observations. This thesis contributes to the setting in which a reinforcement learning agent does not have access to a data-stream sufficient to make the best decisions.

This thesis investigates the use of recurrent architectures to learn state (i.e. a summarization of the agent's history), and identifies the key modifications needed to enable efficient learning of the state in reinforcement learning applications. Two types of recurrent architectures are considered. The first is known as a recurrent neural network. The key feature of these networks is the state is learned through the application of gradient descent on the agent's overall objective. The second is a novel recurrent architecture named general value function networks (GVFNs). The key feature of GVFNs is each component of the state is constrained to be a action-conditioned prediction of the future stream of sensori-motor observations. Using these architectures, this thesis identifies several areas of improvement for learning state in reinforcement learning applications which can be applied generally.

\end{abstract}

\end{document}