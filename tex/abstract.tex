% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

% environment for abstract.
\begin{abstract}

% All intelligent decision making organisms in the real world must be able to make decisions when their is information lacking to make the best decision possible. This is even the case for artificially constructed agents, whose perception---much like biological agents---is constrained by the available sensors on its body. To make better decisions, an agent will have to take advantage of its history to better identify key pieces of information not provided by its sensor observations. This thesis contributes to the setting in which a reinforcement learning agent does not have access to a data-stream sufficient to make the best decisions.
Partial observability---when the senses lack enough detail to make an optimal decision---is the reality of any decision making agent acting in the real world. While an agent could be made to make due with its available senses, taking advantage of the history of senses can provide more context and enable the agent to make better decisions. This thesis investigates recurrent architectures to learn agent state (a summarization of the agent's history), and identifies some modifications---inspired by predictive representations of state---to enable efficient learning in (continual) reinforcement learning. First, I contribute to standard recurrent neural networks trained through back-propagation through time. This contribution provides pragmatic recommendations for incorporating action information into a recurrent architecture, and through extensive empirical investigations shows the trade-offs of several techniques. Second, I develop a recurrent predictive architecture which uses temporal abstractions---predictions in the form of general value functions---as the basis for its state representation. I show advantages of this architecture over standard recurrent networks in a continuing reinforcement learning domain, derive an objective and corresponding learning algorithm, and discuss several added concerns when using this architecture---such as discovery, what types of networks can be constructed, and off-policy prediction.

%Finally, I discuss several areas of future research when applying recurrent networks to the reinforcement learning problem.
% Two types of recurrent architectures are considered. The first is known as a recurrent neural network. The key feature of these networks is the state is learned through the application of gradient descent on the agent's overall objective. The second is a novel recurrent architecture named general value function networks (GVFNs). The key feature of GVFNs is each component of the state is constrained to be a action-conditioned prediction of the future stream of sensori-motor observations. Using these architectures, this thesis identifies several areas of improvement for learning state in reinforcement learning applications which can be applied generally.

\end{abstract}

\end{document}